{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20cdd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, optim \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import random\n",
    "from torch.utils import data\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8e41b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6dca358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=pd.read_csv(\"data_x_new.csv\")\n",
    "data_t=pd.read_csv(\"data_t_new.csv\")\n",
    "data_xx=pd.read_csv(\"data_x_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "969f73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h10_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h10_t=data_x.iloc[:, [1]]\n",
    "h10_R=data_x.iloc[:,[2]]#初值h实部\n",
    "h10_C=data_x.iloc[:,[3]]#初值h虚部\n",
    "hb_x1=data_t.iloc[:,[1]]#边值-5，t\n",
    "hb_t1=data_t.iloc[:,[0]]\n",
    "hb_x2=data_t.iloc[:,[2]]#边值5，t\n",
    "hb_t2=data_t.iloc[:,[0]]\n",
    "\n",
    "h20_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h20_t=data_x.iloc[:, [1]]\n",
    "h20_R=data_x.iloc[:,[4]]#初值h实部\n",
    "h20_C=data_x.iloc[:,[5]]#初值h虚部\n",
    "\n",
    "h30_x=data_xx.iloc[:, [0]]\n",
    "h30_t=data_xx.iloc[:, [1]]\n",
    "h30_r1=data_xx.iloc[:, [2]]\n",
    "h30_c1=data_xx.iloc[:, [3]]\n",
    "h30_r2=data_xx.iloc[:, [4]]\n",
    "h30_c2=data_xx.iloc[:, [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76bd3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x['x']\n",
    "t = data_t['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b2ca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc71aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf= np.hstack((X.flatten()[:,None], T.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "950258e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_x=hf[:,[0]]\n",
    "hf_t=hf[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78ff78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train10_x,test10_x,train10_t,test10_t,train10_r,test10_r,train10_c,test10_c,trainb_x1,testb_x1,trainb_t1,testb_t1,trainb_x2,testb_x2,trainb_t2,testb_t2,train20_r,test20_r,train20_c,test20_c = train_test_split(h10_x,h10_t,h10_R,h10_C,hb_x1,hb_t1,hb_x2,hb_t2,h20_R,h20_C,test_size=0.2)\n",
    "\n",
    "train30_x,test30_x,train30_t,test30_t,train30_r1,test30_r1,train30_c1,test30_c1,train30_r2,test30_r2,train30_c2,test30_c2=train_test_split(h30_x,h30_t,h30_r1,h30_c1,h30_r2,h30_c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8301e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainu_x, testu_x, trainu_t, testu_t = train_test_split(hf_x, hf_t, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68c1b3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train10_x=torch.from_numpy(train10_x.to_numpy()).float()\n",
    "train10_t=torch.from_numpy(train10_t.to_numpy()).float()\n",
    "\n",
    "train10_r=torch.from_numpy(train10_r.to_numpy()).float()\n",
    "train10_c=torch.from_numpy(train10_c.to_numpy()).float()\n",
    "\n",
    "trainb_x1=torch.from_numpy(trainb_x1.to_numpy()).float()\n",
    "trainb_t1=torch.from_numpy(trainb_t1.to_numpy()).float()\n",
    "trainb_x2=torch.from_numpy(trainb_x2.to_numpy()).float()\n",
    "trainb_t2=torch.from_numpy(trainb_t2.to_numpy()).float()\n",
    "\n",
    "# train20_x=torch.from_numpy(train20_x.to_numpy()).float()\n",
    "# train20_t=torch.from_numpy(train20_t.to_numpy()).float()\n",
    "train20_r=torch.from_numpy(train20_r.to_numpy()).float()\n",
    "train20_c=torch.from_numpy(train20_c.to_numpy()).float()\n",
    "\n",
    "\n",
    "train30_x=torch.from_numpy(train30_x.to_numpy()).float()\n",
    "train30_t=torch.from_numpy(train30_t.to_numpy()).float()\n",
    "train30_r1=torch.from_numpy(train30_r1.to_numpy()).float()\n",
    "train30_c1=torch.from_numpy(train30_c1.to_numpy()).float()\n",
    "train30_r2=torch.from_numpy(train30_r2.to_numpy()).float()\n",
    "train30_c2=torch.from_numpy(train30_c2.to_numpy()).float()\n",
    "\n",
    "trainu_x=torch.from_numpy(trainu_x).float()\n",
    "trainu_t=torch.from_numpy(trainu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2e60ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10_x=torch.from_numpy(test10_x.to_numpy()).float()\n",
    "test10_t=torch.from_numpy(test10_t.to_numpy()).float()\n",
    "test10_r=torch.from_numpy(test10_r.to_numpy()).float()\n",
    "test10_c=torch.from_numpy(test10_c.to_numpy()).float()\n",
    "\n",
    "testb_x1=torch.from_numpy(testb_x1.to_numpy()).float()\n",
    "testb_t1=torch.from_numpy(testb_t1.to_numpy()).float()\n",
    "testb_x2=torch.from_numpy(testb_x2.to_numpy()).float()\n",
    "testb_t2=torch.from_numpy(testb_t2.to_numpy()).float()\n",
    "\n",
    "# test20_x=torch.from_numpy(test20_x.to_numpy()).float()\n",
    "# test20_t=torch.from_numpy(test20_t.to_numpy()).float()\n",
    "test20_r=torch.from_numpy(test20_r.to_numpy()).float()\n",
    "test20_c=torch.from_numpy(test20_c.to_numpy()).float()\n",
    "\n",
    "testu_x=torch.from_numpy(testu_x).float()\n",
    "testu_t=torch.from_numpy(testu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20c0aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7b860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f5e4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN():\n",
    "    def __init__(self, u10_x, u10_t, u10_r, u10_c, ub_x1, ub_t1, ub_x2, ub_t2, u20_r, u20_c, uf_x, uf_t,u30_x,u30_t,u30_r1,u30_c1,u30_r2,u30_c2):\n",
    "        self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
    "        self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
    "        self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
    "        self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
    "        self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
    "        self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
    "        self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
    "        \n",
    "#         self.u20_x = u20_x.clone().detach().requires_grad_(True).float().to(device)\n",
    "#         self.u20_t = torch.tensor(u20_t, requires_grad=True).float().to(device)\n",
    "        self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
    "        self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
    "        self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
    "        self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
    "        self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
    "        self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
    "        self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
    "      \n",
    "        \n",
    "        self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
    "        self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.tensor([0.8], requires_grad=True).to(device)\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        \n",
    "        self.lambda_2 = torch.tensor([2.2], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        self.dnn = DNN().to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params':self.dnn.parameters()},\n",
    "            {'params': [self.lambda_1], 'lr': 0.0003},\n",
    "            {'params': [self.lambda_2], 'lr': 0.0002}\n",
    "        ],lr=0.006 )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "        self.losshistory=[]\n",
    "    def net_u(self, x, t):  \n",
    "        u1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,0],1)\n",
    "        v1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,1],1)\n",
    "        u2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,2],1)\n",
    "        v2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,3],1)\n",
    "        return u1, v1, u2, v2\n",
    "\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        beta=1\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        lambda_3 = -1\n",
    "        lambda_4 = 0\n",
    "        u1, v1,u2, v2= self.net_u(x, t)\n",
    "        \n",
    "        u1_t = torch.autograd.grad(\n",
    "            u1, t, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_x = torch.autograd.grad(\n",
    "            u1, x, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_xx = torch.autograd.grad(\n",
    "            u1_x, x, \n",
    "            grad_outputs=torch.ones_like(u1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v1_t = torch.autograd.grad(\n",
    "            v1, t, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_x = torch.autograd.grad(\n",
    "            v1, x, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_xx = torch.autograd.grad(\n",
    "            v1_x, x, \n",
    "            grad_outputs=torch.ones_like(v1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        u2_t = torch.autograd.grad(\n",
    "            u2, t, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_x = torch.autograd.grad(\n",
    "            u2, x, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_xx = torch.autograd.grad(\n",
    "            u2_x, x, \n",
    "            grad_outputs=torch.ones_like(u2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v2_t = torch.autograd.grad(\n",
    "            v2, t, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_x = torch.autograd.grad(\n",
    "            v2, x, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_xx = torch.autograd.grad(\n",
    "            v2_x, x, \n",
    "            grad_outputs=torch.ones_like(v2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f_u1 = (\n",
    "            v1_t + u1_xx\n",
    "        + lambda_1*u1*(u1**2 + v1**2) + lambda_2*u1*(u2**2 + v2**2) + lambda_3*(u1*(u2**2 - v2**2) + 2*u2*v1*v2) + lambda_4*(u2*(u1**2 - v1**2) + 2*u1*v1*v2)\n",
    "    )\n",
    "\n",
    "        f_v1 = (\n",
    "                -u1_t + v1_xx\n",
    "            + lambda_1*v1*(u1**2 + v1**2) + lambda_2*v1*(u2**2 + v2**2) - lambda_3*(v1*(u2**2 - v2**2) - 2*u1*u2*v2) - lambda_4*(v2*(u1**2 - v1**2) - 2*u1*u2*v1)\n",
    "        )\n",
    "\n",
    "        f_u2 = (\n",
    "                v2_t + beta*u2_xx\n",
    "            + lambda_1*u2*(u2**2 + v2**2) + lambda_2*u2*(u1**2 + v1**2) + lambda_3*(u2*(u1**2 - v1**2) + 2*u1*v2*v1) + lambda_4*(u1*(u2**2 - v2**2) + 2*u2*v2*v1)\n",
    "        )\n",
    "\n",
    "        f_v2 = (\n",
    "                -u2_t + beta*v2_xx\n",
    "            + lambda_1*v2*(u2**2 + v2**2) + lambda_2*v2*(u1**2 + v1**2) - lambda_3*(v2*(u1**2 - v1**2) - 2*u2*u1*v1) - lambda_4*(v1*(u2**2 - v2**2) - 2*u2*u1*v2)\n",
    "        )\n",
    "        a=u1_x\n",
    "        b=v1_x\n",
    "        c=u2_x\n",
    "        d=v2_x\n",
    "        return f_u1, f_v1, f_u2, f_v2, a, b, c, d\n",
    "     \n",
    "    def loss_func(self):\n",
    "        for self.iter in tqdm(range(50000)):\n",
    "            torch.cuda.empty_cache()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            u10_pred, v10_pred, u20_pred, v20_pred = self.net_u(self.u10_x,self.u10_t)\n",
    "            u30_pred1, v30_pred1, u30_pred2, v30_pred2 = self.net_u(self.u30_x,self.u30_t)\n",
    "            u1b_pred1, v1b_pred1, u2b_pred1, v2b_pred1 = self.net_u(self.ub_x1,self.ub_t1)\n",
    "            u1b_pred2, v1b_pred2, u2b_pred2, v2b_pred2 = self .net_u(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            \n",
    "            a1,b1,c1,d1,u1bx_pred1, v1bx_pred1,u2bx_pred1, v2bx_pred1 = self.net_f(self.ub_x1,self.ub_t1)\n",
    "            a2,b2,c2,d2,u1bx_pred2, v1bx_pred2,u2bx_pred2, v2bx_pred2 = self.net_f(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            f_predu1, f_predv1, f_predu2, f_predv2,a1,b1,c1,d1 = self.net_f(self.uf_x,self.uf_t)\n",
    "            \n",
    "            loss_u0 = torch.mean((self.u10_r - u10_pred) ** 2)+torch.mean((self.u10_c - v10_pred) ** 2)+torch.mean((self.u20_r - u20_pred) ** 2)+torch.mean((self.u20_c - v20_pred) ** 2)\n",
    "            loss_ub = torch.mean((u1b_pred1 - u1b_pred2) ** 2)+torch.mean((v1b_pred1 - v1b_pred2) ** 2)+torch.mean((u2b_pred1 - u2b_pred2) ** 2)+torch.mean((v2b_pred1 - v2b_pred2) ** 2)\n",
    "            loss_ubx = torch.mean((u1bx_pred1 - u1bx_pred2) ** 2)+torch.mean((v1bx_pred1 - v1bx_pred2) ** 2)+torch.mean((u2bx_pred1 - u2bx_pred2) ** 2)+torch.mean((v2bx_pred1 - v2bx_pred2) ** 2)\n",
    "            loss_f = torch.mean(f_predu1 ** 2)+torch.mean(f_predv1 ** 2)+torch.mean(f_predu2 ** 2)+torch.mean(f_predv2 ** 2)\n",
    "            loss_u1=torch.mean((self.u30_r1 - u30_pred1) ** 2)+torch.mean((self.u30_c1 - v30_pred1) ** 2)+torch.mean((self.u30_r2 - u30_pred2) ** 2)+torch.mean((self.u30_c2 - v30_pred2) ** 2)\n",
    "            \n",
    "            loss = loss_f+loss_u0 + loss_ub + loss_ubx+loss_u1\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losshistory.append(loss.clone().detach().cpu())\n",
    "            \n",
    "            self.iter += 1\n",
    "            with torch.no_grad():\n",
    "                if self.iter % 500 == 0:\n",
    "                    print('Iter %d, Loss: %.5e, Loss_u0: %.5e, Loss_ub: %.5e, Loss_ubx: %.5e, Loss_f: %.5e, Loss_u1: %.5e, lambda1: %.5e, lambda2: %.5e' % (self.iter, loss.item(), loss_u0.item(), loss_ub.item(), loss_ubx.item(), loss_f.item(), loss_u1.item(), self.lambda_1,self.lambda_2))\n",
    "        return float(loss)\n",
    "        \n",
    " \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "        \n",
    "    def predict1(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u1, v1 , u2, v2= self.net_u(x, t)\n",
    "        u1 = u1.detach().cpu().numpy()\n",
    "        v1 = v1.detach().cpu().numpy()\n",
    "        u2 = u2.detach().cpu().numpy()\n",
    "        v2 = v2.detach().cpu().numpy()\n",
    "        return u1, v1, u2, v2\n",
    "    \n",
    "    def predict2(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        f1, f2, f3, f4, a, b, c, d = self.net_f(x, t)\n",
    "        f1 = f1.detach().cpu().numpy()\n",
    "        f2 = f2.detach().cpu().numpy()\n",
    "        f3 = f3.detach().cpu().numpy()\n",
    "        f4 = f4.detach().cpu().numpy()\n",
    "        return  f1, f2, f3, f4, a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08905a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PhysicsInformedNN(train10_x, train10_t, train10_r, train10_c, \n",
    "                          trainb_x1, trainb_t1, trainb_x2, trainb_t2,\n",
    "                        train20_r, train20_c, trainu_x, trainu_t,\n",
    "                         train30_x,train30_t, train30_r1, train30_c1,train30_r2,train30_c2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7626fd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/50000 [00:57<1:38:18,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500, Loss: 1.26258e-01, Loss_u0: 7.28430e-03, Loss_ub: 6.35796e-05, Loss_ubx: 5.06922e-05, Loss_f: 3.48961e-02, Loss_u1: 8.39631e-02, lambda1: 8.76312e-01, lambda2: 2.23819e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1001/50000 [01:54<1:31:32,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Loss: 9.68210e-02, Loss_u0: 4.70757e-03, Loss_ub: 1.27961e-05, Loss_ubx: 1.27172e-05, Loss_f: 2.45876e-02, Loss_u1: 6.75003e-02, lambda1: 8.66862e-01, lambda2: 2.21734e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1501/50000 [02:51<1:28:22,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1500, Loss: 7.87860e-02, Loss_u0: 3.04141e-03, Loss_ub: 2.17428e-05, Loss_ubx: 1.67903e-05, Loss_f: 1.97373e-02, Loss_u1: 5.59687e-02, lambda1: 8.53575e-01, lambda2: 2.20359e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2001/50000 [03:48<1:30:08,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2000, Loss: 8.36866e-02, Loss_u0: 2.27283e-03, Loss_ub: 9.64381e-05, Loss_ubx: 1.46400e-05, Loss_f: 2.89883e-02, Loss_u1: 5.23144e-02, lambda1: 8.45064e-01, lambda2: 2.19502e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2501/50000 [04:46<1:27:54,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2500, Loss: 6.17886e-02, Loss_u0: 1.44148e-03, Loss_ub: 1.15445e-05, Loss_ubx: 1.48240e-05, Loss_f: 1.45089e-02, Loss_u1: 4.58119e-02, lambda1: 8.39175e-01, lambda2: 2.18779e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3001/50000 [05:43<1:29:38,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3000, Loss: 6.18382e-02, Loss_u0: 1.63159e-03, Loss_ub: 4.36535e-05, Loss_ubx: 2.24343e-05, Loss_f: 1.66905e-02, Loss_u1: 4.34500e-02, lambda1: 8.32884e-01, lambda2: 2.18011e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3501/50000 [06:40<1:31:22,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3500, Loss: 5.57104e-02, Loss_u0: 1.39475e-03, Loss_ub: 8.43019e-06, Loss_ubx: 1.44455e-05, Loss_f: 1.30395e-02, Loss_u1: 4.12534e-02, lambda1: 8.27411e-01, lambda2: 2.17305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4001/50000 [07:37<1:27:45,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4000, Loss: 9.13389e-02, Loss_u0: 2.77587e-03, Loss_ub: 2.94069e-05, Loss_ubx: 2.80223e-05, Loss_f: 2.04927e-02, Loss_u1: 6.80130e-02, lambda1: 8.37944e-01, lambda2: 2.17939e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4501/50000 [08:34<1:28:37,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4500, Loss: 7.38362e-02, Loss_u0: 1.98267e-03, Loss_ub: 1.61696e-05, Loss_ubx: 1.83576e-05, Loss_f: 1.79280e-02, Loss_u1: 5.38910e-02, lambda1: 8.40720e-01, lambda2: 2.18020e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5001/50000 [09:31<1:26:31,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5000, Loss: 1.52316e-01, Loss_u0: 5.64949e-03, Loss_ub: 1.65551e-04, Loss_ubx: 5.70168e-05, Loss_f: 2.68944e-02, Loss_u1: 1.19549e-01, lambda1: 8.47381e-01, lambda2: 2.18471e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5501/50000 [10:28<1:26:33,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5500, Loss: 8.44022e-02, Loss_u0: 2.43760e-03, Loss_ub: 3.69406e-05, Loss_ubx: 1.72796e-05, Loss_f: 2.05950e-02, Loss_u1: 6.13154e-02, lambda1: 8.57063e-01, lambda2: 2.18646e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6001/50000 [11:26<1:23:18,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6000, Loss: 7.92119e-02, Loss_u0: 1.68969e-03, Loss_ub: 1.08450e-05, Loss_ubx: 1.75950e-05, Loss_f: 2.16750e-02, Loss_u1: 5.58188e-02, lambda1: 8.50842e-01, lambda2: 2.17998e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6501/50000 [12:23<1:21:08,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6500, Loss: 6.96249e-02, Loss_u0: 1.58934e-03, Loss_ub: 1.07445e-05, Loss_ubx: 1.41413e-05, Loss_f: 1.87791e-02, Loss_u1: 4.92316e-02, lambda1: 8.45348e-01, lambda2: 2.17522e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7001/50000 [13:20<1:24:01,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7000, Loss: 6.17473e-02, Loss_u0: 1.58513e-03, Loss_ub: 9.30044e-06, Loss_ubx: 9.47357e-06, Loss_f: 1.58502e-02, Loss_u1: 4.42933e-02, lambda1: 8.45818e-01, lambda2: 2.17190e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 7501/50000 [14:17<1:17:42,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7500, Loss: 7.36207e-02, Loss_u0: 1.66645e-03, Loss_ub: 1.45086e-05, Loss_ubx: 1.17952e-05, Loss_f: 1.79860e-02, Loss_u1: 5.39420e-02, lambda1: 8.48062e-01, lambda2: 2.17040e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8001/50000 [15:15<1:20:53,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8000, Loss: 6.55179e-02, Loss_u0: 1.58874e-03, Loss_ub: 1.29060e-05, Loss_ubx: 8.62542e-06, Loss_f: 1.88589e-02, Loss_u1: 4.50487e-02, lambda1: 8.45860e-01, lambda2: 2.16736e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 8501/50000 [16:12<1:20:33,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8500, Loss: 5.34626e-02, Loss_u0: 1.19952e-03, Loss_ub: 9.52039e-06, Loss_ubx: 5.74221e-06, Loss_f: 1.30184e-02, Loss_u1: 3.92294e-02, lambda1: 8.40975e-01, lambda2: 2.16175e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9001/50000 [17:10<1:17:29,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9000, Loss: 4.75950e-02, Loss_u0: 1.06548e-03, Loss_ub: 1.03962e-05, Loss_ubx: 1.06685e-05, Loss_f: 1.22775e-02, Loss_u1: 3.42310e-02, lambda1: 8.36272e-01, lambda2: 2.15531e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 9501/50000 [18:07<1:18:55,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9500, Loss: 9.06543e-02, Loss_u0: 3.06842e-03, Loss_ub: 4.04666e-05, Loss_ubx: 1.81657e-05, Loss_f: 2.44905e-02, Loss_u1: 6.30368e-02, lambda1: 8.62838e-01, lambda2: 2.16870e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 9696/50000 [18:30<1:16:57,  8.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py\u001b[0m in \u001b[0;36mloss_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu1bx_pred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1bx_pred1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu2bx_pred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2bx_pred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_x1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_t1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m             \u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu1bx_pred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1bx_pred2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu2bx_pred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2bx_pred2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_x2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_t2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py\u001b[0m in \u001b[0;36mnet_f\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mlambda_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mlambda_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mu1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_u\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         u1_t = torch.autograd.grad(\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12224\\4083284633.py\u001b[0m in \u001b[0;36mnet_u\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mu2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mu1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12224\\1766163557.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e3d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20379802f10>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqGUlEQVR4nO3df3RU9Z3/8ded/BiSkIz8zCQQMGrQaoAqWExsC2rBUqU/2NO1Ql38btuVIlbW9tAi3z1Gj00ou8uh38NKV7dLcb9F+u1Ru571F3GroTVQAaVCUEqXCBEyRGmYBAiTZObz/SOZC0MAmZB8JjN5Ps65J5l7PzPz/sxnfrzuZ+7MOMYYIwAAAEs8iS4AAAAMLoQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFalJ7qAs0UiER0+fFi5ublyHCfR5QAAgItgjFFra6sKCwvl8Vx4bmPAhY/Dhw+rqKgo0WUAAIBeaGho0NixYy/YZsCFj9zcXEldxefl5SW4GgAAcDFaWlpUVFTkvo5fyIALH9G3WvLy8ggfAAAkmYs5ZIIDTgEAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFbFHT4OHTqkb37zmxoxYoSys7P16U9/Wjt27HC3G2NUUVGhwsJCZWVlacaMGaqrq+vTogEAg8/R4yH9rOZ/1NR6KtGl4BLFFT6am5t18803KyMjQy+//LL27Nmjf/7nf9Zll13mtlm5cqVWrVqlNWvWaNu2bfL7/Zo5c6ZaW1v7unYAwCDy3V++rRUvv697/31bokvBJUqPp/FPfvITFRUVad26de66yy+/3P3fGKPVq1dr+fLlmjt3riRp/fr1ys/P14YNG3Tffff1TdUAgEHnrfq/SJL2NLYkuBJcqrhmPl544QVNnTpVX//61zV69Ghdf/31euqpp9zt9fX1CgQCmjVrlrvO6/Vq+vTpqq2tPedlhkIhtbS0xCwAACB1xRU+9u/fr7Vr16qkpESvvvqqFi5cqO9973t6+umnJUmBQECSlJ+fH3O+/Px8d9vZqqqq5PP53KWoqKg3/QAAAEkirvARiUR0ww03qLKyUtdff73uu+8+fec739HatWtj2jmOE3PaGNNjXdSyZcsUDAbdpaGhIc4uAACAZBJX+CgoKNC1114bs+5Tn/qUDh48KEny+/2S1GOWo6mpqcdsSJTX61VeXl7MAgAAUldc4ePmm2/W3r17Y9b96U9/0vjx4yVJxcXF8vv9qq6udre3t7erpqZG5eXlfVAuAABIdnF92uXv//7vVV5ersrKSv31X/+13nrrLT355JN68sknJXW93bJkyRJVVlaqpKREJSUlqqysVHZ2tubNm9cvHQAAAMklrvBx44036vnnn9eyZcv02GOPqbi4WKtXr9b8+fPdNkuXLlVbW5sWLVqk5uZmTZs2TZs2bVJubm6fFw8AAJKPY4wxiS7iTC0tLfL5fAoGgxz/AQBwXf6jF93/P1hxRwIrwbnE8/rNb7sAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArIorfFRUVMhxnJjF7/e7240xqqioUGFhobKysjRjxgzV1dX1edEAACB5xT3zcd1116mxsdFddu3a5W5buXKlVq1apTVr1mjbtm3y+/2aOXOmWltb+7RoAACQvOIOH+np6fL7/e4yatQoSV2zHqtXr9by5cs1d+5clZaWav369Tp58qQ2bNjQ54UDAIDkFHf42LdvnwoLC1VcXKxvfOMb2r9/vySpvr5egUBAs2bNctt6vV5Nnz5dtbW1fVcxAABIaunxNJ42bZqefvppTZgwQUeOHNHjjz+u8vJy1dXVKRAISJLy8/NjzpOfn68DBw6c9zJDoZBCoZB7uqWlJZ6SAABAkokrfMyePdv9f+LEiSorK9OVV16p9evX66abbpIkOY4Tcx5jTI91Z6qqqtKjjz4aTxkAACCJXdJHbXNycjRx4kTt27fP/dRLdAYkqqmpqcdsyJmWLVumYDDoLg0NDZdSEgAAGOAuKXyEQiG99957KigoUHFxsfx+v6qrq93t7e3tqqmpUXl5+Xkvw+v1Ki8vL2YBAACpK663XX7wgx9ozpw5GjdunJqamvT444+rpaVFCxYskOM4WrJkiSorK1VSUqKSkhJVVlYqOztb8+bN66/6AQBAkokrfHz44Ye6++679fHHH2vUqFG66aabtHXrVo0fP16StHTpUrW1tWnRokVqbm7WtGnTtGnTJuXm5vZL8QAAIPk4xhiT6CLO1NLSIp/Pp2AwyFswAADX5T960f3/gxV3JLASnEs8r9/8tgsAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACw6pLCR1VVlRzH0ZIlS9x1xhhVVFSosLBQWVlZmjFjhurq6i61TgAAkCJ6HT62bdumJ598UpMmTYpZv3LlSq1atUpr1qzRtm3b5Pf7NXPmTLW2tl5ysQAAIPn1KnwcP35c8+fP11NPPaVhw4a5640xWr16tZYvX665c+eqtLRU69ev18mTJ7Vhw4Y+KxoAACSvXoWP+++/X3fccYe+8IUvxKyvr69XIBDQrFmz3HVer1fTp09XbW3tOS8rFAqppaUlZgEAAKkrPd4zbNy4UTt27ND27dt7bAsEApKk/Pz8mPX5+fk6cODAOS+vqqpKjz76aLxlAACAJBXXzEdDQ4MefPBB/fKXv9SQIUPO285xnJjTxpge66KWLVumYDDoLg0NDfGUBAAAkkxcMx87duxQU1OTpkyZ4q4Lh8PavHmz1qxZo71790rqmgEpKChw2zQ1NfWYDYnyer3yer29qR0AACShuGY+brvtNu3atUs7d+50l6lTp2r+/PnauXOnrrjiCvn9flVXV7vnaW9vV01NjcrLy/u8eAAAkHzimvnIzc1VaWlpzLqcnByNGDHCXb9kyRJVVlaqpKREJSUlqqysVHZ2tubNm9d3VQMAgKQV9wGnn2Tp0qVqa2vTokWL1NzcrGnTpmnTpk3Kzc3t66sCAABJyDHGmEQXcaaWlhb5fD4Fg0Hl5eUluhwAwABx+Y9edP//YMUdCawE5xLP6ze/7QIAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsiit8rF27VpMmTVJeXp7y8vJUVlaml19+2d1ujFFFRYUKCwuVlZWlGTNmqK6urs+LBgAAySuu8DF27FitWLFC27dv1/bt23XrrbfqK1/5ihswVq5cqVWrVmnNmjXatm2b/H6/Zs6cqdbW1n4pHgAAJJ+4wsecOXP0pS99SRMmTNCECRP04x//WEOHDtXWrVtljNHq1au1fPlyzZ07V6WlpVq/fr1OnjypDRs29Ff9AAAgyfT6mI9wOKyNGzfqxIkTKisrU319vQKBgGbNmuW28Xq9mj59umpra897OaFQSC0tLTELAABIXXGHj127dmno0KHyer1auHChnn/+eV177bUKBAKSpPz8/Jj2+fn57rZzqaqqks/nc5eioqJ4SwIAAEkk7vBx9dVXa+fOndq6dau++93vasGCBdqzZ4+73XGcmPbGmB7rzrRs2TIFg0F3aWhoiLckAACQRNLjPUNmZqauuuoqSdLUqVO1bds2/fSnP9UPf/hDSVIgEFBBQYHbvqmpqcdsyJm8Xq+8Xm+8ZQAAgCR1yd/zYYxRKBRScXGx/H6/qqur3W3t7e2qqalReXn5pV4NAABIEXHNfDz88MOaPXu2ioqK1Nraqo0bN+qNN97QK6+8IsdxtGTJElVWVqqkpEQlJSWqrKxUdna25s2b11/1AwCAJBNX+Dhy5IjuueceNTY2yufzadKkSXrllVc0c+ZMSdLSpUvV1tamRYsWqbm5WdOmTdOmTZuUm5vbL8UDAIDk4xhjTKKLOFNLS4t8Pp+CwaDy8vISXQ4AYIC4/Ecvuv9/sOKOBFaCc4nn9ZvfdgEAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYFVf4qKqq0o033qjc3FyNHj1aX/3qV7V3796YNsYYVVRUqLCwUFlZWZoxY4bq6ur6tGgAAJC84gofNTU1uv/++7V161ZVV1ers7NTs2bN0okTJ9w2K1eu1KpVq7RmzRpt27ZNfr9fM2fOVGtra58XDwAAkk96PI1feeWVmNPr1q3T6NGjtWPHDn3+85+XMUarV6/W8uXLNXfuXEnS+vXrlZ+frw0bNui+++7ru8oBAEBSuqRjPoLBoCRp+PDhkqT6+noFAgHNmjXLbeP1ejV9+nTV1tae8zJCoZBaWlpiFgAAkLp6HT6MMXrooYf02c9+VqWlpZKkQCAgScrPz49pm5+f7247W1VVlXw+n7sUFRX1tiQAAJAEeh0+Fi9erHfffVfPPPNMj22O48ScNsb0WBe1bNkyBYNBd2loaOhtSQAAIAnEdcxH1AMPPKAXXnhBmzdv1tixY931fr9fUtcMSEFBgbu+qampx2xIlNfrldfr7U0ZAAAgCcU182GM0eLFi/Xcc8/pt7/9rYqLi2O2FxcXy+/3q7q62l3X3t6umpoalZeX903FAAAgqcU183H//fdrw4YN+s///E/l5ua6x3H4fD5lZWXJcRwtWbJElZWVKikpUUlJiSorK5Wdna158+b1SwcAAEByiSt8rF27VpI0Y8aMmPXr1q3TvffeK0launSp2tratGjRIjU3N2vatGnatGmTcnNz+6RgAACQ3OIKH8aYT2zjOI4qKipUUVHR25oAAEAK47ddAACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+ACQdNo7I4kuAcAlIHwASCq/2/eRJvzvl/XU5v2JLgVAL8UdPjZv3qw5c+aosLBQjuPoN7/5Tcx2Y4wqKipUWFiorKwszZgxQ3V1dX1VL4BB7ge//qMk6ccvvZfgSgD0Vtzh48SJE5o8ebLWrFlzzu0rV67UqlWrtGbNGm3btk1+v18zZ85Ua2vrJRcLAACSX3q8Z5g9e7Zmz559zm3GGK1evVrLly/X3LlzJUnr169Xfn6+NmzYoPvuu+/SqgUAAEmvT4/5qK+vVyAQ0KxZs9x1Xq9X06dPV21t7TnPEwqF1NLSErMAwPk4chJdAoBL1KfhIxAISJLy8/Nj1ufn57vbzlZVVSWfz+cuRUVFfVkSAAAYYPrl0y6OE7tnYozpsS5q2bJlCgaD7tLQ0NAfJQFIEed5KgGQROI+5uNC/H6/pK4ZkIKCAnd9U1NTj9mQKK/XK6/X25dlAABSkONIxiS6CvSFPp35KC4ult/vV3V1tbuuvb1dNTU1Ki8v78urAgAASSrumY/jx4/rz3/+s3u6vr5eO3fu1PDhwzVu3DgtWbJElZWVKikpUUlJiSorK5Wdna158+b1aeEABifedQGSX9zhY/v27brlllvc0w899JAkacGCBfrFL36hpUuXqq2tTYsWLVJzc7OmTZumTZs2KTc3t++qBgAASSvu8DFjxgyZC7zp5jiOKioqVFFRcSl1AcA5ne/gdQDJg992AQAAVhE+AACAVYQPAABgFeEDAABYRfgAkFQ43hRIfoQPAABgFeEDAABYRfgAAABWET4AAIBVhA8ASYUDToHkR/gAAABWET4AAIBVhA8AScUR77sMVox86iB8AAAAqwgfAJIKB5wCyY/wAQAArCJ8AEgqTHwAyY/wAQAArCJ8AAAAqwgfAJKKwxGnQNIjfAAAAKsIHwCSCvMeQPIjfAAAAKsIHwAAwCrCBwAAsIrwAQBICibRBaDPED4AJBeOOAWSHuEDAJAUDFMfKYPwAQAArCJ8AEgqvOsCJD/CBwAAsIrwASCp8NsuQPIjfAAAAKsIHwAAwCrCB4CkwpsuQPIjfCSRdz88pn//fb0iET7sDgBIXumJLgAX78tr3pQk+bIy9FdTxia4GiAxON4USH7MfCShvUdaE10CAAC9RvhIQrztAgBIZoSPJBTmBw4wiDkccgokPcJHEhos2aMzHNGvth3UBx+fSHQpGEAMP6wOJD3CRxIygyR9/MfWA/rhs7s045/eSHQpfaa9M6LX9hxRy6mORJcSt3DE6KH/t1O//MOBhNbBu45A8iN8JKHB8uS7/YPmRJfQ51a/9id9++nt+pufv5XoUuL2al1Az719SMuf353QOiKDJHwDqWzQhI9THWFtqgvoP7Z8kOhSLlkyHfPREY7o4NGTvTpvKn6k8rm3D0mSdjYcS2whvXAi1JnoEiQNnrcdgVTWb9/z8cQTT+gf//Ef1djYqOuuu06rV6/W5z73uf66uk8U6ozo7/5jhyRp7g1jleNN3Fec/H7fxxo3PFvjRmT36vwX+7aLMUaHg6c05rKsXl1PX/i7p7fr9b0f6Rf/60bNuHp0XOdN96Re+khPS94+ZaQNjH0VZj4QD2OMnt5yQCX5Q1V+5cg+ucym1lN6te6IjDFyHEcep+tAaI8jeRxHjiN3ffT0S7satbPhmBbNuGpA7Fh5HEffvGl8wq6/X16Bf/WrX2nJkiV64okndPPNN+tf//VfNXv2bO3Zs0fjxo3rj6v8RL6sDA3LzlDzyQ799v0m3TmpICG/jrmz4Zi++fM/SJI+WHFHry7jYp97H/uvPVr35gf6yV9N1F03JuZ2f33vR5KkJzfvjzt8NDS39UdJcTt2sl2tpzpVNLx3YfFMyRyoBkr48AyEZ24kXFnVfytvSIYy0h2lebrum8YYGdMVUKN/3w+c/l6kacXDleZxlNb9OPREg0P3X50VIDyOI0VPS+6259451Ou6H3mh7hJ63Xcy0z2pFz5WrVqlb33rW/r2t78tSVq9erVeffVVrV27VlVVVf1xlRdlzuRCPb3lgB545h098Mw7mpA/VBEjfanUr9tL/fJlZchxHBXkDVF7OKIhGWl9XsOuQ8Fena8jHHH/P3Ts4l6U1735gSTph8/u+sTwEU3wkvTyrkb9+KX39H/uvl43jBvWq3rPVvs/R+M+z44Dp4/5ONJySiOHet0nDZs+/Vi1JOkPD9+m/Lwhl3RZh4+divs8rac61Hyi46JmytrawzrR3qmRQ709tjWfaNeuQ0F9rmRkr4J3Zvrp8PHz39frtmtGqzMS0X+926hvfbZYuUMy4r7M3rhyVI7quz8BdcWyF1U0PFtjh2XpzT8flT9viAItp7ThO9PkTU+TN92jIRketbVH5Pd1jV12Zpoy0z1Kcxx5BmgYjESMFv7fHRrqTdequz6d6HIGjAVl47V+S9cBz43BU2oMxvd4+kP9X/q0njGXZWniGJ8ixihiusOPFHu6OwRFnwNvu2a0vBmJD/LpnsTW4Jg+/uhEe3u7srOz9etf/1pf+9rX3PUPPvigdu7cqZqamguev6WlRT6fT8FgUHl5eX1Zmj4+HtLUx1+7qLYeR/Kmp7mpOJqCzzutpu7Tnmi76Lqu0w3NJ3WqIxJzHf68Icodkq6MNE9MQr9iVI6GetP1pyOtPc4TlRN9AvV4lJHWleTTPdG/HqV5HO1pbHHbX5adoYw0jzLTTreXuu6AHxw9oVBnRNf4c5U7JF3bzjjQc/JYnxynq31a916AOeOB1eMBF5GMTj/ozvw21nHDs+XLypDH4yjNkbsHkpHm6bqN1bXOGKOwkeoOBXX0RHuPcYn2Iy2tq6Y0j6OszDR3DKJ9i961o3fwdI8jR11jl57maP9HJ1R4WZYuy8roul51HRuUnZkmj+OoM2KUlZGmmj995F7/TVcMPz2+Pe4Tp1/IPj4e0rDsDGVnpsvjOf3NFC/88XBMfyaP9Skz3SNvelrM5am7L46k/36/SZJUMnqoxg3PVma6R44jdYaNPN39dZyuGbEXdzUq3dNV+4T8obpi5FAd/MtJBds63NA6bni2SsfkubeFc9Z9VTr9420ej6NIxKgzYvTHD4/pwAWO35k81idvRpoy0zzqjER0IhTWmMuylJbWdd88EerU7kMtmjTWp7ysDDmKvg3luNPQ0et1nNPf53HmNsdx9IvaD85bQ2940z0KdUaU1/1YjAYSj3P6CbojHOkx8/NRa0jZ3jQV+rJixiD6f8aZT+6O9Fb3C1/5lSOUme5RZ9jIqOsxEzbGfTxFjFHDX9r08fGQJCk/z6u8IRnyOI4y0k/fh8MRo4N/OakROZka0R02o48fR477kWSP4ygcMW5dXW26zp+e1jVrkHbm2HfPAHQ/3HtI83Td/426HuPOGfdXdZ82kvt/dFvM49HIPf+Z4+BxHEW6d4SMMadnuZyu80RnHP7qhrH6m7Lxaj3VqY5IRJ1h0zUz4ZF7+5z5GD12skMd4YjS07pui9MzI2fNluj0rMnZz3PRt/uiwWLssCzdOanwIu5hp525k5eK4nn97vPwcfjwYY0ZM0ZvvvmmysvL3fWVlZVav3699u7dG9M+FAopFArFFF9UVNQv4UPq2oucWLGpzy8XgF1fu36Mbr1mtP5p094eoWj8iGwZ0/XR5pPtnWo5NTAOlkXfqJhzre69uTjRZeAs8YSPfjvq8ux0d77EV1VVpUcffbS/yughd0hGj2MtOsMRdUaM2sMRmYjU1hFWe2dEjhM7fRbzt3uP5ew0fHZqjp4+1RFW7f98rDGXZakxeErTrhiu3CEZ6ghH1BE2ikSMAi2n1HqqQ+OG5ygjzdGHzW06HGzTnwKtyvGm68pRQ3Xbp0areGSOjh5vV0d33eHuPdNw9x5AOGLUEem6zGE5mSrwDVFLW4c6wqb7+rquM9QZ1sG/nNT4ETk6ePSERgz1ypHU2b2XdFlWhto6wu6eQvQjvud7b/R8s0F+3xC1tYfVcqpDbe1dlxc2XfV1RIw6wxH3ssORSNfsRfce/ZAMjz4/YZTSPI7a2sNq7669vTOicCSicKRrrzTUGXb3ZMIRc449acd96ypijDrDRvs/PqGPWkP6dJFPnZGucdp9KKjLR+YoOzOta2bBI6V5PDLGKN3j6Rp30zUtfuZ9ILrnFL1vRGdV0pzuvavuOg4fa1OBb4guH5Ej031bhjpj64/ep6J7ZBlpHu0+HFRpoU/t3WNujInZg4107yU6jhQIntJl2RlqaetUfp5XaR6P3v3wmCbk52r/x8d1jT+v596dTn9tf3SP2ZiuPfLoWER94VP5Gp3n1c6GY8pI8+j9QKuKhmUpHDFq6wgr1BHR+4EWXTFqqDq774cd4YjawxH91x8bNf3qUcodkt7V38jprwyL7gaduUfs7hmZ2HbDczJ192fGKSszTXMmX9zeZ3QGpz0cUagjrJPd96dIxMhzxm155p5x2BidDHV2zayd8fx1sr1T+44c1/FQp64rzHPH33GkjvDp2zYqbIze/PNRXTV6qEYOzdSJUNcM2+nZrthjDzzds6hFw7J1rK1DHZ1dj5GOSMS93EhE2n04qCtGDVVmmscdyzNnLKJ78tHbLTorKXXNYIQjXY+F6MzL6Tbn/zK3SCT2eqKP3eh9MGKMe/1nfi3A2TNYZz9GT3VEZGRivsH2zPtCtP2IHK/unFzwieONgS3hb7vYnvkAAAB9L56Zjz4/4iQzM1NTpkxRdXV1zPrq6uqYt2GivF6v8vLyYhYAAJC6+uVtl4ceekj33HOPpk6dqrKyMj355JM6ePCgFi5c2B9XBwAAkki/hI+77rpLR48e1WOPPabGxkaVlpbqpZde0vjxiftMMQAAGBj6/JiPS9WfH7UFAAD9I6HHfAAAAFwI4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgVb98vfqliH7haktLS4IrAQAAFyv6un0xX5w+4MJHa2urJKmoqCjBlQAAgHi1trbK5/NdsM2A+22XSCSiw4cPKzc3V47j9Ollt7S0qKioSA0NDYPud2Po++DsuzS4+0/f6ftg67uUuP4bY9Ta2qrCwkJ5PBc+qmPAzXx4PB6NHTu2X68jLy9vUN4hJfo+WPsuDe7+03f6Phglov+fNOMRxQGnAADAKsIHAACwalCFD6/Xq0ceeURerzfRpVhH3wdn36XB3X/6Tt8Ho2To/4A74BQAAKS2QTXzAQAAEo/wAQAArCJ8AAAAqwgfAADAqkETPp544gkVFxdryJAhmjJlin73u98luqS4VVRUyHGcmMXv97vbjTGqqKhQYWGhsrKyNGPGDNXV1cVcRigU0gMPPKCRI0cqJydHX/7yl/Xhhx/GtGlubtY999wjn88nn8+ne+65R8eOHbPRRdfmzZs1Z84cFRYWynEc/eY3v4nZbrOvBw8e1Jw5c5STk6ORI0fqe9/7ntrb2/uj25I+ue/33ntvj/vBTTfdFNMmWfteVVWlG2+8Ubm5uRo9erS++tWvau/evTFtUnXsL6bvqTr2a9eu1aRJk9wvxSorK9PLL7/sbk/VMY/6pP6n5LibQWDjxo0mIyPDPPXUU2bPnj3mwQcfNDk5OebAgQOJLi0ujzzyiLnuuutMY2OjuzQ1NbnbV6xYYXJzc82zzz5rdu3aZe666y5TUFBgWlpa3DYLFy40Y8aMMdXV1ebtt982t9xyi5k8ebLp7Ox023zxi180paWlpra21tTW1prS0lJz5513Wu3rSy+9ZJYvX26effZZI8k8//zzMdtt9bWzs9OUlpaaW265xbz99tumurraFBYWmsWLFyes7wsWLDBf/OIXY+4HR48ejWmTrH2//fbbzbp168zu3bvNzp07zR133GHGjRtnjh8/7rZJ1bG/mL6n6ti/8MIL5sUXXzR79+41e/fuNQ8//LDJyMgwu3fvNsak7phfbP9TcdwHRfj4zGc+YxYuXBiz7pprrjE/+tGPElRR7zzyyCNm8uTJ59wWiUSM3+83K1ascNedOnXK+Hw+87Of/cwYY8yxY8dMRkaG2bhxo9vm0KFDxuPxmFdeecUYY8yePXuMJLN161a3zZYtW4wk8/777/dDrz7Z2S/ANvv60ksvGY/HYw4dOuS2eeaZZ4zX6zXBYLBf+num84WPr3zlK+c9T6r03RhjmpqajCRTU1NjjBlcY392340ZXGM/bNgw82//9m+DaszPFO2/Mak57in/tkt7e7t27NihWbNmxayfNWuWamtrE1RV7+3bt0+FhYUqLi7WN77xDe3fv1+SVF9fr0AgENNPr9er6dOnu/3csWOHOjo6YtoUFhaqtLTUbbNlyxb5fD5NmzbNbXPTTTfJ5/MNmNvLZl+3bNmi0tJSFRYWum1uv/12hUIh7dixo1/7eSFvvPGGRo8erQkTJug73/mOmpqa3G2p1PdgMChJGj58uKTBNfZn9z0q1cc+HA5r48aNOnHihMrKygbVmEs9+x+VauM+4H5Yrq99/PHHCofDys/Pj1mfn5+vQCCQoKp6Z9q0aXr66ac1YcIEHTlyRI8//rjKy8tVV1fn9uVc/Txw4IAkKRAIKDMzU8OGDevRJnr+QCCg0aNH97ju0aNHD5jby2ZfA4FAj+sZNmyYMjMzE3Z7zJ49W1//+tc1fvx41dfX6x/+4R906623aseOHfJ6vSnTd2OMHnroIX32s59VaWmpW5OU+mN/rr5LqT32u3btUllZmU6dOqWhQ4fq+eef17XXXuu+MKb6mJ+v/1JqjnvKh48ox3FiThtjeqwb6GbPnu3+P3HiRJWVlenKK6/U+vXr3YOPetPPs9ucq/1AvL1s9XWg3R533XWX+39paammTp2q8ePH68UXX9TcuXPPe75k6/vixYv17rvv6ve//32Pbak+9ufreyqP/dVXX62dO3fq2LFjevbZZ7VgwQLV1NSct55UG/Pz9f/aa69NyXFP+bddRo4cqbS0tB6prampqUfCSzY5OTmaOHGi9u3b537q5UL99Pv9am9vV3Nz8wXbHDlypMd1ffTRRwPm9rLZV7/f3+N6mpub1dHRMWBuj4KCAo0fP1779u2TlBp9f+CBB/TCCy/o9ddf19ixY931g2Hsz9f3c0mlsc/MzNRVV12lqVOnqqqqSpMnT9ZPf/rTQTHm0vn7fy6pMO4pHz4yMzM1ZcoUVVdXx6yvrq5WeXl5gqrqG6FQSO+9954KCgpUXFwsv98f08/29nbV1NS4/ZwyZYoyMjJi2jQ2Nmr37t1um7KyMgWDQb311ltumz/84Q8KBoMD5vay2deysjLt3r1bjY2NbptNmzbJ6/VqypQp/drPi3X06FE1NDSooKBAUnL33RijxYsX67nnntNvf/tbFRcXx2xP5bH/pL6fSyqN/dmMMQqFQik95hcS7f+5pMS49+nhqwNU9KO2P//5z82ePXvMkiVLTE5Ojvnggw8SXVpcvv/975s33njD7N+/32zdutXceeedJjc31+3HihUrjM/nM88995zZtWuXufvuu8/5cbSxY8ea1157zbz99tvm1ltvPefHsSZNmmS2bNlitmzZYiZOnGj9o7atra3mnXfeMe+8846RZFatWmXeeecd9+PRtvoa/ejZbbfdZt5++23z2muvmbFjx/brR+8u1PfW1lbz/e9/39TW1pr6+nrz+uuvm7KyMjNmzJiU6Pt3v/td4/P5zBtvvBHzscKTJ0+6bVJ17D+p76k89suWLTObN2829fX15t133zUPP/yw8Xg8ZtOmTcaY1B3zi+l/qo77oAgfxhjzL//yL2b8+PEmMzPT3HDDDTEfX0sW0c+2Z2RkmMLCQjN37lxTV1fnbo9EIuaRRx4xfr/feL1e8/nPf97s2rUr5jLa2trM4sWLzfDhw01WVpa58847zcGDB2PaHD161MyfP9/k5uaa3NxcM3/+fNPc3Gyji67XX3/dSOqxLFiwwBhjt68HDhwwd9xxh8nKyjLDhw83ixcvNqdOnUpI30+ePGlmzZplRo0aZTIyMsy4cePMggULevQrWft+rn5LMuvWrXPbpOrYf1LfU3ns//Zv/9Z9fh41apS57bbb3OBhTOqOedSF+p+q4+4YY0zfzqUAAACcX8of8wEAAAYWwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACr/j9uN6Nq/vWdKQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losshistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e027874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\3908834773.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\3908834773.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\3908834773.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_12224\\3908834773.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "u00_1, v00_1, u00_2, v00_2 = model.predict1(test10_x,test10_t)\n",
    "u11_1, v11_1, u11_2, v11_2 = model.predict1(testb_x1,testb_t1)\n",
    "u22_1, v22_1, u22_2, v22_2 = model.predict1(testb_x2,testb_t2)\n",
    "a1,b1,c1,d1, ux11_1, vx11_1, ux11_2, vx11_2 = model.predict2(testb_x1,testb_t1)\n",
    "a1,b1,c1,d1, ux22_1, vx22_1, ux22_2, vx22_2 = model.predict2(testb_x2,testb_t2)\n",
    "\n",
    "f1, f2, f3, f4,a1,b1,c1,d1 = model.predict2(testu_x,testu_t)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c0224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_u0: 5.27684e-02, Error_ub: 0.00000e+00, Error_ubx: 1.67862e-23, Error_f: 9.41316e-08\n"
     ]
    }
   ],
   "source": [
    "error_u0= ((np.linalg.norm(test10_r - u00_1, 2))**2+(np.linalg.norm(test10_c - v00_1, 2))**2+(np.linalg.norm(test20_r - u00_2, 2))**2+(np.linalg.norm(test20_c - v00_2, 2))**2)/20\n",
    "error_ub= ((np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2+(np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2)/20\n",
    "error_ubx= (\n",
    "    (np.linalg.norm(ux11_1.cpu().detach().numpy() - ux22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_1.cpu().detach().numpy() - vx22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(ux11_2.cpu().detach().numpy() - ux22_2.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_2.cpu().detach().numpy() - vx22_2.cpu().detach().numpy(), 2))**2)/20\n",
    "\n",
    "error_f= ((np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2+(np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2)/2000\n",
    "print('Error_u0: %.5e, Error_ub: %.5e, Error_ubx: %.5e, Error_f: %.5e' % (error_u0.item(), error_ub.item(), error_ubx.item(), error_f.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
