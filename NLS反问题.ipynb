{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cdd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, optim \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import random\n",
    "from torch.utils import data\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e41b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dca358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=pd.read_csv(\"data_x_new.csv\")\n",
    "data_t=pd.read_csv(\"data_t_new.csv\")\n",
    "data_xx=pd.read_csv(\"data_x_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969f73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h10_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h10_t=data_x.iloc[:, [1]]\n",
    "h10_R=data_x.iloc[:,[2]]#初值h实部\n",
    "h10_C=data_x.iloc[:,[3]]#初值h虚部\n",
    "hb_x1=data_t.iloc[:,[1]]#边值-5，t\n",
    "hb_t1=data_t.iloc[:,[0]]\n",
    "hb_x2=data_t.iloc[:,[2]]#边值5，t\n",
    "hb_t2=data_t.iloc[:,[0]]\n",
    "\n",
    "h20_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h20_t=data_x.iloc[:, [1]]\n",
    "h20_R=data_x.iloc[:,[4]]#初值h实部\n",
    "h20_C=data_x.iloc[:,[5]]#初值h虚部\n",
    "\n",
    "h30_x=data_xx.iloc[:, [0]]\n",
    "h30_t=data_xx.iloc[:, [1]]\n",
    "h30_r1=data_xx.iloc[:, [2]]\n",
    "h30_c1=data_xx.iloc[:, [3]]\n",
    "h30_r2=data_xx.iloc[:, [4]]\n",
    "h30_c2=data_xx.iloc[:, [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bd3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x['x']\n",
    "t = data_t['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2ca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc71aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf= np.hstack((X.flatten()[:,None], T.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950258e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_x=hf[:,[0]]\n",
    "hf_t=hf[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ff78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train10_x,test10_x,train10_t,test10_t,train10_r,test10_r,train10_c,test10_c,trainb_x1,testb_x1,trainb_t1,testb_t1,trainb_x2,testb_x2,trainb_t2,testb_t2,train20_r,test20_r,train20_c,test20_c = train_test_split(h10_x,h10_t,h10_R,h10_C,hb_x1,hb_t1,hb_x2,hb_t2,h20_R,h20_C,test_size=0.2)\n",
    "\n",
    "train30_x,test30_x,train30_t,test30_t,train30_r1,test30_r1,train30_c1,test30_c1,train30_r2,test30_r2,train30_c2,test30_c2=train_test_split(h30_x,h30_t,h30_r1,h30_c1,h30_r2,h30_c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8301e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainu_x, testu_x, trainu_t, testu_t = train_test_split(hf_x, hf_t, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c1b3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train10_x=torch.from_numpy(train10_x.to_numpy()).float()\n",
    "train10_t=torch.from_numpy(train10_t.to_numpy()).float()\n",
    "\n",
    "train10_r=torch.from_numpy(train10_r.to_numpy()).float()\n",
    "train10_c=torch.from_numpy(train10_c.to_numpy()).float()\n",
    "\n",
    "trainb_x1=torch.from_numpy(trainb_x1.to_numpy()).float()\n",
    "trainb_t1=torch.from_numpy(trainb_t1.to_numpy()).float()\n",
    "trainb_x2=torch.from_numpy(trainb_x2.to_numpy()).float()\n",
    "trainb_t2=torch.from_numpy(trainb_t2.to_numpy()).float()\n",
    "\n",
    "# train20_x=torch.from_numpy(train20_x.to_numpy()).float()\n",
    "# train20_t=torch.from_numpy(train20_t.to_numpy()).float()\n",
    "train20_r=torch.from_numpy(train20_r.to_numpy()).float()\n",
    "train20_c=torch.from_numpy(train20_c.to_numpy()).float()\n",
    "\n",
    "\n",
    "train30_x=torch.from_numpy(train30_x.to_numpy()).float()\n",
    "train30_t=torch.from_numpy(train30_t.to_numpy()).float()\n",
    "train30_r1=torch.from_numpy(train30_r1.to_numpy()).float()\n",
    "train30_c1=torch.from_numpy(train30_c1.to_numpy()).float()\n",
    "train30_r2=torch.from_numpy(train30_r2.to_numpy()).float()\n",
    "train30_c2=torch.from_numpy(train30_c2.to_numpy()).float()\n",
    "\n",
    "trainu_x=torch.from_numpy(trainu_x).float()\n",
    "trainu_t=torch.from_numpy(trainu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e60ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10_x=torch.from_numpy(test10_x.to_numpy()).float()\n",
    "test10_t=torch.from_numpy(test10_t.to_numpy()).float()\n",
    "test10_r=torch.from_numpy(test10_r.to_numpy()).float()\n",
    "test10_c=torch.from_numpy(test10_c.to_numpy()).float()\n",
    "\n",
    "testb_x1=torch.from_numpy(testb_x1.to_numpy()).float()\n",
    "testb_t1=torch.from_numpy(testb_t1.to_numpy()).float()\n",
    "testb_x2=torch.from_numpy(testb_x2.to_numpy()).float()\n",
    "testb_t2=torch.from_numpy(testb_t2.to_numpy()).float()\n",
    "\n",
    "# test20_x=torch.from_numpy(test20_x.to_numpy()).float()\n",
    "# test20_t=torch.from_numpy(test20_t.to_numpy()).float()\n",
    "test20_r=torch.from_numpy(test20_r.to_numpy()).float()\n",
    "test20_c=torch.from_numpy(test20_c.to_numpy()).float()\n",
    "\n",
    "testu_x=torch.from_numpy(testu_x).float()\n",
    "testu_t=torch.from_numpy(testu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c0aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f5e4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN():\n",
    "    def __init__(self, u10_x, u10_t, u10_r, u10_c, ub_x1, ub_t1, ub_x2, ub_t2, u20_r, u20_c, uf_x, uf_t,u30_x,u30_t,u30_r1,u30_c1,u30_r2,u30_c2):\n",
    "        self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
    "        self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
    "        self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
    "        self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
    "        self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
    "        self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
    "        self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
    "        \n",
    "#         self.u20_x = u20_x.clone().detach().requires_grad_(True).float().to(device)\n",
    "#         self.u20_t = torch.tensor(u20_t, requires_grad=True).float().to(device)\n",
    "        self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
    "        self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
    "        self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
    "        self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
    "        self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
    "        self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
    "        self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
    "      \n",
    "        \n",
    "        self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
    "        self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.tensor([0.8], requires_grad=True).to(device)\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        \n",
    "        self.lambda_2 = torch.tensor([2.2], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        self.dnn = DNN().to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params':self.dnn.parameters()},\n",
    "            {'params': [self.lambda_1], 'lr': 0.00015},\n",
    "            {'params': [self.lambda_2], 'lr': 0.0001}\n",
    "        ],lr=0.012 )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "        self.losshistory=[]\n",
    "    def net_u(self, x, t):  \n",
    "        u1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,0],1)\n",
    "        v1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,1],1)\n",
    "        u2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,2],1)\n",
    "        v2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,3],1)\n",
    "        return u1, v1, u2, v2\n",
    "\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        beta=1\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        lambda_3 = -1\n",
    "        lambda_4 = 0\n",
    "        u1, v1,u2, v2= self.net_u(x, t)\n",
    "        \n",
    "        u1_t = torch.autograd.grad(\n",
    "            u1, t, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_x = torch.autograd.grad(\n",
    "            u1, x, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_xx = torch.autograd.grad(\n",
    "            u1_x, x, \n",
    "            grad_outputs=torch.ones_like(u1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v1_t = torch.autograd.grad(\n",
    "            v1, t, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_x = torch.autograd.grad(\n",
    "            v1, x, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_xx = torch.autograd.grad(\n",
    "            v1_x, x, \n",
    "            grad_outputs=torch.ones_like(v1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        u2_t = torch.autograd.grad(\n",
    "            u2, t, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_x = torch.autograd.grad(\n",
    "            u2, x, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_xx = torch.autograd.grad(\n",
    "            u2_x, x, \n",
    "            grad_outputs=torch.ones_like(u2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v2_t = torch.autograd.grad(\n",
    "            v2, t, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_x = torch.autograd.grad(\n",
    "            v2, x, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_xx = torch.autograd.grad(\n",
    "            v2_x, x, \n",
    "            grad_outputs=torch.ones_like(v2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f_u1 = (\n",
    "            v1_t + u1_xx\n",
    "        + lambda_1*u1*(u1**2 + v1**2) + lambda_2*u1*(u2**2 + v2**2) + lambda_3*(u1*(u2**2 - v2**2) + 2*u2*v1*v2) + lambda_4*(u2*(u1**2 - v1**2) + 2*u1*v1*v2)\n",
    "    )\n",
    "\n",
    "        f_v1 = (\n",
    "                -u1_t + v1_xx\n",
    "            + lambda_1*v1*(u1**2 + v1**2) + lambda_2*v1*(u2**2 + v2**2) - lambda_3*(v1*(u2**2 - v2**2) - 2*u1*u2*v2) - lambda_4*(v2*(u1**2 - v1**2) - 2*u1*u2*v1)\n",
    "        )\n",
    "\n",
    "        f_u2 = (\n",
    "                v2_t + beta*u2_xx\n",
    "            + lambda_1*u2*(u2**2 + v2**2) + lambda_2*u2*(u1**2 + v1**2) + lambda_3*(u2*(u1**2 - v1**2) + 2*u1*v2*v1) + lambda_4*(u1*(u2**2 - v2**2) + 2*u2*v2*v1)\n",
    "        )\n",
    "\n",
    "        f_v2 = (\n",
    "                -u2_t + beta*v2_xx\n",
    "            + lambda_1*v2*(u2**2 + v2**2) + lambda_2*v2*(u1**2 + v1**2) - lambda_3*(v2*(u1**2 - v1**2) - 2*u2*u1*v1) - lambda_4*(v1*(u2**2 - v2**2) - 2*u2*u1*v2)\n",
    "        )\n",
    "        a=u1_x\n",
    "        b=v1_x\n",
    "        c=u2_x\n",
    "        d=v2_x\n",
    "        return f_u1, f_v1, f_u2, f_v2, a, b, c, d\n",
    "     \n",
    "    def loss_func(self):\n",
    "        for self.iter in tqdm(range(50000)):\n",
    "            torch.cuda.empty_cache()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            u10_pred, v10_pred, u20_pred, v20_pred = self.net_u(self.u10_x,self.u10_t)\n",
    "            u30_pred1, v30_pred1, u30_pred2, v30_pred2 = self.net_u(self.u30_x,self.u30_t)\n",
    "            u1b_pred1, v1b_pred1, u2b_pred1, v2b_pred1 = self.net_u(self.ub_x1,self.ub_t1)\n",
    "            u1b_pred2, v1b_pred2, u2b_pred2, v2b_pred2 = self .net_u(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            \n",
    "            a1,b1,c1,d1,u1bx_pred1, v1bx_pred1,u2bx_pred1, v2bx_pred1 = self.net_f(self.ub_x1,self.ub_t1)\n",
    "            a2,b2,c2,d2,u1bx_pred2, v1bx_pred2,u2bx_pred2, v2bx_pred2 = self.net_f(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            f_predu1, f_predv1, f_predu2, f_predv2,a1,b1,c1,d1 = self.net_f(self.uf_x,self.uf_t)\n",
    "            \n",
    "            loss_u0 = torch.mean((self.u10_r - u10_pred) ** 2)+torch.mean((self.u10_c - v10_pred) ** 2)+torch.mean((self.u20_r - u20_pred) ** 2)+torch.mean((self.u20_c - v20_pred) ** 2)\n",
    "            loss_ub = torch.mean((u1b_pred1 - u1b_pred2) ** 2)+torch.mean((v1b_pred1 - v1b_pred2) ** 2)+torch.mean((u2b_pred1 - u2b_pred2) ** 2)+torch.mean((v2b_pred1 - v2b_pred2) ** 2)\n",
    "            loss_ubx = torch.mean((u1bx_pred1 - u1bx_pred2) ** 2)+torch.mean((v1bx_pred1 - v1bx_pred2) ** 2)+torch.mean((u2bx_pred1 - u2bx_pred2) ** 2)+torch.mean((v2bx_pred1 - v2bx_pred2) ** 2)\n",
    "            loss_f = torch.mean(f_predu1 ** 2)+torch.mean(f_predv1 ** 2)+torch.mean(f_predu2 ** 2)+torch.mean(f_predv2 ** 2)\n",
    "            loss_u1=torch.mean((self.u30_r1 - u30_pred1) ** 2)+torch.mean((self.u30_c1 - v30_pred1) ** 2)+torch.mean((self.u30_r2 - u30_pred2) ** 2)+torch.mean((self.u30_c2 - v30_pred2) ** 2)\n",
    "            \n",
    "            loss = loss_f+loss_u0 + loss_ub + loss_ubx+loss_u1\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losshistory.append(loss.clone().detach().cpu())\n",
    "            \n",
    "            self.iter += 1\n",
    "            with torch.no_grad():\n",
    "                if self.iter % 500 == 0:\n",
    "                    print('Iter %d, Loss: %.5e, Loss_u0: %.5e, Loss_ub: %.5e, Loss_ubx: %.5e, Loss_f: %.5e, Loss_u1: %.5e, lambda1: %.5e, lambda2: %.5e' % (self.iter, loss.item(), loss_u0.item(), loss_ub.item(), loss_ubx.item(), loss_f.item(), loss_u1.item(), self.lambda_1,self.lambda_2))\n",
    "        return float(loss)\n",
    "        \n",
    " \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "        \n",
    "    def predict1(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u1, v1 , u2, v2= self.net_u(x, t)\n",
    "        u1 = u1.detach().cpu().numpy()\n",
    "        v1 = v1.detach().cpu().numpy()\n",
    "        u2 = u2.detach().cpu().numpy()\n",
    "        v2 = v2.detach().cpu().numpy()\n",
    "        return u1, v1, u2, v2\n",
    "    \n",
    "    def predict2(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        f1, f2, f3, f4, a, b, c, d = self.net_f(x, t)\n",
    "        f1 = f1.detach().cpu().numpy()\n",
    "        f2 = f2.detach().cpu().numpy()\n",
    "        f3 = f3.detach().cpu().numpy()\n",
    "        f4 = f4.detach().cpu().numpy()\n",
    "        return  f1, f2, f3, f4, a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08905a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PhysicsInformedNN(train10_x, train10_t, train10_r, train10_c, \n",
    "                          trainb_x1, trainb_t1, trainb_x2, trainb_t2,\n",
    "                        train20_r, train20_c, trainu_x, trainu_t,\n",
    "                         train30_x,train30_t, train30_r1, train30_c1,train30_r2,train30_c2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7626fd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/50000 [01:05<1:45:29,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500, Loss: 1.67335e-01, Loss_u0: 1.78783e-02, Loss_ub: 1.04701e-04, Loss_ubx: 8.28038e-05, Loss_f: 4.55057e-02, Loss_u1: 1.03763e-01, lambda1: 8.69343e-01, lambda2: 2.19461e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1001/50000 [02:08<1:43:59,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Loss: 1.30426e-01, Loss_u0: 1.04473e-02, Loss_ub: 1.85365e-05, Loss_ubx: 3.29030e-05, Loss_f: 3.49517e-02, Loss_u1: 8.49756e-02, lambda1: 9.08982e-01, lambda2: 2.16370e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1501/50000 [03:11<1:36:34,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1500, Loss: 1.24294e-01, Loss_u0: 5.93363e-03, Loss_ub: 4.46028e-05, Loss_ubx: 6.30569e-05, Loss_f: 3.94137e-02, Loss_u1: 7.88386e-02, lambda1: 9.30728e-01, lambda2: 2.13804e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2001/50000 [04:13<1:35:42,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2000, Loss: 9.10115e-02, Loss_u0: 2.21995e-03, Loss_ub: 3.30727e-05, Loss_ubx: 2.73726e-05, Loss_f: 2.22583e-02, Loss_u1: 6.64728e-02, lambda1: 9.48466e-01, lambda2: 2.12160e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2501/50000 [05:15<1:38:44,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2500, Loss: 9.21569e-02, Loss_u0: 1.92125e-03, Loss_ub: 9.04268e-05, Loss_ubx: 2.92215e-05, Loss_f: 2.70866e-02, Loss_u1: 6.30294e-02, lambda1: 9.54424e-01, lambda2: 2.11456e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3001/50000 [06:16<1:33:39,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3000, Loss: 1.11812e-01, Loss_u0: 2.96672e-03, Loss_ub: 2.57397e-05, Loss_ubx: 1.73596e-05, Loss_f: 2.83167e-02, Loss_u1: 8.04855e-02, lambda1: 9.71963e-01, lambda2: 2.10435e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3501/50000 [07:17<1:35:42,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3500, Loss: 1.31511e-01, Loss_u0: 3.64291e-03, Loss_ub: 7.92457e-05, Loss_ubx: 1.36237e-04, Loss_f: 3.61432e-02, Loss_u1: 9.15090e-02, lambda1: 9.69926e-01, lambda2: 2.09505e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4001/50000 [08:18<1:31:54,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4000, Loss: 8.97600e-02, Loss_u0: 2.44458e-03, Loss_ub: 2.19742e-04, Loss_ubx: 1.98744e-04, Loss_f: 2.07765e-02, Loss_u1: 6.61205e-02, lambda1: 9.62634e-01, lambda2: 2.08820e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4501/50000 [09:19<1:34:57,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4500, Loss: 7.01097e-02, Loss_u0: 2.23574e-03, Loss_ub: 1.14524e-04, Loss_ubx: 7.35547e-05, Loss_f: 1.81941e-02, Loss_u1: 4.94917e-02, lambda1: 9.51975e-01, lambda2: 2.08058e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5001/50000 [10:21<1:28:02,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5000, Loss: 1.19326e-01, Loss_u0: 3.64014e-03, Loss_ub: 2.15847e-05, Loss_ubx: 2.66521e-05, Loss_f: 2.82105e-02, Loss_u1: 8.74270e-02, lambda1: 9.78531e-01, lambda2: 2.06970e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5501/50000 [11:22<1:29:17,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5500, Loss: 9.36895e-02, Loss_u0: 2.13016e-03, Loss_ub: 5.72643e-05, Loss_ubx: 3.57525e-05, Loss_f: 2.32292e-02, Loss_u1: 6.82371e-02, lambda1: 9.86616e-01, lambda2: 2.05298e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6001/50000 [12:24<1:30:50,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6000, Loss: 1.45186e-01, Loss_u0: 5.11802e-03, Loss_ub: 9.09522e-03, Loss_ubx: 6.02943e-04, Loss_f: 4.87627e-02, Loss_u1: 8.16069e-02, lambda1: 9.82706e-01, lambda2: 2.04491e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6501/50000 [13:27<1:37:32,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6500, Loss: 7.86301e-02, Loss_u0: 1.77008e-03, Loss_ub: 2.52636e-04, Loss_ubx: 4.49800e-05, Loss_f: 2.11139e-02, Loss_u1: 5.54485e-02, lambda1: 9.76469e-01, lambda2: 2.03843e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7001/50000 [14:30<1:25:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7000, Loss: 6.33205e-02, Loss_u0: 9.41432e-04, Loss_ub: 6.91865e-05, Loss_ubx: 7.72303e-06, Loss_f: 1.62889e-02, Loss_u1: 4.60132e-02, lambda1: 9.68132e-01, lambda2: 2.02939e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 7501/50000 [15:33<1:27:33,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7500, Loss: 2.21229e-01, Loss_u0: 5.00155e-03, Loss_ub: 7.36229e-04, Loss_ubx: 5.36335e-05, Loss_f: 3.72328e-02, Loss_u1: 1.78204e-01, lambda1: 9.61383e-01, lambda2: 2.02199e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8001/50000 [16:36<1:30:22,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8000, Loss: 1.71317e-01, Loss_u0: 3.82439e-03, Loss_ub: 5.49754e-05, Loss_ubx: 8.92537e-06, Loss_f: 3.39641e-02, Loss_u1: 1.33465e-01, lambda1: 9.65554e-01, lambda2: 2.02186e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 8501/50000 [17:39<1:29:34,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8500, Loss: 1.26279e-01, Loss_u0: 2.21592e-03, Loss_ub: 3.88260e-05, Loss_ubx: 1.45269e-05, Loss_f: 2.05334e-02, Loss_u1: 1.03476e-01, lambda1: 9.68062e-01, lambda2: 2.02222e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9001/50000 [18:42<1:24:52,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9000, Loss: 1.27763e-01, Loss_u0: 2.44897e-03, Loss_ub: 1.20014e-04, Loss_ubx: 2.86750e-05, Loss_f: 3.70730e-02, Loss_u1: 8.80920e-02, lambda1: 9.68630e-01, lambda2: 2.02230e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 9501/50000 [19:44<1:24:27,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9500, Loss: 8.97550e-02, Loss_u0: 1.48855e-03, Loss_ub: 2.04828e-04, Loss_ubx: 1.25384e-05, Loss_f: 2.03646e-02, Loss_u1: 6.76845e-02, lambda1: 9.67654e-01, lambda2: 2.02239e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10001/50000 [20:47<1:21:27,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10000, Loss: 8.47629e-02, Loss_u0: 1.39500e-03, Loss_ub: 2.55467e-05, Loss_ubx: 1.40961e-05, Loss_f: 2.27547e-02, Loss_u1: 6.05735e-02, lambda1: 9.66401e-01, lambda2: 2.02242e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10501/50000 [21:50<1:21:31,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10500, Loss: 7.52666e-02, Loss_u0: 9.52727e-04, Loss_ub: 3.43063e-05, Loss_ubx: 1.35078e-05, Loss_f: 1.67653e-02, Loss_u1: 5.75007e-02, lambda1: 9.64409e-01, lambda2: 2.02168e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11001/50000 [22:53<1:23:44,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11000, Loss: 6.30270e-02, Loss_u0: 9.04669e-04, Loss_ub: 2.92673e-04, Loss_ubx: 1.73267e-05, Loss_f: 1.65880e-02, Loss_u1: 4.52244e-02, lambda1: 9.62052e-01, lambda2: 2.02050e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 11501/50000 [23:56<1:21:04,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11500, Loss: 6.90241e-02, Loss_u0: 9.57609e-04, Loss_ub: 4.23906e-04, Loss_ubx: 8.60578e-05, Loss_f: 2.70093e-02, Loss_u1: 4.05472e-02, lambda1: 9.58603e-01, lambda2: 2.01816e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12001/50000 [25:00<1:18:47,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12000, Loss: 5.19356e-02, Loss_u0: 5.41925e-04, Loss_ub: 9.96174e-05, Loss_ubx: 1.10043e-05, Loss_f: 1.40650e-02, Loss_u1: 3.72180e-02, lambda1: 9.53960e-01, lambda2: 2.01507e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 12501/50000 [26:03<1:25:37,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12500, Loss: 6.03052e-02, Loss_u0: 9.82426e-04, Loss_ub: 2.38075e-04, Loss_ubx: 3.50245e-06, Loss_f: 1.41355e-02, Loss_u1: 4.49457e-02, lambda1: 9.48959e-01, lambda2: 2.01214e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13001/50000 [27:07<1:18:05,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13000, Loss: 5.31005e-02, Loss_u0: 7.44299e-04, Loss_ub: 7.42978e-05, Loss_ubx: 1.84417e-05, Loss_f: 1.39614e-02, Loss_u1: 3.83021e-02, lambda1: 9.41602e-01, lambda2: 2.00655e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 13501/50000 [28:10<1:20:29,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13500, Loss: 5.03027e-02, Loss_u0: 5.76263e-04, Loss_ub: 2.57474e-04, Loss_ubx: 1.27499e-05, Loss_f: 1.37411e-02, Loss_u1: 3.57152e-02, lambda1: 9.32817e-01, lambda2: 1.99914e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14001/50000 [29:13<1:17:06,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14000, Loss: 4.26479e-02, Loss_u0: 5.25515e-04, Loss_ub: 6.10278e-05, Loss_ubx: 5.17926e-06, Loss_f: 9.74425e-03, Loss_u1: 3.23119e-02, lambda1: 9.22966e-01, lambda2: 1.99077e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 14501/50000 [30:17<1:15:09,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14500, Loss: 1.74207e+00, Loss_u0: 7.47613e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 2.36381e-01, Loss_u1: 7.58072e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15001/50000 [31:20<1:13:14,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15000, Loss: 1.73099e+00, Loss_u0: 7.44860e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 2.31296e-01, Loss_u1: 7.54834e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 15501/50000 [32:24<1:10:03,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15500, Loss: 1.71714e+00, Loss_u0: 7.41382e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 2.25015e-01, Loss_u1: 7.50744e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16001/50000 [33:27<1:08:41,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16000, Loss: 1.69990e+00, Loss_u0: 7.37004e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 2.17304e-01, Loss_u1: 7.45597e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16501/50000 [34:30<1:08:54,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16500, Loss: 1.67861e+00, Loss_u0: 7.31524e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 2.07934e-01, Loss_u1: 7.39155e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17001/50000 [35:32<1:08:11,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17000, Loss: 1.65255e+00, Loss_u0: 7.24709e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 1.96700e-01, Loss_u1: 7.31146e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 17501/50000 [36:35<1:08:44,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17500, Loss: 1.62102e+00, Loss_u0: 7.16299e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 1.83458e-01, Loss_u1: 7.21265e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18001/50000 [37:38<1:05:27,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18000, Loss: 1.58338e+00, Loss_u0: 7.06021e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 1.68171e-01, Loss_u1: 7.09192e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 18501/50000 [38:41<1:04:55,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18500, Loss: 1.53917e+00, Loss_u0: 6.93599e-01, Loss_ub: 0.00000e+00, Loss_ubx: 0.00000e+00, Loss_f: 1.50960e-01, Loss_u1: 6.94608e-01, lambda1: 9.06167e-01, lambda2: 1.98119e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 18819/50000 [39:21<1:05:13,  7.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py\u001b[0m in \u001b[0;36mloss_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu1bx_pred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1bx_pred1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu2bx_pred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2bx_pred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_x1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_t1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m             \u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu1bx_pred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1bx_pred2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu2bx_pred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2bx_pred2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_x2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mub_t2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28096\\2713112350.py\u001b[0m in \u001b[0;36mnet_f\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    113\u001b[0m         u2_xx = torch.autograd.grad(\n\u001b[0;32m    114\u001b[0m             \u001b[0mu2_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu2_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.losshistory=[]            \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "117e3d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26833a9ea00>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losshistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e027874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "u00_1, v00_1, u00_2, v00_2 = model.predict1(test10_x,test10_t)\n",
    "u11_1, v11_1, u11_2, v11_2 = model.predict1(testb_x1,testb_t1)\n",
    "u22_1, v22_1, u22_2, v22_2 = model.predict1(testb_x2,testb_t2)\n",
    "a1,b1,c1,d1, ux11_1, vx11_1, ux11_2, vx11_2 = model.predict2(testb_x1,testb_t1)\n",
    "a1,b1,c1,d1, ux22_1, vx22_1, ux22_2, vx22_2 = model.predict2(testb_x2,testb_t2)\n",
    "\n",
    "f1, f2, f3, f4,a1,b1,c1,d1 = model.predict2(testu_x,testu_t)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c0224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_u0: 5.27684e-02, Error_ub: 0.00000e+00, Error_ubx: 4.74056e-67, Error_f: 8.12052e-08\n"
     ]
    }
   ],
   "source": [
    "error_u0= ((np.linalg.norm(test10_r - u00_1, 2))**2+(np.linalg.norm(test10_c - v00_1, 2))**2+(np.linalg.norm(test20_r - u00_2, 2))**2+(np.linalg.norm(test20_c - v00_2, 2))**2)/20\n",
    "error_ub= ((np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2+(np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2)/20\n",
    "error_ubx= (\n",
    "    (np.linalg.norm(ux11_1.cpu().detach().numpy() - ux22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_1.cpu().detach().numpy() - vx22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(ux11_2.cpu().detach().numpy() - ux22_2.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_2.cpu().detach().numpy() - vx22_2.cpu().detach().numpy(), 2))**2)/20\n",
    "\n",
    "error_f= ((np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2+(np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2)/2000\n",
    "print('Error_u0: %.5e, Error_ub: %.5e, Error_ubx: %.5e, Error_f: %.5e' % (error_u0.item(), error_ub.item(), error_ubx.item(), error_f.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
