{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cdd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, optim \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import random\n",
    "from torch.utils import data\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e41b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dca358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=pd.read_csv(\"data_x_new.csv\")\n",
    "data_t=pd.read_csv(\"data_t_new.csv\")\n",
    "data_xx=pd.read_csv(\"data_x_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969f73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h10_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h10_t=data_x.iloc[:, [1]]\n",
    "h10_R=data_x.iloc[:,[2]]#初值h实部\n",
    "h10_C=data_x.iloc[:,[3]]#初值h虚部\n",
    "hb_x1=data_t.iloc[:,[1]]#边值-5，t\n",
    "hb_t1=data_t.iloc[:,[0]]\n",
    "hb_x2=data_t.iloc[:,[2]]#边值5，t\n",
    "hb_t2=data_t.iloc[:,[0]]\n",
    "\n",
    "h20_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h20_t=data_x.iloc[:, [1]]\n",
    "h20_R=data_x.iloc[:,[4]]#初值h实部\n",
    "h20_C=data_x.iloc[:,[5]]#初值h虚部\n",
    "\n",
    "h30_x=data_xx.iloc[:, [0]]\n",
    "h30_t=data_xx.iloc[:, [1]]\n",
    "h30_r1=data_xx.iloc[:, [2]]\n",
    "h30_c1=data_xx.iloc[:, [3]]\n",
    "h30_r2=data_xx.iloc[:, [4]]\n",
    "h30_c2=data_xx.iloc[:, [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bd3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x['x']\n",
    "t = data_t['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2ca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc71aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf= np.hstack((X.flatten()[:,None], T.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950258e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_x=hf[:,[0]]\n",
    "hf_t=hf[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ff78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train10_x,test10_x,train10_t,test10_t,train10_r,test10_r,train10_c,test10_c,trainb_x1,testb_x1,trainb_t1,testb_t1,trainb_x2,testb_x2,trainb_t2,testb_t2,train20_r,test20_r,train20_c,test20_c = train_test_split(h10_x,h10_t,h10_R,h10_C,hb_x1,hb_t1,hb_x2,hb_t2,h20_R,h20_C,test_size=0.2)\n",
    "\n",
    "train30_x,test30_x,train30_t,test30_t,train30_r1,test30_r1,train30_c1,test30_c1,train30_r2,test30_r2,train30_c2,test30_c2=train_test_split(h30_x,h30_t,h30_r1,h30_c1,h30_r2,h30_c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8301e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainu_x, testu_x, trainu_t, testu_t = train_test_split(hf_x, hf_t, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c1b3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train10_x=torch.from_numpy(train10_x.to_numpy()).float()\n",
    "train10_t=torch.from_numpy(train10_t.to_numpy()).float()\n",
    "\n",
    "train10_r=torch.from_numpy(train10_r.to_numpy()).float()\n",
    "train10_c=torch.from_numpy(train10_c.to_numpy()).float()\n",
    "\n",
    "trainb_x1=torch.from_numpy(trainb_x1.to_numpy()).float()\n",
    "trainb_t1=torch.from_numpy(trainb_t1.to_numpy()).float()\n",
    "trainb_x2=torch.from_numpy(trainb_x2.to_numpy()).float()\n",
    "trainb_t2=torch.from_numpy(trainb_t2.to_numpy()).float()\n",
    "\n",
    "# train20_x=torch.from_numpy(train20_x.to_numpy()).float()\n",
    "# train20_t=torch.from_numpy(train20_t.to_numpy()).float()\n",
    "train20_r=torch.from_numpy(train20_r.to_numpy()).float()\n",
    "train20_c=torch.from_numpy(train20_c.to_numpy()).float()\n",
    "\n",
    "\n",
    "train30_x=torch.from_numpy(train30_x.to_numpy()).float()\n",
    "train30_t=torch.from_numpy(train30_t.to_numpy()).float()\n",
    "train30_r1=torch.from_numpy(train30_r1.to_numpy()).float()\n",
    "train30_c1=torch.from_numpy(train30_c1.to_numpy()).float()\n",
    "train30_r2=torch.from_numpy(train30_r2.to_numpy()).float()\n",
    "train30_c2=torch.from_numpy(train30_c2.to_numpy()).float()\n",
    "\n",
    "trainu_x=torch.from_numpy(trainu_x).float()\n",
    "trainu_t=torch.from_numpy(trainu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e60ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10_x=torch.from_numpy(test10_x.to_numpy()).float()\n",
    "test10_t=torch.from_numpy(test10_t.to_numpy()).float()\n",
    "test10_r=torch.from_numpy(test10_r.to_numpy()).float()\n",
    "test10_c=torch.from_numpy(test10_c.to_numpy()).float()\n",
    "\n",
    "testb_x1=torch.from_numpy(testb_x1.to_numpy()).float()\n",
    "testb_t1=torch.from_numpy(testb_t1.to_numpy()).float()\n",
    "testb_x2=torch.from_numpy(testb_x2.to_numpy()).float()\n",
    "testb_t2=torch.from_numpy(testb_t2.to_numpy()).float()\n",
    "\n",
    "# test20_x=torch.from_numpy(test20_x.to_numpy()).float()\n",
    "# test20_t=torch.from_numpy(test20_t.to_numpy()).float()\n",
    "test20_r=torch.from_numpy(test20_r.to_numpy()).float()\n",
    "test20_c=torch.from_numpy(test20_c.to_numpy()).float()\n",
    "\n",
    "testu_x=torch.from_numpy(testu_x).float()\n",
    "testu_t=torch.from_numpy(testu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c0aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f5e4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN():\n",
    "    def __init__(self, u10_x, u10_t, u10_r, u10_c, ub_x1, ub_t1, ub_x2, ub_t2, u20_r, u20_c, uf_x, uf_t,u30_x,u30_t,u30_r1,u30_c1,u30_r2,u30_c2):\n",
    "        self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
    "        self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
    "        self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
    "        self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
    "        self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
    "        self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
    "        self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
    "        \n",
    "#         self.u20_x = u20_x.clone().detach().requires_grad_(True).float().to(device)\n",
    "#         self.u20_t = torch.tensor(u20_t, requires_grad=True).float().to(device)\n",
    "        self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
    "        self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
    "        self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
    "        self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
    "        self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
    "        self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
    "        self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
    "      \n",
    "        \n",
    "        self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
    "        self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.tensor([0.8], requires_grad=True).to(device)\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        \n",
    "        self.lambda_2 = torch.tensor([2.2], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        self.dnn = DNN().to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params':self.dnn.parameters()},\n",
    "            {'params': [self.lambda_1], 'lr': 0.00021},\n",
    "            {'params': [self.lambda_2], 'lr': 0.0001}\n",
    "        ],lr=0.012 )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "        self.losshistory=[]\n",
    "    def net_u(self, x, t):  \n",
    "        u1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,0],1)\n",
    "        v1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,1],1)\n",
    "        u2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,2],1)\n",
    "        v2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,3],1)\n",
    "        return u1, v1, u2, v2\n",
    "\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        beta=1\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        lambda_3 = -1\n",
    "        lambda_4 = 0\n",
    "        u1, v1,u2, v2= self.net_u(x, t)\n",
    "        \n",
    "        u1_t = torch.autograd.grad(\n",
    "            u1, t, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_x = torch.autograd.grad(\n",
    "            u1, x, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_xx = torch.autograd.grad(\n",
    "            u1_x, x, \n",
    "            grad_outputs=torch.ones_like(u1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v1_t = torch.autograd.grad(\n",
    "            v1, t, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_x = torch.autograd.grad(\n",
    "            v1, x, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_xx = torch.autograd.grad(\n",
    "            v1_x, x, \n",
    "            grad_outputs=torch.ones_like(v1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        u2_t = torch.autograd.grad(\n",
    "            u2, t, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_x = torch.autograd.grad(\n",
    "            u2, x, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_xx = torch.autograd.grad(\n",
    "            u2_x, x, \n",
    "            grad_outputs=torch.ones_like(u2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v2_t = torch.autograd.grad(\n",
    "            v2, t, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_x = torch.autograd.grad(\n",
    "            v2, x, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_xx = torch.autograd.grad(\n",
    "            v2_x, x, \n",
    "            grad_outputs=torch.ones_like(v2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f_u1 = (\n",
    "            v1_t + u1_xx\n",
    "        + lambda_1*u1*(u1**2 + v1**2) + lambda_2*u1*(u2**2 + v2**2) + lambda_3*(u1*(u2**2 - v2**2) + 2*u2*v1*v2) + lambda_4*(u2*(u1**2 - v1**2) + 2*u1*v1*v2)\n",
    "    )\n",
    "\n",
    "        f_v1 = (\n",
    "                -u1_t + v1_xx\n",
    "            + lambda_1*v1*(u1**2 + v1**2) + lambda_2*v1*(u2**2 + v2**2) - lambda_3*(v1*(u2**2 - v2**2) - 2*u1*u2*v2) - lambda_4*(v2*(u1**2 - v1**2) - 2*u1*u2*v1)\n",
    "        )\n",
    "\n",
    "        f_u2 = (\n",
    "                v2_t + beta*u2_xx\n",
    "            + lambda_1*u2*(u2**2 + v2**2) + lambda_2*u2*(u1**2 + v1**2) + lambda_3*(u2*(u1**2 - v1**2) + 2*u1*v2*v1) + lambda_4*(u1*(u2**2 - v2**2) + 2*u2*v2*v1)\n",
    "        )\n",
    "\n",
    "        f_v2 = (\n",
    "                -u2_t + beta*v2_xx\n",
    "            + lambda_1*v2*(u2**2 + v2**2) + lambda_2*v2*(u1**2 + v1**2) - lambda_3*(v2*(u1**2 - v1**2) - 2*u2*u1*v1) - lambda_4*(v1*(u2**2 - v2**2) - 2*u2*u1*v2)\n",
    "        )\n",
    "        a=u1_x\n",
    "        b=v1_x\n",
    "        c=u2_x\n",
    "        d=v2_x\n",
    "        return f_u1, f_v1, f_u2, f_v2, a, b, c, d\n",
    "     \n",
    "    def loss_func(self):\n",
    "        for self.iter in tqdm(range(50000)):\n",
    "            torch.cuda.empty_cache()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            u10_pred, v10_pred, u20_pred, v20_pred = self.net_u(self.u10_x,self.u10_t)\n",
    "            u30_pred1, v30_pred1, u30_pred2, v30_pred2 = self.net_u(self.u30_x,self.u30_t)\n",
    "            u1b_pred1, v1b_pred1, u2b_pred1, v2b_pred1 = self.net_u(self.ub_x1,self.ub_t1)\n",
    "            u1b_pred2, v1b_pred2, u2b_pred2, v2b_pred2 = self .net_u(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            \n",
    "            a1,b1,c1,d1,u1bx_pred1, v1bx_pred1,u2bx_pred1, v2bx_pred1 = self.net_f(self.ub_x1,self.ub_t1)\n",
    "            a2,b2,c2,d2,u1bx_pred2, v1bx_pred2,u2bx_pred2, v2bx_pred2 = self.net_f(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            f_predu1, f_predv1, f_predu2, f_predv2,a1,b1,c1,d1 = self.net_f(self.uf_x,self.uf_t)\n",
    "            \n",
    "            loss_u0 = torch.mean((self.u10_r - u10_pred) ** 2)+torch.mean((self.u10_c - v10_pred) ** 2)+torch.mean((self.u20_r - u20_pred) ** 2)+torch.mean((self.u20_c - v20_pred) ** 2)\n",
    "            loss_ub = torch.mean((u1b_pred1 - u1b_pred2) ** 2)+torch.mean((v1b_pred1 - v1b_pred2) ** 2)+torch.mean((u2b_pred1 - u2b_pred2) ** 2)+torch.mean((v2b_pred1 - v2b_pred2) ** 2)\n",
    "            loss_ubx = torch.mean((u1bx_pred1 - u1bx_pred2) ** 2)+torch.mean((v1bx_pred1 - v1bx_pred2) ** 2)+torch.mean((u2bx_pred1 - u2bx_pred2) ** 2)+torch.mean((v2bx_pred1 - v2bx_pred2) ** 2)\n",
    "            loss_f = torch.mean(f_predu1 ** 2)+torch.mean(f_predv1 ** 2)+torch.mean(f_predu2 ** 2)+torch.mean(f_predv2 ** 2)\n",
    "            loss_u1=torch.mean((self.u30_r1 - u30_pred1) ** 2)+torch.mean((self.u30_c1 - v30_pred1) ** 2)+torch.mean((self.u30_r2 - u30_pred2) ** 2)+torch.mean((self.u30_c2 - v30_pred2) ** 2)\n",
    "            \n",
    "            loss = loss_f+loss_u0 + loss_ub + loss_ubx+loss_u1\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losshistory.append(loss.clone().detach().cpu())\n",
    "            \n",
    "            self.iter += 1\n",
    "            with torch.no_grad():\n",
    "                if self.iter % 500 == 0:\n",
    "                    print('Iter %d, Loss: %.5e, Loss_u0: %.5e, Loss_ub: %.5e, Loss_ubx: %.5e, Loss_f: %.5e, Loss_u1: %.5e, lambda1: %.5e, lambda2: %.5e' % (self.iter, loss.item(), loss_u0.item(), loss_ub.item(), loss_ubx.item(), loss_f.item(), loss_u1.item(), self.lambda_1,self.lambda_2))\n",
    "        return float(loss)\n",
    "        \n",
    " \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "        \n",
    "    def predict1(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u1, v1 , u2, v2= self.net_u(x, t)\n",
    "        u1 = u1.detach().cpu().numpy()\n",
    "        v1 = v1.detach().cpu().numpy()\n",
    "        u2 = u2.detach().cpu().numpy()\n",
    "        v2 = v2.detach().cpu().numpy()\n",
    "        return u1, v1, u2, v2\n",
    "    \n",
    "    def predict2(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        f1, f2, f3, f4, a, b, c, d = self.net_f(x, t)\n",
    "        f1 = f1.detach().cpu().numpy()\n",
    "        f2 = f2.detach().cpu().numpy()\n",
    "        f3 = f3.detach().cpu().numpy()\n",
    "        f4 = f4.detach().cpu().numpy()\n",
    "        return  f1, f2, f3, f4, a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08905a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PhysicsInformedNN(train10_x, train10_t, train10_r, train10_c, \n",
    "                          trainb_x1, trainb_t1, trainb_x2, trainb_t2,\n",
    "                        train20_r, train20_c, trainu_x, trainu_t,\n",
    "                         train30_x,train30_t, train30_r1, train30_c1,train30_r2,train30_c2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7626fd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/50000 [01:06<1:37:22,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500, Loss: 1.53383e-01, Loss_u0: 1.43724e-02, Loss_ub: 1.68749e-05, Loss_ubx: 1.30587e-05, Loss_f: 4.54135e-02, Loss_u1: 9.35673e-02, lambda1: 8.75759e-01, lambda2: 2.18608e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1001/50000 [02:05<1:37:18,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Loss: 2.76552e-01, Loss_u0: 1.91530e-02, Loss_ub: 2.83440e-04, Loss_ubx: 8.63614e-05, Loss_f: 9.11291e-02, Loss_u1: 1.65900e-01, lambda1: 9.23145e-01, lambda2: 2.16611e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1501/50000 [03:05<1:33:20,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1500, Loss: 1.44274e-01, Loss_u0: 1.08592e-02, Loss_ub: 8.14922e-05, Loss_ubx: 4.04311e-05, Loss_f: 4.48246e-02, Loss_u1: 8.84685e-02, lambda1: 9.69148e-01, lambda2: 2.14381e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2001/50000 [04:04<1:34:53,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2000, Loss: 1.15439e-01, Loss_u0: 7.22482e-03, Loss_ub: 1.65049e-05, Loss_ubx: 1.47974e-05, Loss_f: 3.16922e-02, Loss_u1: 7.64911e-02, lambda1: 9.82943e-01, lambda2: 2.12036e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2501/50000 [05:04<1:32:30,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2500, Loss: 8.55655e-02, Loss_u0: 2.39914e-03, Loss_ub: 3.73264e-05, Loss_ubx: 3.20359e-05, Loss_f: 2.22194e-02, Loss_u1: 6.08776e-02, lambda1: 9.89647e-01, lambda2: 2.10438e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3001/50000 [06:03<1:33:50,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3000, Loss: 1.32014e-01, Loss_u0: 5.00336e-03, Loss_ub: 2.81448e-04, Loss_ubx: 1.62542e-04, Loss_f: 3.86548e-02, Loss_u1: 8.79119e-02, lambda1: 9.88994e-01, lambda2: 2.09580e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3501/50000 [07:03<1:35:21,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3500, Loss: 8.75984e-02, Loss_u0: 3.38892e-03, Loss_ub: 1.84860e-04, Loss_ubx: 1.55620e-04, Loss_f: 2.60736e-02, Loss_u1: 5.77954e-02, lambda1: 9.88124e-01, lambda2: 2.08724e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4001/50000 [08:04<1:31:49,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4000, Loss: 6.14810e-02, Loss_u0: 1.36933e-03, Loss_ub: 3.23914e-05, Loss_ubx: 4.33390e-05, Loss_f: 1.46945e-02, Loss_u1: 4.53414e-02, lambda1: 9.82828e-01, lambda2: 2.07756e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4501/50000 [09:03<1:31:46,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4500, Loss: 1.73944e-01, Loss_u0: 6.33122e-03, Loss_ub: 6.39958e-07, Loss_ubx: 4.21486e-07, Loss_f: 3.55782e-02, Loss_u1: 1.32033e-01, lambda1: 9.86540e-01, lambda2: 2.06793e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5001/50000 [10:03<1:29:36,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5000, Loss: 2.11764e-01, Loss_u0: 1.02152e-02, Loss_ub: 6.47084e-08, Loss_ubx: 1.40522e-07, Loss_f: 5.97635e-02, Loss_u1: 1.41785e-01, lambda1: 1.00876e+00, lambda2: 2.06037e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5501/50000 [11:03<1:28:49,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5500, Loss: 1.07791e-01, Loss_u0: 2.10925e-03, Loss_ub: 5.04020e-07, Loss_ubx: 5.37027e-08, Loss_f: 2.51954e-02, Loss_u1: 8.04856e-02, lambda1: 1.03621e+00, lambda2: 2.05204e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6001/50000 [12:05<1:30:43,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6000, Loss: 9.58808e-02, Loss_u0: 1.89793e-03, Loss_ub: 2.26816e-10, Loss_ubx: 1.39203e-09, Loss_f: 1.99428e-02, Loss_u1: 7.40401e-02, lambda1: 1.03972e+00, lambda2: 2.04679e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6501/50000 [13:04<1:28:58,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6500, Loss: 8.97928e-02, Loss_u0: 1.54813e-03, Loss_ub: 4.04229e-06, Loss_ubx: 4.21059e-06, Loss_f: 1.91191e-02, Loss_u1: 6.91173e-02, lambda1: 1.03985e+00, lambda2: 2.04072e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7001/50000 [14:04<1:24:58,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7000, Loss: 6.98683e-01, Loss_u0: 3.86399e-01, Loss_ub: 2.92660e-19, Loss_ubx: 3.12566e-19, Loss_f: 1.65710e-06, Loss_u1: 3.12282e-01, lambda1: 1.03546e+00, lambda2: 2.03304e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 7501/50000 [15:04<1:25:22,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.33527e-20, Loss_f: 1.40884e-06, Loss_u1: 3.11696e-01, lambda1: 1.03547e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8001/50000 [16:03<1:20:58,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.33548e-20, Loss_f: 1.40884e-06, Loss_u1: 3.11696e-01, lambda1: 1.03547e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 8502/50000 [17:04<1:19:20,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.33595e-20, Loss_f: 1.40873e-06, Loss_u1: 3.11696e-01, lambda1: 1.03543e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9002/50000 [18:03<1:17:47,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.33649e-20, Loss_f: 1.40857e-06, Loss_u1: 3.11696e-01, lambda1: 1.03537e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 9501/50000 [19:02<1:21:59,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.33722e-20, Loss_f: 1.40841e-06, Loss_u1: 3.11696e-01, lambda1: 1.03531e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10001/50000 [20:00<1:18:36,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.33809e-20, Loss_f: 1.40825e-06, Loss_u1: 3.11696e-01, lambda1: 1.03525e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10501/50000 [21:00<1:22:12,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.33965e-20, Loss_f: 1.40804e-06, Loss_u1: 3.11696e-01, lambda1: 1.03517e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11001/50000 [21:59<1:18:26,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.34144e-20, Loss_f: 1.40772e-06, Loss_u1: 3.11696e-01, lambda1: 1.03505e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 11501/50000 [22:59<1:17:54,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.34440e-20, Loss_f: 1.40736e-06, Loss_u1: 3.11696e-01, lambda1: 1.03492e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12001/50000 [24:00<1:17:06,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.72457e-20, Loss_ubx: 3.34920e-20, Loss_f: 1.40687e-06, Loss_u1: 3.11696e-01, lambda1: 1.03474e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 12501/50000 [25:00<1:17:10,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 1.87704e-20, Loss_ubx: 3.35556e-20, Loss_f: 1.40624e-06, Loss_u1: 3.11696e-01, lambda1: 1.03451e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13001/50000 [25:59<1:15:28,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 2.02950e-20, Loss_ubx: 3.36355e-20, Loss_f: 1.40544e-06, Loss_u1: 3.11696e-01, lambda1: 1.03421e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 13501/50000 [26:59<1:13:18,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 2.02950e-20, Loss_ubx: 3.37434e-20, Loss_f: 1.40441e-06, Loss_u1: 3.11696e-01, lambda1: 1.03383e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14001/50000 [27:59<1:11:44,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 2.02950e-20, Loss_ubx: 3.39167e-20, Loss_f: 1.40309e-06, Loss_u1: 3.11696e-01, lambda1: 1.03334e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 14501/50000 [28:59<1:09:04,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 2.02950e-20, Loss_ubx: 3.41678e-20, Loss_f: 1.40139e-06, Loss_u1: 3.11696e-01, lambda1: 1.03272e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15001/50000 [30:00<1:11:17,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 2.14809e-20, Loss_ubx: 3.44909e-20, Loss_f: 1.39922e-06, Loss_u1: 3.11696e-01, lambda1: 1.03192e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 15501/50000 [31:00<1:09:37,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15500, Loss: 6.98658e-01, Loss_u0: 3.86969e-01, Loss_ub: 2.26648e-20, Loss_ubx: 3.49207e-20, Loss_f: 1.39316e-06, Loss_u1: 3.11688e-01, lambda1: 1.03089e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16001/50000 [32:00<1:09:42,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16000, Loss: 6.98663e-01, Loss_u0: 3.86937e-01, Loss_ub: 2.04625e-20, Loss_ubx: 3.55194e-20, Loss_f: 1.40814e-06, Loss_u1: 3.11724e-01, lambda1: 1.02957e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16501/50000 [33:00<1:07:24,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16500, Loss: 6.98658e-01, Loss_u0: 3.86961e-01, Loss_ub: 2.26648e-20, Loss_ubx: 3.63698e-20, Loss_f: 1.38813e-06, Loss_u1: 3.11696e-01, lambda1: 1.02787e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17001/50000 [33:59<1:05:22,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17000, Loss: 6.98661e-01, Loss_u0: 3.86964e-01, Loss_ub: 2.04611e-20, Loss_ubx: 3.75199e-20, Loss_f: 1.38516e-06, Loss_u1: 3.11695e-01, lambda1: 1.02571e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 17501/50000 [35:00<1:05:08,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17500, Loss: 6.98658e-01, Loss_u0: 3.86954e-01, Loss_ub: 2.26661e-20, Loss_ubx: 3.91617e-20, Loss_f: 1.37735e-06, Loss_u1: 3.11702e-01, lambda1: 1.02293e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18001/50000 [36:00<1:06:09,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18000, Loss: 6.98658e-01, Loss_u0: 3.86963e-01, Loss_ub: 2.41934e-20, Loss_ubx: 4.15883e-20, Loss_f: 1.36452e-06, Loss_u1: 3.11694e-01, lambda1: 1.01937e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 18501/50000 [37:01<1:03:03,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18500, Loss: 6.98658e-01, Loss_u0: 3.87020e-01, Loss_ub: 2.49835e-20, Loss_ubx: 4.57512e-20, Loss_f: 1.33111e-06, Loss_u1: 3.11637e-01, lambda1: 1.01482e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19001/50000 [38:01<1:02:18,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19000, Loss: 6.98658e-01, Loss_u0: 3.86964e-01, Loss_ub: 2.96326e-20, Loss_ubx: 6.08479e-20, Loss_f: 1.33663e-06, Loss_u1: 3.11693e-01, lambda1: 1.00901e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 19501/50000 [39:02<1:04:24,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19500, Loss: 6.98658e-01, Loss_u0: 3.86959e-01, Loss_ub: 0.00000e+00, Loss_ubx: 2.34763e-23, Loss_f: 1.31815e-06, Loss_u1: 3.11698e-01, lambda1: 1.00162e+00, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20001/50000 [40:02<59:12,  8.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 0.00000e+00, Loss_ubx: 4.50328e-23, Loss_f: 1.29380e-06, Loss_u1: 3.11696e-01, lambda1: 9.92204e-01, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 20501/50000 [41:02<1:00:36,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20500, Loss: 6.98658e-01, Loss_u0: 3.86961e-01, Loss_ub: 7.94093e-24, Loss_ubx: 3.79527e-22, Loss_f: 1.26272e-06, Loss_u1: 3.11696e-01, lambda1: 9.80274e-01, lambda2: 2.03305e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21001/50000 [42:02<58:16,  8.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 7.54389e-22, Loss_ubx: 4.29089e-21, Loss_f: 1.22528e-06, Loss_u1: 3.11697e-01, lambda1: 9.65514e-01, lambda2: 2.03316e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 21501/50000 [43:03<56:25,  8.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21500, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 7.53727e-22, Loss_ubx: 4.29422e-21, Loss_f: 1.17795e-06, Loss_u1: 3.11697e-01, lambda1: 9.46645e-01, lambda2: 2.03316e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22001/50000 [44:03<57:15,  8.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22000, Loss: 6.98658e-01, Loss_u0: 3.86960e-01, Loss_ub: 7.53727e-22, Loss_ubx: 4.28737e-21, Loss_f: 1.12036e-06, Loss_u1: 3.11697e-01, lambda1: 9.23173e-01, lambda2: 2.03316e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 22501/50000 [45:03<54:14,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22500, Loss: 6.98658e-01, Loss_u0: 3.86959e-01, Loss_ub: 7.53065e-22, Loss_ubx: 4.27024e-21, Loss_f: 1.05150e-06, Loss_u1: 3.11697e-01, lambda1: 8.94289e-01, lambda2: 2.03316e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23001/50000 [46:03<54:36,  8.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23000, Loss: 6.98658e-01, Loss_u0: 3.86959e-01, Loss_ub: 7.53065e-22, Loss_ubx: 4.24469e-21, Loss_f: 9.70775e-07, Loss_u1: 3.11697e-01, lambda1: 8.59259e-01, lambda2: 2.03316e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 23501/50000 [47:04<53:14,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23500, Loss: 6.98658e-01, Loss_u0: 3.86954e-01, Loss_ub: 7.62330e-22, Loss_ubx: 4.20038e-21, Loss_f: 8.80942e-07, Loss_u1: 3.11703e-01, lambda1: 8.17551e-01, lambda2: 2.03316e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24001/50000 [48:04<51:31,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24000, Loss: 6.98657e-01, Loss_u0: 3.86959e-01, Loss_ub: 7.53065e-22, Loss_ubx: 4.13589e-21, Loss_f: 7.77971e-07, Loss_u1: 3.11698e-01, lambda1: 7.68999e-01, lambda2: 2.03316e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 24501/50000 [49:03<50:46,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24500, Loss: 6.98658e-01, Loss_u0: 3.86958e-01, Loss_ub: 1.00585e-21, Loss_ubx: 4.04941e-21, Loss_f: 6.71590e-07, Loss_u1: 3.11699e-01, lambda1: 7.13912e-01, lambda2: 2.03315e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25001/50000 [50:03<49:57,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25000, Loss: 6.98657e-01, Loss_u0: 3.86973e-01, Loss_ub: 9.29089e-22, Loss_ubx: 3.92987e-21, Loss_f: 5.59030e-07, Loss_u1: 3.11684e-01, lambda1: 6.53137e-01, lambda2: 2.03314e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 25501/50000 [51:04<48:47,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25500, Loss: 6.98659e-01, Loss_u0: 3.86778e-01, Loss_ub: 1.08869e-21, Loss_ubx: 3.77805e-21, Loss_f: 4.78624e-07, Loss_u1: 3.11880e-01, lambda1: 5.87939e-01, lambda2: 2.03311e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26001/50000 [52:04<50:08,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26000, Loss: 6.98657e-01, Loss_u0: 3.86957e-01, Loss_ub: 1.07798e-21, Loss_ubx: 3.58915e-21, Loss_f: 3.56144e-07, Loss_u1: 3.11700e-01, lambda1: 5.19851e-01, lambda2: 2.03304e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 26501/50000 [53:04<48:12,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26500, Loss: 6.98657e-01, Loss_u0: 3.86953e-01, Loss_ub: 1.07401e-21, Loss_ubx: 3.36086e-21, Loss_f: 2.67855e-07, Loss_u1: 3.11703e-01, lambda1: 4.50491e-01, lambda2: 2.03292e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27001/50000 [54:05<47:12,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27000, Loss: 6.98657e-01, Loss_u0: 3.86940e-01, Loss_ub: 9.04605e-22, Loss_ubx: 3.09454e-21, Loss_f: 1.92850e-07, Loss_u1: 3.11717e-01, lambda1: 3.81441e-01, lambda2: 2.03279e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 27501/50000 [55:05<44:10,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27500, Loss: 6.98666e-01, Loss_u0: 3.86566e-01, Loss_ub: 7.34536e-22, Loss_ubx: 2.79354e-21, Loss_f: 1.45531e-07, Loss_u1: 3.12099e-01, lambda1: 3.14211e-01, lambda2: 2.03265e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 27629/50000 [55:20<44:48,  8.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23288\\1291242424.py\u001b[0m in \u001b[0;36mloss_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss_u0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_ub\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_ubx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss_u1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.losshistory=[]            \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e3d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268597665b0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losshistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e027874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "u00_1, v00_1, u00_2, v00_2 = model.predict1(test10_x,test10_t)\n",
    "u11_1, v11_1, u11_2, v11_2 = model.predict1(testb_x1,testb_t1)\n",
    "u22_1, v22_1, u22_2, v22_2 = model.predict1(testb_x2,testb_t2)\n",
    "a1,b1,c1,d1, ux11_1, vx11_1, ux11_2, vx11_2 = model.predict2(testb_x1,testb_t1)\n",
    "a1,b1,c1,d1, ux22_1, vx22_1, ux22_2, vx22_2 = model.predict2(testb_x2,testb_t2)\n",
    "\n",
    "f1, f2, f3, f4,a1,b1,c1,d1 = model.predict2(testu_x,testu_t)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c0224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_u0: 5.27684e-02, Error_ub: 0.00000e+00, Error_ubx: 4.74056e-67, Error_f: 8.12052e-08\n"
     ]
    }
   ],
   "source": [
    "error_u0= ((np.linalg.norm(test10_r - u00_1, 2))**2+(np.linalg.norm(test10_c - v00_1, 2))**2+(np.linalg.norm(test20_r - u00_2, 2))**2+(np.linalg.norm(test20_c - v00_2, 2))**2)/20\n",
    "error_ub= ((np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2+(np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2)/20\n",
    "error_ubx= (\n",
    "    (np.linalg.norm(ux11_1.cpu().detach().numpy() - ux22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_1.cpu().detach().numpy() - vx22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(ux11_2.cpu().detach().numpy() - ux22_2.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_2.cpu().detach().numpy() - vx22_2.cpu().detach().numpy(), 2))**2)/20\n",
    "\n",
    "error_f= ((np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2+(np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2)/2000\n",
    "print('Error_u0: %.5e, Error_ub: %.5e, Error_ubx: %.5e, Error_f: %.5e' % (error_u0.item(), error_ub.item(), error_ubx.item(), error_f.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
