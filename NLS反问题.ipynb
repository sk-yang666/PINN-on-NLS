{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20cdd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, optim \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import random\n",
    "from torch.utils import data\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8e41b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dca358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=pd.read_csv(\"data_x_new.csv\")\n",
    "data_t=pd.read_csv(\"data_t_new.csv\")\n",
    "data_xx=pd.read_csv(\"data_x_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "969f73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h10_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h10_t=data_x.iloc[:, [1]]\n",
    "h10_R=data_x.iloc[:,[2]]#初值h实部\n",
    "h10_C=data_x.iloc[:,[3]]#初值h虚部\n",
    "hb_x1=data_t.iloc[:,[1]]#边值-5，t\n",
    "hb_t1=data_t.iloc[:,[0]]\n",
    "hb_x2=data_t.iloc[:,[2]]#边值5，t\n",
    "hb_t2=data_t.iloc[:,[0]]\n",
    "\n",
    "h20_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h20_t=data_x.iloc[:, [1]]\n",
    "h20_R=data_x.iloc[:,[4]]#初值h实部\n",
    "h20_C=data_x.iloc[:,[5]]#初值h虚部\n",
    "\n",
    "h30_x=data_xx.iloc[:, [0]]\n",
    "h30_t=data_xx.iloc[:, [1]]\n",
    "h30_r1=data_xx.iloc[:, [2]]\n",
    "h30_c1=data_xx.iloc[:, [3]]\n",
    "h30_r2=data_xx.iloc[:, [4]]\n",
    "h30_c2=data_xx.iloc[:, [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76bd3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x['x']\n",
    "t = data_t['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b2ca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc71aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf= np.hstack((X.flatten()[:,None], T.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "950258e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_x=hf[:,[0]]\n",
    "hf_t=hf[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78ff78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train10_x,test10_x,train10_t,test10_t,train10_r,test10_r,train10_c,test10_c,trainb_x1,testb_x1,trainb_t1,testb_t1,trainb_x2,testb_x2,trainb_t2,testb_t2,train20_r,test20_r,train20_c,test20_c = train_test_split(h10_x,h10_t,h10_R,h10_C,hb_x1,hb_t1,hb_x2,hb_t2,h20_R,h20_C,test_size=0.2)\n",
    "\n",
    "train30_x,test30_x,train30_t,test30_t,train30_r1,test30_r1,train30_c1,test30_c1,train30_r2,test30_r2,train30_c2,test30_c2=train_test_split(h30_x,h30_t,h30_r1,h30_c1,h30_r2,h30_c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8301e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainu_x, testu_x, trainu_t, testu_t = train_test_split(hf_x, hf_t, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68c1b3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train10_x=torch.from_numpy(train10_x.to_numpy()).float()\n",
    "train10_t=torch.from_numpy(train10_t.to_numpy()).float()\n",
    "\n",
    "train10_r=torch.from_numpy(train10_r.to_numpy()).float()\n",
    "train10_c=torch.from_numpy(train10_c.to_numpy()).float()\n",
    "\n",
    "trainb_x1=torch.from_numpy(trainb_x1.to_numpy()).float()\n",
    "trainb_t1=torch.from_numpy(trainb_t1.to_numpy()).float()\n",
    "trainb_x2=torch.from_numpy(trainb_x2.to_numpy()).float()\n",
    "trainb_t2=torch.from_numpy(trainb_t2.to_numpy()).float()\n",
    "\n",
    "# train20_x=torch.from_numpy(train20_x.to_numpy()).float()\n",
    "# train20_t=torch.from_numpy(train20_t.to_numpy()).float()\n",
    "train20_r=torch.from_numpy(train20_r.to_numpy()).float()\n",
    "train20_c=torch.from_numpy(train20_c.to_numpy()).float()\n",
    "\n",
    "\n",
    "train30_x=torch.from_numpy(train30_x.to_numpy()).float()\n",
    "train30_t=torch.from_numpy(train30_t.to_numpy()).float()\n",
    "train30_r1=torch.from_numpy(train30_r1.to_numpy()).float()\n",
    "train30_c1=torch.from_numpy(train30_c1.to_numpy()).float()\n",
    "train30_r2=torch.from_numpy(train30_r2.to_numpy()).float()\n",
    "train30_c2=torch.from_numpy(train30_c2.to_numpy()).float()\n",
    "\n",
    "trainu_x=torch.from_numpy(trainu_x).float()\n",
    "trainu_t=torch.from_numpy(trainu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2e60ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10_x=torch.from_numpy(test10_x.to_numpy()).float()\n",
    "test10_t=torch.from_numpy(test10_t.to_numpy()).float()\n",
    "test10_r=torch.from_numpy(test10_r.to_numpy()).float()\n",
    "test10_c=torch.from_numpy(test10_c.to_numpy()).float()\n",
    "\n",
    "testb_x1=torch.from_numpy(testb_x1.to_numpy()).float()\n",
    "testb_t1=torch.from_numpy(testb_t1.to_numpy()).float()\n",
    "testb_x2=torch.from_numpy(testb_x2.to_numpy()).float()\n",
    "testb_t2=torch.from_numpy(testb_t2.to_numpy()).float()\n",
    "\n",
    "# test20_x=torch.from_numpy(test20_x.to_numpy()).float()\n",
    "# test20_t=torch.from_numpy(test20_t.to_numpy()).float()\n",
    "test20_r=torch.from_numpy(test20_r.to_numpy()).float()\n",
    "test20_c=torch.from_numpy(test20_c.to_numpy()).float()\n",
    "\n",
    "testu_x=torch.from_numpy(testu_x).float()\n",
    "testu_t=torch.from_numpy(testu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20c0aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7b860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f5e4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN():\n",
    "    def __init__(self, u10_x, u10_t, u10_r, u10_c, ub_x1, ub_t1, ub_x2, ub_t2, u20_r, u20_c, uf_x, uf_t,u30_x,u30_t,u30_r1,u30_c1,u30_r2,u30_c2):\n",
    "        self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
    "        self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
    "        self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
    "        self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
    "        self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
    "        self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
    "        self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
    "        \n",
    "#         self.u20_x = u20_x.clone().detach().requires_grad_(True).float().to(device)\n",
    "#         self.u20_t = torch.tensor(u20_t, requires_grad=True).float().to(device)\n",
    "        self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
    "        self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
    "        self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
    "        self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
    "        self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
    "        self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
    "        self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
    "      \n",
    "        \n",
    "        self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
    "        self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.tensor([0.9], requires_grad=True).to(device)\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        \n",
    "        self.lambda_2 = torch.tensor([2.3], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        self.dnn = DNN().to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params':self.dnn.parameters()},\n",
    "            {'params': [self.lambda_1], 'lr': 0.000206},\n",
    "            {'params': [self.lambda_2], 'lr': 0.000106}\n",
    "        ],lr=0.012 )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "        self.losshistory=[]\n",
    "    def net_u(self, x, t):  \n",
    "        u1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,0],1)\n",
    "        v1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,1],1)\n",
    "        u2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,2],1)\n",
    "        v2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,3],1)\n",
    "        return u1, v1, u2, v2\n",
    "\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        beta=1\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        lambda_3 = -1\n",
    "        lambda_4 = 0\n",
    "        u1, v1,u2, v2= self.net_u(x, t)\n",
    "        \n",
    "        u1_t = torch.autograd.grad(\n",
    "            u1, t, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_x = torch.autograd.grad(\n",
    "            u1, x, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_xx = torch.autograd.grad(\n",
    "            u1_x, x, \n",
    "            grad_outputs=torch.ones_like(u1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v1_t = torch.autograd.grad(\n",
    "            v1, t, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_x = torch.autograd.grad(\n",
    "            v1, x, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_xx = torch.autograd.grad(\n",
    "            v1_x, x, \n",
    "            grad_outputs=torch.ones_like(v1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        u2_t = torch.autograd.grad(\n",
    "            u2, t, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_x = torch.autograd.grad(\n",
    "            u2, x, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_xx = torch.autograd.grad(\n",
    "            u2_x, x, \n",
    "            grad_outputs=torch.ones_like(u2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v2_t = torch.autograd.grad(\n",
    "            v2, t, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_x = torch.autograd.grad(\n",
    "            v2, x, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_xx = torch.autograd.grad(\n",
    "            v2_x, x, \n",
    "            grad_outputs=torch.ones_like(v2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f_u1 = (\n",
    "            v1_t + u1_xx\n",
    "        + lambda_1*u1*(u1**2 + v1**2) + lambda_2*u1*(u2**2 + v2**2) + lambda_3*(u1*(u2**2 - v2**2) + 2*u2*v1*v2) + lambda_4*(u2*(u1**2 - v1**2) + 2*u1*v1*v2)\n",
    "    )\n",
    "\n",
    "        f_v1 = (\n",
    "                -u1_t + v1_xx\n",
    "            + lambda_1*v1*(u1**2 + v1**2) + lambda_2*v1*(u2**2 + v2**2) - lambda_3*(v1*(u2**2 - v2**2) - 2*u1*u2*v2) - lambda_4*(v2*(u1**2 - v1**2) - 2*u1*u2*v1)\n",
    "        )\n",
    "\n",
    "        f_u2 = (\n",
    "                v2_t + beta*u2_xx\n",
    "            + lambda_1*u2*(u2**2 + v2**2) + lambda_2*u2*(u1**2 + v1**2) + lambda_3*(u2*(u1**2 - v1**2) + 2*u1*v2*v1) + lambda_4*(u1*(u2**2 - v2**2) + 2*u2*v2*v1)\n",
    "        )\n",
    "\n",
    "        f_v2 = (\n",
    "                -u2_t + beta*v2_xx\n",
    "            + lambda_1*v2*(u2**2 + v2**2) + lambda_2*v2*(u1**2 + v1**2) - lambda_3*(v2*(u1**2 - v1**2) - 2*u2*u1*v1) - lambda_4*(v1*(u2**2 - v2**2) - 2*u2*u1*v2)\n",
    "        )\n",
    "        a=u1_x\n",
    "        b=v1_x\n",
    "        c=u2_x\n",
    "        d=v2_x\n",
    "        return f_u1, f_v1, f_u2, f_v2, a, b, c, d\n",
    "     \n",
    "    def loss_func(self):\n",
    "        for self.iter in tqdm(range(12000)):\n",
    "            torch.cuda.empty_cache()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            u10_pred, v10_pred, u20_pred, v20_pred = self.net_u(self.u10_x,self.u10_t)\n",
    "            u30_pred1, v30_pred1, u30_pred2, v30_pred2 = self.net_u(self.u30_x,self.u30_t)\n",
    "            u1b_pred1, v1b_pred1, u2b_pred1, v2b_pred1 = self.net_u(self.ub_x1,self.ub_t1)\n",
    "            u1b_pred2, v1b_pred2, u2b_pred2, v2b_pred2 = self .net_u(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            \n",
    "            a1,b1,c1,d1,u1bx_pred1, v1bx_pred1,u2bx_pred1, v2bx_pred1 = self.net_f(self.ub_x1,self.ub_t1)\n",
    "            a2,b2,c2,d2,u1bx_pred2, v1bx_pred2,u2bx_pred2, v2bx_pred2 = self.net_f(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            f_predu1, f_predv1, f_predu2, f_predv2,a1,b1,c1,d1 = self.net_f(self.uf_x,self.uf_t)\n",
    "            \n",
    "            loss_u0 = torch.mean((self.u10_r - u10_pred) ** 2)+torch.mean((self.u10_c - v10_pred) ** 2)+torch.mean((self.u20_r - u20_pred) ** 2)+torch.mean((self.u20_c - v20_pred) ** 2)\n",
    "            loss_ub = torch.mean((u1b_pred1 - u1b_pred2) ** 2)+torch.mean((v1b_pred1 - v1b_pred2) ** 2)+torch.mean((u2b_pred1 - u2b_pred2) ** 2)+torch.mean((v2b_pred1 - v2b_pred2) ** 2)\n",
    "            loss_ubx = torch.mean((u1bx_pred1 - u1bx_pred2) ** 2)+torch.mean((v1bx_pred1 - v1bx_pred2) ** 2)+torch.mean((u2bx_pred1 - u2bx_pred2) ** 2)+torch.mean((v2bx_pred1 - v2bx_pred2) ** 2)\n",
    "            loss_f = torch.mean(f_predu1 ** 2)+torch.mean(f_predv1 ** 2)+torch.mean(f_predu2 ** 2)+torch.mean(f_predv2 ** 2)\n",
    "            loss_u1=torch.mean((self.u30_r1 - u30_pred1) ** 2)+torch.mean((self.u30_c1 - v30_pred1) ** 2)+torch.mean((self.u30_r2 - u30_pred2) ** 2)+torch.mean((self.u30_c2 - v30_pred2) ** 2)\n",
    "            \n",
    "            loss = 3*loss_f+loss_u0 + loss_ub + loss_ubx+loss_u1\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losshistory.append(loss.clone().detach().cpu())\n",
    "            \n",
    "            self.iter += 1\n",
    "            with torch.no_grad():\n",
    "                if self.iter % 500 == 0:\n",
    "                    print('Iter %d, Loss: %.5e, Loss_u0: %.5e, Loss_ub: %.5e, Loss_ubx: %.5e, Loss_f: %.5e, Loss_u1: %.5e, lambda1: %.5e, lambda2: %.5e' % (self.iter, loss.item(), loss_u0.item(), loss_ub.item(), loss_ubx.item(), loss_f.item(), loss_u1.item(), self.lambda_1,self.lambda_2))\n",
    "        return float(loss)\n",
    "        \n",
    " \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "        \n",
    "    def predict1(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u1, v1 , u2, v2= self.net_u(x, t)\n",
    "        u1 = u1.detach().cpu().numpy()\n",
    "        v1 = v1.detach().cpu().numpy()\n",
    "        u2 = u2.detach().cpu().numpy()\n",
    "        v2 = v2.detach().cpu().numpy()\n",
    "        return u1, v1, u2, v2\n",
    "    \n",
    "    def predict2(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        f1, f2, f3, f4, a, b, c, d = self.net_f(x, t)\n",
    "        f1 = f1.detach().cpu().numpy()\n",
    "        f2 = f2.detach().cpu().numpy()\n",
    "        f3 = f3.detach().cpu().numpy()\n",
    "        f4 = f4.detach().cpu().numpy()\n",
    "        return  f1, f2, f3, f4, a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08905a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PhysicsInformedNN(train10_x, train10_t, train10_r, train10_c, \n",
    "                          trainb_x1, trainb_t1, trainb_x2, trainb_t2,\n",
    "                        train20_r, train20_c, trainu_x, trainu_t,\n",
    "                         train30_x,train30_t, train30_r1, train30_c1,train30_r2,train30_c2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7626fd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 501/12000 [01:09<26:16,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500, Loss: 3.06851e-01, Loss_u0: 2.86183e-02, Loss_ub: 6.36243e-05, Loss_ubx: 6.05416e-05, Loss_f: 1.73875e-02, Loss_u1: 2.25946e-01, lambda1: 9.73275e-01, lambda2: 2.31893e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1001/12000 [02:18<24:16,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Loss: 2.28110e-01, Loss_u0: 4.56717e-03, Loss_ub: 1.66180e-04, Loss_ubx: 7.37655e-05, Loss_f: 3.07496e-02, Loss_u1: 1.31054e-01, lambda1: 9.84710e-01, lambda2: 2.32176e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1501/12000 [03:26<22:31,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1500, Loss: 1.19704e-01, Loss_u0: 4.18112e-03, Loss_ub: 4.60063e-05, Loss_ubx: 2.79326e-05, Loss_f: 8.69157e-03, Loss_u1: 8.93745e-02, lambda1: 9.82508e-01, lambda2: 2.32681e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2001/12000 [04:31<21:32,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2000, Loss: 1.01402e-01, Loss_u0: 3.99123e-03, Loss_ub: 4.17465e-05, Loss_ubx: 3.65926e-05, Loss_f: 4.97725e-03, Loss_u1: 8.24004e-02, lambda1: 9.77867e-01, lambda2: 2.32625e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2501/12000 [05:36<20:03,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2500, Loss: 9.82378e-02, Loss_u0: 3.18354e-03, Loss_ub: 3.57637e-05, Loss_ubx: 3.23628e-05, Loss_f: 8.83991e-03, Loss_u1: 6.84663e-02, lambda1: 9.70837e-01, lambda2: 2.32428e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3001/12000 [06:41<19:41,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3000, Loss: 8.17291e-02, Loss_u0: 3.22700e-03, Loss_ub: 4.07716e-05, Loss_ubx: 8.39939e-06, Loss_f: 4.69595e-03, Loss_u1: 6.43651e-02, lambda1: 9.63928e-01, lambda2: 2.32118e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 3501/12000 [07:48<18:31,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3500, Loss: 7.95852e-02, Loss_u0: 3.11327e-03, Loss_ub: 3.83270e-05, Loss_ubx: 1.21348e-05, Loss_f: 5.38334e-03, Loss_u1: 6.02715e-02, lambda1: 9.57562e-01, lambda2: 2.31780e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4001/12000 [08:54<17:33,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4000, Loss: 1.05867e-01, Loss_u0: 3.29455e-03, Loss_ub: 5.83900e-05, Loss_ubx: 1.74173e-05, Loss_f: 1.40656e-02, Loss_u1: 6.02996e-02, lambda1: 9.51503e-01, lambda2: 2.31450e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 4501/12000 [09:59<16:07,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4500, Loss: 1.11581e-01, Loss_u0: 3.83109e-03, Loss_ub: 2.42801e-05, Loss_ubx: 3.55985e-05, Loss_f: 7.29598e-03, Loss_u1: 8.58024e-02, lambda1: 9.52934e-01, lambda2: 2.31795e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5001/12000 [11:04<14:37,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5000, Loss: 9.80838e-02, Loss_u0: 3.38106e-03, Loss_ub: 1.42240e-05, Loss_ubx: 1.91875e-05, Loss_f: 5.88040e-03, Loss_u1: 7.70281e-02, lambda1: 9.46610e-01, lambda2: 2.31605e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 5501/12000 [12:10<14:31,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5500, Loss: 9.27589e-02, Loss_u0: 2.66606e-03, Loss_ub: 2.04202e-04, Loss_ubx: 1.09868e-04, Loss_f: 5.99253e-03, Loss_u1: 7.18012e-02, lambda1: 9.40055e-01, lambda2: 2.31363e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6001/12000 [13:16<13:26,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6000, Loss: 8.01112e-02, Loss_u0: 2.84054e-03, Loss_ub: 3.36380e-05, Loss_ubx: 2.96763e-05, Loss_f: 4.94401e-03, Loss_u1: 6.23753e-02, lambda1: 9.33872e-01, lambda2: 2.31097e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 6501/12000 [14:22<14:24,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6500, Loss: 8.41444e-02, Loss_u0: 2.47060e-03, Loss_ub: 2.19961e-05, Loss_ubx: 2.76201e-05, Loss_f: 5.94198e-03, Loss_u1: 6.37982e-02, lambda1: 9.27562e-01, lambda2: 2.30810e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7001/12000 [15:29<11:10,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7000, Loss: 1.61442e-01, Loss_u0: 5.89477e-03, Loss_ub: 1.97943e-05, Loss_ubx: 4.18574e-06, Loss_f: 9.30938e-03, Loss_u1: 1.27595e-01, lambda1: 9.39548e-01, lambda2: 2.31485e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 7501/12000 [16:34<10:01,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7500, Loss: 1.22149e-01, Loss_u0: 3.91647e-03, Loss_ub: 4.40915e-05, Loss_ubx: 2.21850e-05, Loss_f: 7.75565e-03, Loss_u1: 9.48998e-02, lambda1: 9.40365e-01, lambda2: 2.31803e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8001/12000 [17:39<08:42,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8000, Loss: 1.12056e-01, Loss_u0: 3.26908e-03, Loss_ub: 6.49944e-05, Loss_ubx: 2.27629e-05, Loss_f: 7.76403e-03, Loss_u1: 8.54075e-02, lambda1: 9.36338e-01, lambda2: 2.31883e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 8501/12000 [18:44<08:05,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8500, Loss: 1.02927e-01, Loss_u0: 3.13068e-03, Loss_ub: 6.44515e-05, Loss_ubx: 2.26361e-05, Loss_f: 6.10490e-03, Loss_u1: 8.13949e-02, lambda1: 9.32036e-01, lambda2: 2.31859e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9001/12000 [19:48<06:32,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9000, Loss: 9.85718e-02, Loss_u0: 3.23522e-03, Loss_ub: 5.74748e-05, Loss_ubx: 1.46972e-05, Loss_f: 5.41635e-03, Loss_u1: 7.90153e-02, lambda1: 9.27704e-01, lambda2: 2.31786e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 9501/12000 [20:52<05:19,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9500, Loss: 9.31129e-02, Loss_u0: 2.62291e-03, Loss_ub: 4.10918e-05, Loss_ubx: 9.55641e-06, Loss_f: 5.45401e-03, Loss_u1: 7.40773e-02, lambda1: 9.23764e-01, lambda2: 2.31688e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10001/12000 [21:57<04:20,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10000, Loss: 9.74663e-02, Loss_u0: 3.06993e-03, Loss_ub: 3.77362e-05, Loss_ubx: 8.43513e-06, Loss_f: 6.56570e-03, Loss_u1: 7.46531e-02, lambda1: 9.19726e-01, lambda2: 2.31574e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 10501/12000 [23:01<03:15,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10500, Loss: 9.69863e-02, Loss_u0: 2.47296e-03, Loss_ub: 2.90525e-05, Loss_ubx: 9.56871e-06, Loss_f: 7.72673e-03, Loss_u1: 7.12945e-02, lambda1: 9.15420e-01, lambda2: 2.31441e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11001/12000 [24:07<02:15,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11000, Loss: 8.39722e-02, Loss_u0: 2.43516e-03, Loss_ub: 3.31170e-05, Loss_ubx: 7.35459e-06, Loss_f: 5.18441e-03, Loss_u1: 6.59433e-02, lambda1: 9.11417e-01, lambda2: 2.31297e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 11501/12000 [25:13<01:09,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11500, Loss: 8.31560e-02, Loss_u0: 2.27626e-03, Loss_ub: 3.29350e-05, Loss_ubx: 8.26985e-06, Loss_f: 5.91330e-03, Loss_u1: 6.30986e-02, lambda1: 9.06864e-01, lambda2: 2.31097e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [26:18<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12000, Loss: 1.43741e+00, Loss_u0: 3.08471e-01, Loss_ub: 5.45040e-04, Loss_ubx: 1.68097e-04, Loss_f: 2.20100e-01, Loss_u1: 4.67923e-01, lambda1: 8.98635e-01, lambda2: 2.30698e+00\n",
      "Wall time: 26min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.losshistory=[]            \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "117e3d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b99b196370>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm/UlEQVR4nO3df3BV9Z3/8dclPy4hTc6SZJPrLZHG2XyRNuC6wQ1h3QUFIkjM9uvOUg290lkWapEfWWBRys6UdmrCMFPATlZE6ohboHF2Kl3XdbOEVqMMgUD0bvkhaL+N8sNcgvZyb9CQRPL5/qEcewliLgSST3g+Zu4o57xz7zkfI3lyuCfxGGOMAAAALDOkvw8AAADgShAxAADASkQMAACwEhEDAACsRMQAAAArETEAAMBKRAwAALASEQMAAKyU2N8HcK10d3fr/fffV1pamjweT38fDgAA6AVjjNra2uT3+zVkyOWvtQzaiHn//feVm5vb34cBAACuwPHjxzVixIjLzgzaiElLS5P06SKkp6f389EAAIDeiEajys3Ndb+OX86gjZgLf4WUnp5OxAAAYJnevBWEN/YCAAArETEAAMBKRAwAALASEQMAAKxExAAAACsRMQAAwEpEDAAAsBIRAwAArETEAAAAKxExAADASkQMAACwEhEDAACsRMQAAIC4fNz5iZ5+7f/p3Q8+6tfjIGIAAEBc1tQeVeXLR3T3T17t1+OIK2JWrVolj8cT8/D5fO5+Y4xWrVolv9+vlJQUTZo0SYcOHYp5jo6ODi1cuFBZWVlKTU1VWVmZTpw4ETMTDocVCATkOI4cx1EgENCZM2eu/CwBAECfaWz+gySp2/TvccR9JeYb3/iGWlpa3MeBAwfcfWvWrNHatWtVXV2tffv2yefzaerUqWpra3NnKioqtH37dtXU1GjXrl06e/asSktLdf78eXemvLxcwWBQtbW1qq2tVTAYVCAQuMpTBQAAg0li3B+QmBhz9eUCY4zWr1+vlStX6v7775ckPffcc8rJydG2bdv03e9+V5FIRM8884x+/vOfa8qUKZKkLVu2KDc3Vzt37tQ999yjt956S7W1tdqzZ4+KiookSZs2bVJxcbGOHj2qUaNGXc35AgCAQSLuKzHvvPOO/H6/8vLy9MADD+j3v/+9JKm5uVmhUEglJSXurNfr1cSJE7V7925JUlNTk7q6umJm/H6/CgoK3JmGhgY5juMGjCSNHz9ejuO4M5fS0dGhaDQa8wAAAINXXBFTVFSkf/u3f9P//M//aNOmTQqFQpowYYI+/PBDhUIhSVJOTk7Mx+Tk5Lj7QqGQkpOTNXz48MvOZGdn93jt7Oxsd+ZSqqqq3PfQOI6j3NzceE4NAABYJq6ImT59uv7u7/5OY8aM0ZQpU/Rf//Vfkj79a6MLPB5PzMcYY3psu9jFM5ea/7LnWbFihSKRiPs4fvx4r84JAADY6apusU5NTdWYMWP0zjvvuO+TufhqSWtrq3t1xufzqbOzU+Fw+LIzp06d6vFap0+f7nGV5495vV6lp6fHPAAAwOB1VRHT0dGht956SzfddJPy8vLk8/lUV1fn7u/s7FR9fb0mTJggSSosLFRSUlLMTEtLiw4ePOjOFBcXKxKJqLGx0Z3Zu3evIpGIOwMAABDX3UnLli3Tfffdp5tvvlmtra368Y9/rGg0qtmzZ8vj8aiiokKVlZXKz89Xfn6+KisrNWzYMJWXl0uSHMfRnDlztHTpUmVmZiojI0PLli1z/3pKkkaPHq1p06Zp7ty52rhxoyRp3rx5Ki0t5c4kAADgiitiTpw4oQcffFAffPCB/vRP/1Tjx4/Xnj17NHLkSEnS8uXL1d7ervnz5yscDquoqEg7duxQWlqa+xzr1q1TYmKiZs6cqfb2dk2ePFmbN29WQkKCO7N161YtWrTIvYuprKxM1dXVfXG+AABgkPAYY/r5++1dG9FoVI7jKBKJ8P4YAAD60L1PvK7DLZ9+K5N3V8/o0+eO5+s3PzsJAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWuKmKqqqrk8XhUUVHhbjPGaNWqVfL7/UpJSdGkSZN06NChmI/r6OjQwoULlZWVpdTUVJWVlenEiRMxM+FwWIFAQI7jyHEcBQIBnTlz5moOFwAADCJXHDH79u3T008/rbFjx8ZsX7NmjdauXavq6mrt27dPPp9PU6dOVVtbmztTUVGh7du3q6amRrt27dLZs2dVWlqq8+fPuzPl5eUKBoOqra1VbW2tgsGgAoHAlR4uAAAYZK4oYs6ePatZs2Zp06ZNGj58uLvdGKP169dr5cqVuv/++1VQUKDnnntOH3/8sbZt2yZJikQieuaZZ/STn/xEU6ZM0e23364tW7bowIED2rlzpyTprbfeUm1trX72s5+puLhYxcXF2rRpk1566SUdPXq0D04bAADY7ooi5pFHHtGMGTM0ZcqUmO3Nzc0KhUIqKSlxt3m9Xk2cOFG7d++WJDU1Namrqytmxu/3q6CgwJ1paGiQ4zgqKipyZ8aPHy/HcdyZi3V0dCgajcY8AADA4JUY7wfU1NSoqalJ+/fv77EvFApJknJycmK25+Tk6L333nNnkpOTY67gXJi58PGhUEjZ2dk9nj87O9uduVhVVZV++MMfxns6AADAUnFdiTl+/LgWL16srVu3aujQoV845/F4Yn5tjOmx7WIXz1xq/nLPs2LFCkUiEfdx/Pjxy74eAACwW1wR09TUpNbWVhUWFioxMVGJiYmqr6/XT3/6UyUmJrpXYC6+WtLa2uru8/l86uzsVDgcvuzMqVOnerz+6dOne1zlucDr9So9PT3mAQAABq+4Imby5Mk6cOCAgsGg+xg3bpxmzZqlYDCoW265RT6fT3V1de7HdHZ2qr6+XhMmTJAkFRYWKikpKWampaVFBw8edGeKi4sViUTU2Njozuzdu1eRSMSdAQAAN7a43hOTlpamgoKCmG2pqanKzMx0t1dUVKiyslL5+fnKz89XZWWlhg0bpvLyckmS4ziaM2eOli5dqszMTGVkZGjZsmUaM2aM+0bh0aNHa9q0aZo7d642btwoSZo3b55KS0s1atSoqz5pAABgv7jf2Ptlli9frvb2ds2fP1/hcFhFRUXasWOH0tLS3Jl169YpMTFRM2fOVHt7uyZPnqzNmzcrISHBndm6dasWLVrk3sVUVlam6urqvj5cAABgKY8xxvT3QVwL0WhUjuMoEonw/hgAAPrQvU+8rsMtn34rk3dXz+jT547n6zc/OwkAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAleKKmA0bNmjs2LFKT09Xenq6iouL9d///d/ufmOMVq1aJb/fr5SUFE2aNEmHDh2KeY6Ojg4tXLhQWVlZSk1NVVlZmU6cOBEzEw6HFQgE5DiOHMdRIBDQmTNnrvwsAQDAoBNXxIwYMUKrV6/W/v37tX//ft19993627/9WzdU1qxZo7Vr16q6ulr79u2Tz+fT1KlT1dbW5j5HRUWFtm/frpqaGu3atUtnz55VaWmpzp8/786Ul5crGAyqtrZWtbW1CgaDCgQCfXTKAADgapj+PoALzFUaPny4+dnPfma6u7uNz+czq1evdvedO3fOOI5jnnrqKWOMMWfOnDFJSUmmpqbGnTl58qQZMmSIqa2tNcYYc/jwYSPJ7Nmzx51paGgwksyRI0d6fVyRSMRIMpFI5GpPEQAA/JFp618zIx99yYx89KU+f+54vn5f8Xtizp8/r5qaGn300UcqLi5Wc3OzQqGQSkpK3Bmv16uJEydq9+7dkqSmpiZ1dXXFzPj9fhUUFLgzDQ0NchxHRUVF7sz48ePlOI47cykdHR2KRqMxDwAA0Pc8/X0An4k7Yg4cOKCvfOUr8nq9evjhh7V9+3Z9/etfVygUkiTl5OTEzOfk5Lj7QqGQkpOTNXz48MvOZGdn93jd7Oxsd+ZSqqqq3PfQOI6j3NzceE8NAABYJO6IGTVqlILBoPbs2aPvfe97mj17tg4fPuzu93hi+8wY02PbxS6eudT8lz3PihUrFIlE3Mfx48d7e0oAAMBCcUdMcnKy/uzP/kzjxo1TVVWVbrvtNj3xxBPy+XyS1ONqSWtrq3t1xufzqbOzU+Fw+LIzp06d6vG6p0+f7nGV5495vV73rqkLDwAAMHhd9feJMcaoo6NDeXl58vl8qqurc/d1dnaqvr5eEyZMkCQVFhYqKSkpZqalpUUHDx50Z4qLixWJRNTY2OjO7N27V5FIxJ0BAABIjGf4+9//vqZPn67c3Fy1tbWppqZGr776qmpra+XxeFRRUaHKykrl5+crPz9flZWVGjZsmMrLyyVJjuNozpw5Wrp0qTIzM5WRkaFly5ZpzJgxmjJliiRp9OjRmjZtmubOnauNGzdKkubNm6fS0lKNGjWqj08fAADYKq6IOXXqlAKBgFpaWuQ4jsaOHava2lpNnTpVkrR8+XK1t7dr/vz5CofDKioq0o4dO5SWluY+x7p165SYmKiZM2eqvb1dkydP1ubNm5WQkODObN26VYsWLXLvYiorK1N1dXVfnC8AABgkPMaYAfM9a/pSNBqV4ziKRCK8PwYAgD507xOv63DLp9/K5N3VM/r0ueP5+s3PTgIAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYKa6Iqaqq0h133KG0tDRlZ2frm9/8po4ePRozY4zRqlWr5Pf7lZKSokmTJunQoUMxMx0dHVq4cKGysrKUmpqqsrIynThxImYmHA4rEAjIcRw5jqNAIKAzZ85c2VkCAIBBJ66Iqa+v1yOPPKI9e/aorq5On3zyiUpKSvTRRx+5M2vWrNHatWtVXV2tffv2yefzaerUqWpra3NnKioqtH37dtXU1GjXrl06e/asSktLdf78eXemvLxcwWBQtbW1qq2tVTAYVCAQ6INTBgAAV8P09wFcYK5Ca2urkWTq6+uNMcZ0d3cbn89nVq9e7c6cO3fOOI5jnnrqKWOMMWfOnDFJSUmmpqbGnTl58qQZMmSIqa2tNcYYc/jwYSPJ7Nmzx51paGgwksyRI0d6dWyRSMRIMpFI5GpOEQAAXGTa+tfMyEdfMiMffanPnzuer99X9Z6YSCQiScrIyJAkNTc3KxQKqaSkxJ3xer2aOHGidu/eLUlqampSV1dXzIzf71dBQYE709DQIMdxVFRU5M6MHz9ejuO4Mxfr6OhQNBqNeQAAgL7n6e8D+MwVR4wxRkuWLNGdd96pgoICSVIoFJIk5eTkxMzm5OS4+0KhkJKTkzV8+PDLzmRnZ/d4zezsbHfmYlVVVe77ZxzHUW5u7pWeGgAAsMAVR8yCBQv029/+Vr/4xS967PN4YhvNGNNj28UunrnU/OWeZ8WKFYpEIu7j+PHjvTkNAABgqSuKmIULF+rFF1/UK6+8ohEjRrjbfT6fJPW4WtLa2upenfH5fOrs7FQ4HL7szKlTp3q87unTp3tc5bnA6/UqPT095gEAAAavuCLGGKMFCxbohRde0G9+8xvl5eXF7M/Ly5PP51NdXZ27rbOzU/X19ZowYYIkqbCwUElJSTEzLS0tOnjwoDtTXFysSCSixsZGd2bv3r2KRCLuDAAAuLElxjP8yCOPaNu2bfqP//gPpaWluVdcHMdRSkqKPB6PKioqVFlZqfz8fOXn56uyslLDhg1TeXm5OztnzhwtXbpUmZmZysjI0LJlyzRmzBhNmTJFkjR69GhNmzZNc+fO1caNGyVJ8+bNU2lpqUaNGtWX5w8AACwVV8Rs2LBBkjRp0qSY7c8++6y+853vSJKWL1+u9vZ2zZ8/X+FwWEVFRdqxY4fS0tLc+XXr1ikxMVEzZ85Ue3u7Jk+erM2bNyshIcGd2bp1qxYtWuTexVRWVqbq6uorOUcAADAIeYwxA+Z71vSlaDQqx3EUiUR4fwwAAH3o3ide1+GWT7+VyburZ/Tpc8fz9ZufnQQAAKxExAAAACsRMQAAwEpEDAAAsBIRAwAArETEAAAAKxExAADASkQMAACwEhEDAACsRMQAAAArETEAAMBKRAwAALASEQMAAKxExAAAACsRMQAAwEpEDAAAsBIRAwAArETEAAAAKxExAADASkQMAACwEhEDAACsRMQAAAArETEAAMBKRAwAALASEQMAAOJi+vsAPkPEAAAAKxExAAAgLp7+PoDPEDEAAMBKRAwAALASEQMAAKxExAAAACsRMQAAwEpEDAAAsBIRAwAArETEAAAAKxExAADASkQMAACwEhEDAACsRMQAAAArETEAAMBKRAwAALASEQMAAKxExAAAACsRMQAAwEpEDAAAsBIRAwAArETEAAAAKxExAADASkQMAACwEhEDAACsRMQAAAArETEAAMBKRAwAALASEQMAAKxExAAAACvFHTGvvfaa7rvvPvn9fnk8Hv3qV7+K2W+M0apVq+T3+5WSkqJJkybp0KFDMTMdHR1auHChsrKylJqaqrKyMp04cSJmJhwOKxAIyHEcOY6jQCCgM2fOxH2CAABgcIo7Yj766CPddtttqq6uvuT+NWvWaO3ataqurta+ffvk8/k0depUtbW1uTMVFRXavn27ampqtGvXLp09e1alpaU6f/68O1NeXq5gMKja2lrV1tYqGAwqEAhcwSkCAIDBKDHeD5g+fbqmT59+yX3GGK1fv14rV67U/fffL0l67rnnlJOTo23btum73/2uIpGInnnmGf385z/XlClTJElbtmxRbm6udu7cqXvuuUdvvfWWamtrtWfPHhUVFUmSNm3apOLiYh09elSjRo260vMFAABXyfT3AXymT98T09zcrFAopJKSEneb1+vVxIkTtXv3bklSU1OTurq6Ymb8fr8KCgrcmYaGBjmO4waMJI0fP16O47gzF+vo6FA0Go15AACAwatPIyYUCkmScnJyYrbn5OS4+0KhkJKTkzV8+PDLzmRnZ/d4/uzsbHfmYlVVVe77ZxzHUW5u7lWfDwAAGLiuyd1JHo8n5tfGmB7bLnbxzKXmL/c8K1asUCQScR/Hjx+/giMHAABf5vJf0a+fPo0Yn88nST2ulrS2trpXZ3w+nzo7OxUOhy87c+rUqR7Pf/r06R5XeS7wer1KT0+PeQAAgMGrTyMmLy9PPp9PdXV17rbOzk7V19drwoQJkqTCwkIlJSXFzLS0tOjgwYPuTHFxsSKRiBobG92ZvXv3KhKJuDMAAODGFvfdSWfPntXvfvc799fNzc0KBoPKyMjQzTffrIqKClVWVio/P1/5+fmqrKzUsGHDVF5eLklyHEdz5szR0qVLlZmZqYyMDC1btkxjxoxx71YaPXq0pk2bprlz52rjxo2SpHnz5qm0tJQ7kwAAgKQriJj9+/frrrvucn+9ZMkSSdLs2bO1efNmLV++XO3t7Zo/f77C4bCKioq0Y8cOpaWluR+zbt06JSYmaubMmWpvb9fkyZO1efNmJSQkuDNbt27VokWL3LuYysrKvvB70wAAgBuPxxgzUG737lPRaFSO4ygSifD+GAAA+tC9T7yuwy2ffiuTd1fP6NPnjufrNz87CQAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAcTH9fQCfIWIAAICViBgAABAXT38fwGeIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAcYme6+rvQ5BkQcQ8+eSTysvL09ChQ1VYWKjXX3+9vw8JAIAb2olwe38fgqQBHjHPP/+8KioqtHLlSr355pv667/+a02fPl3Hjh3r70MDAAD9bEBHzNq1azVnzhz94z/+o0aPHq3169crNzdXGzZs6O9DAwAA/Syxvw/gi3R2dqqpqUmPPfZYzPaSkhLt3r27x3xHR4c6OjrcX0ej0WtyXG+fatMzrzfL45E8HknyfPrv0mf/9FyT170Uz/V7qet4VpLnep4YcJEdh0JKSU7QHV/LUFLCkEv+f/Zln6F8Dt9YjDH9fQg3rAEbMR988IHOnz+vnJycmO05OTkKhUI95quqqvTDH/7wmh9XS+Scnt9//Jq/DoD+9f9Of9TfhwDgSwzYiLng4j/RGGMu+aecFStWaMmSJe6vo9GocnNz+/x48jJTtazk/3x2LJJx/2l0PWP8unb/dTyx63le1/e/F39S62vX6qrnO61tCkXO6a5bs9XdbT6/5PnZJ8yX/Ze80f9QfqNehLrRTvvQ+1H9+kirHv+/Bf16HAM2YrKyspSQkNDjqktra2uPqzOS5PV65fV6r/lx3Zw5TAvuzr/mrwMAAC5vwL6xNzk5WYWFhaqrq4vZXldXpwkTJvTTUQEAgIFiwF6JkaQlS5YoEAho3LhxKi4u1tNPP61jx47p4Ycf7u9DAwAA/WxAR8y3vvUtffjhh/rRj36klpYWFRQU6OWXX9bIkSP7+9AAAEA/85hBem9YNBqV4ziKRCJKT0/v78MBAAC9EM/X7wH7nhgAAIDLIWIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYCUiBgAAWImIAQAAVhrQP3bgalz4RsTRaLSfjwQAAPTWha/bvfmBAoM2Ytra2iRJubm5/XwkAAAgXm1tbXIc57Izg/ZnJ3V3d+v9999XWlqaPB5Pnz53NBpVbm6ujh8/zs9l+hKsVe+xVr3HWvUeaxUf1qv3rtVaGWPU1tYmv9+vIUMu/66XQXslZsiQIRoxYsQ1fY309HQ+yXuJteo91qr3WKveY63iw3r13rVYqy+7AnMBb+wFAABWImIAAICViJgr4PV69YMf/EBer7e/D2XAY616j7XqPdaq91ir+LBevTcQ1mrQvrEXAAAMblyJAQAAViJiAACAlYgYAABgJSIGAABYiYiJ05NPPqm8vDwNHTpUhYWFev311/v7kK6pqqoq3XHHHUpLS1N2dra++c1v6ujRozEzxhitWrVKfr9fKSkpmjRpkg4dOhQz09HRoYULFyorK0upqakqKyvTiRMnYmbC4bACgYAcx5HjOAoEAjpz5sy1PsVrpqqqSh6PRxUVFe421upzJ0+e1Le//W1lZmZq2LBh+vM//3M1NTW5+1mrz33yySf6l3/5F+Xl5SklJUW33HKLfvSjH6m7u9uduVHX67XXXtN9990nv98vj8ejX/3qVzH7r+e6HDt2TPfdd59SU1OVlZWlRYsWqbOz81qc9hW53Fp1dXXp0Ucf1ZgxY5Samiq/36+HHnpI77//fsxzDLi1Mui1mpoak5SUZDZt2mQOHz5sFi9ebFJTU817773X34d2zdxzzz3m2WefNQcPHjTBYNDMmDHD3Hzzzebs2bPuzOrVq01aWpr55S9/aQ4cOGC+9a1vmZtuuslEo1F35uGHHzZf/epXTV1dnXnjjTfMXXfdZW677TbzySefuDPTpk0zBQUFZvfu3Wb37t2moKDAlJaWXtfz7SuNjY3ma1/7mhk7dqxZvHixu521+tQf/vAHM3LkSPOd73zH7N271zQ3N5udO3ea3/3ud+4Ma/W5H//4xyYzM9O89NJLprm52fz7v/+7+cpXvmLWr1/vztyo6/Xyyy+blStXml/+8pdGktm+fXvM/uu1Lp988okpKCgwd911l3njjTdMXV2d8fv9ZsGCBdd8DXrrcmt15swZM2XKFPP888+bI0eOmIaGBlNUVGQKCwtjnmOgrRURE4e//Mu/NA8//HDMtltvvdU89thj/XRE119ra6uRZOrr640xxnR3dxufz2dWr17tzpw7d844jmOeeuopY8yn/3MkJSWZmpoad+bkyZNmyJAhpra21hhjzOHDh40ks2fPHnemoaHBSDJHjhy5HqfWZ9ra2kx+fr6pq6szEydOdCOGtfrco48+au68884v3M9axZoxY4b5h3/4h5ht999/v/n2t79tjGG9Lrj4C/P1XJeXX37ZDBkyxJw8edKd+cUvfmG8Xq+JRCLX5HyvxqWC72KNjY1GkvsH9YG4Vvx1Ui91dnaqqalJJSUlMdtLSkq0e/fufjqq6y8SiUiSMjIyJEnNzc0KhUIx6+L1ejVx4kR3XZqamtTV1RUz4/f7VVBQ4M40NDTIcRwVFRW5M+PHj5fjONat7yOPPKIZM2ZoypQpMdtZq8+9+OKLGjdunP7+7/9e2dnZuv3227Vp0yZ3P2sV684779Svf/1rvf3225Kk//3f/9WuXbt07733SmK9vsj1XJeGhgYVFBTI7/e7M/fcc486Ojpi/prUJpFIRB6PR3/yJ38iaWCu1aD9AZB97YMPPtD58+eVk5MTsz0nJ0ehUKifjur6MsZoyZIluvPOO1VQUCBJ7rlfal3ee+89dyY5OVnDhw/vMXPh40OhkLKzs3u8ZnZ2tlXrW1NTo6amJu3fv7/HPtbqc7///e+1YcMGLVmyRN///vfV2NioRYsWyev16qGHHmKtLvLoo48qEono1ltvVUJCgs6fP6/HH39cDz74oCQ+t77I9VyXUCjU43WGDx+u5ORkK9fu3Llzeuyxx1ReXu7+cMeBuFZETJw8Hk/Mr40xPbYNVgsWLNBvf/tb7dq1q8e+K1mXi2cuNW/T+h4/flyLFy/Wjh07NHTo0C+cY62k7u5ujRs3TpWVlZKk22+/XYcOHdKGDRv00EMPuXOs1aeef/55bdmyRdu2bdM3vvENBYNBVVRUyO/3a/bs2e4c63Vp12tdBsvadXV16YEHHlB3d7eefPLJL53vz7Xir5N6KSsrSwkJCT0qsbW1tUdRDkYLFy7Uiy++qFdeeUUjRoxwt/t8Pkm67Lr4fD51dnYqHA5fdubUqVM9Xvf06dPWrG9TU5NaW1tVWFioxMREJSYmqr6+Xj/96U+VmJjongdrJd100036+te/HrNt9OjROnbsmCQ+ry72z//8z3rsscf0wAMPaMyYMQoEAvqnf/onVVVVSWK9vsj1XBefz9fjdcLhsLq6uqxau66uLs2cOVPNzc2qq6tzr8JIA3OtiJheSk5OVmFhoerq6mK219XVacKECf10VNeeMUYLFizQCy+8oN/85jfKy8uL2Z+XlyefzxezLp2dnaqvr3fXpbCwUElJSTEzLS0tOnjwoDtTXFysSCSixsZGd2bv3r2KRCLWrO/kyZN14MABBYNB9zFu3DjNmjVLwWBQt9xyC2v1mb/6q7/qcav+22+/rZEjR0ri8+piH3/8sYYMif3tOiEhwb3FmvW6tOu5LsXFxTp48KBaWlrcmR07dsjr9aqwsPCanmdfuRAw77zzjnbu3KnMzMyY/QNyreJ6G/AN7sIt1s8884w5fPiwqaioMKmpqebdd9/t70O7Zr73ve8Zx3HMq6++alpaWtzHxx9/7M6sXr3aOI5jXnjhBXPgwAHz4IMPXvIWxhEjRpidO3eaN954w9x9992XvC1v7NixpqGhwTQ0NJgxY8YM6Fs7e+OP704yhrW6oLGx0SQmJprHH3/cvPPOO2br1q1m2LBhZsuWLe4Ma/W52bNnm69+9avuLdYvvPCCycrKMsuXL3dnbtT1amtrM2+++aZ58803jSSzdu1a8+abb7p31Fyvdblw2/DkyZPNG2+8YXbu3GlGjBgxoG6xvtxadXV1mbKyMjNixAgTDAZjfr/v6Ohwn2OgrRURE6d//dd/NSNHjjTJycnmL/7iL9xbjQcrSZd8PPvss+5Md3e3+cEPfmB8Pp/xer3mb/7mb8yBAwdinqe9vd0sWLDAZGRkmJSUFFNaWmqOHTsWM/Phhx+aWbNmmbS0NJOWlmZmzZplwuHwdTjLa+fiiGGtPvef//mfpqCgwHi9XnPrrbeap59+OmY/a/W5aDRqFi9ebG6++WYzdOhQc8stt5iVK1fGfHG5UdfrlVdeueTvUbNnzzbGXN91ee+998yMGTNMSkqKycjIMAsWLDDnzp27lqcfl8utVXNz8xf+fv/KK6+4zzHQ1spjjDHxXbsBAADof7wnBgAAWImIAQAAViJiAACAlYgYAABgJSIGAABYiYgBAABWImIAAICViBgAAGAlIgYAAFiJiAEAAFYiYgAAgJWIGAAAYKX/Dzb5UHuwT3eRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losshistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e027874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28092\\2187313917.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "u00_1, v00_1, u00_2, v00_2 = model.predict1(test10_x,test10_t)\n",
    "u11_1, v11_1, u11_2, v11_2 = model.predict1(testb_x1,testb_t1)\n",
    "u22_1, v22_1, u22_2, v22_2 = model.predict1(testb_x2,testb_t2)\n",
    "a1,b1,c1,d1, ux11_1, vx11_1, ux11_2, vx11_2 = model.predict2(testb_x1,testb_t1)\n",
    "a1,b1,c1,d1, ux22_1, vx22_1, ux22_2, vx22_2 = model.predict2(testb_x2,testb_t2)\n",
    "\n",
    "f1, f2, f3, f4,a1,b1,c1,d1 = model.predict2(testu_x,testu_t)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f5c0224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_u0: 2.88422e-01, Error_ub: 2.42360e+00, Error_ubx: 6.23008e-02, Error_f: 4.45312e+01\n"
     ]
    }
   ],
   "source": [
    "error_u0= ((np.linalg.norm(test10_r - u00_1, 2))**2+(np.linalg.norm(test10_c - v00_1, 2))**2+(np.linalg.norm(test20_r - u00_2, 2))**2+(np.linalg.norm(test20_c - v00_2, 2))**2)/20\n",
    "error_ub= ((np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2+(np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2)/20\n",
    "error_ubx= (\n",
    "    (np.linalg.norm(ux11_1.cpu().detach().numpy() - ux22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_1.cpu().detach().numpy() - vx22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(ux11_2.cpu().detach().numpy() - ux22_2.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_2.cpu().detach().numpy() - vx22_2.cpu().detach().numpy(), 2))**2)/20\n",
    "\n",
    "error_f= ((np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2+(np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2)/2000\n",
    "print('Error_u0: %.5e, Error_ub: %.5e, Error_ubx: %.5e, Error_f: %.5e' % (error_u0.item(), error_ub.item(), error_ubx.item(), error_f.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
