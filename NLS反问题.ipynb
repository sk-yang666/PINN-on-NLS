{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20cdd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, optim \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import random\n",
    "from torch.utils import data\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8e41b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dca358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=pd.read_csv(\"data_x_new.csv\")\n",
    "data_t=pd.read_csv(\"data_t_new.csv\")\n",
    "data_xx=pd.read_csv(\"data_x_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "969f73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h10_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h10_t=data_x.iloc[:, [1]]\n",
    "h10_R=data_x.iloc[:,[2]]#初值h实部\n",
    "h10_C=data_x.iloc[:,[3]]#初值h虚部\n",
    "hb_x1=data_t.iloc[:,[1]]#边值-5，t\n",
    "hb_t1=data_t.iloc[:,[0]]\n",
    "hb_x2=data_t.iloc[:,[2]]#边值5，t\n",
    "hb_t2=data_t.iloc[:,[0]]\n",
    "\n",
    "h20_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h20_t=data_x.iloc[:, [1]]\n",
    "h20_R=data_x.iloc[:,[4]]#初值h实部\n",
    "h20_C=data_x.iloc[:,[5]]#初值h虚部\n",
    "\n",
    "h30_x=data_xx.iloc[:, [0]]\n",
    "h30_t=data_xx.iloc[:, [1]]\n",
    "h30_r1=data_xx.iloc[:, [2]]\n",
    "h30_c1=data_xx.iloc[:, [3]]\n",
    "h30_r2=data_xx.iloc[:, [4]]\n",
    "h30_c2=data_xx.iloc[:, [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76bd3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x['x']\n",
    "t = data_t['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b2ca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc71aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf= np.hstack((X.flatten()[:,None], T.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "950258e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_x=hf[:,[0]]\n",
    "hf_t=hf[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78ff78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train10_x,test10_x,train10_t,test10_t,train10_r,test10_r,train10_c,test10_c,trainb_x1,testb_x1,trainb_t1,testb_t1,trainb_x2,testb_x2,trainb_t2,testb_t2,train20_r,test20_r,train20_c,test20_c = train_test_split(h10_x,h10_t,h10_R,h10_C,hb_x1,hb_t1,hb_x2,hb_t2,h20_R,h20_C,test_size=0.2)\n",
    "\n",
    "train30_x,test30_x,train30_t,test30_t,train30_r1,test30_r1,train30_c1,test30_c1,train30_r2,test30_r2,train30_c2,test30_c2=train_test_split(h30_x,h30_t,h30_r1,h30_c1,h30_r2,h30_c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8301e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainu_x, testu_x, trainu_t, testu_t = train_test_split(hf_x, hf_t, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68c1b3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train10_x=torch.from_numpy(train10_x.to_numpy()).float()\n",
    "train10_t=torch.from_numpy(train10_t.to_numpy()).float()\n",
    "\n",
    "train10_r=torch.from_numpy(train10_r.to_numpy()).float()\n",
    "train10_c=torch.from_numpy(train10_c.to_numpy()).float()\n",
    "\n",
    "trainb_x1=torch.from_numpy(trainb_x1.to_numpy()).float()\n",
    "trainb_t1=torch.from_numpy(trainb_t1.to_numpy()).float()\n",
    "trainb_x2=torch.from_numpy(trainb_x2.to_numpy()).float()\n",
    "trainb_t2=torch.from_numpy(trainb_t2.to_numpy()).float()\n",
    "\n",
    "# train20_x=torch.from_numpy(train20_x.to_numpy()).float()\n",
    "# train20_t=torch.from_numpy(train20_t.to_numpy()).float()\n",
    "train20_r=torch.from_numpy(train20_r.to_numpy()).float()\n",
    "train20_c=torch.from_numpy(train20_c.to_numpy()).float()\n",
    "\n",
    "\n",
    "train30_x=torch.from_numpy(train30_x.to_numpy()).float()\n",
    "train30_t=torch.from_numpy(train30_t.to_numpy()).float()\n",
    "train30_r1=torch.from_numpy(train30_r1.to_numpy()).float()\n",
    "train30_c1=torch.from_numpy(train30_c1.to_numpy()).float()\n",
    "train30_r2=torch.from_numpy(train30_r2.to_numpy()).float()\n",
    "train30_c2=torch.from_numpy(train30_c2.to_numpy()).float()\n",
    "\n",
    "trainu_x=torch.from_numpy(trainu_x).float()\n",
    "trainu_t=torch.from_numpy(trainu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2e60ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10_x=torch.from_numpy(test10_x.to_numpy()).float()\n",
    "test10_t=torch.from_numpy(test10_t.to_numpy()).float()\n",
    "test10_r=torch.from_numpy(test10_r.to_numpy()).float()\n",
    "test10_c=torch.from_numpy(test10_c.to_numpy()).float()\n",
    "\n",
    "testb_x1=torch.from_numpy(testb_x1.to_numpy()).float()\n",
    "testb_t1=torch.from_numpy(testb_t1.to_numpy()).float()\n",
    "testb_x2=torch.from_numpy(testb_x2.to_numpy()).float()\n",
    "testb_t2=torch.from_numpy(testb_t2.to_numpy()).float()\n",
    "\n",
    "# test20_x=torch.from_numpy(test20_x.to_numpy()).float()\n",
    "# test20_t=torch.from_numpy(test20_t.to_numpy()).float()\n",
    "test20_r=torch.from_numpy(test20_r.to_numpy()).float()\n",
    "test20_c=torch.from_numpy(test20_c.to_numpy()).float()\n",
    "\n",
    "testu_x=torch.from_numpy(testu_x).float()\n",
    "testu_t=torch.from_numpy(testu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20c0aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7b860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f5e4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN():\n",
    "    def __init__(self, u10_x, u10_t, u10_r, u10_c, ub_x1, ub_t1, ub_x2, ub_t2, u20_r, u20_c, uf_x, uf_t,u30_x,u30_t,u30_r1,u30_c1,u30_r2,u30_c2):\n",
    "        self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
    "        self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
    "        self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
    "        self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
    "        self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
    "        self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
    "        self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
    "        \n",
    "#         self.u20_x = u20_x.clone().detach().requires_grad_(True).float().to(device)\n",
    "#         self.u20_t = torch.tensor(u20_t, requires_grad=True).float().to(device)\n",
    "        self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
    "        self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
    "        self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
    "        self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
    "        self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
    "        self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
    "        self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
    "      \n",
    "        \n",
    "        self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
    "        self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.tensor([0.8], requires_grad=True).to(device)\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        \n",
    "        self.lambda_2 = torch.tensor([2.2], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        self.dnn = DNN().to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params':self.dnn.parameters()},\n",
    "            {'params': [self.lambda_1], 'lr': 0.00021},\n",
    "            {'params': [self.lambda_2], 'lr': 0.0001}\n",
    "        ],lr=0.012 )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "        self.losshistory=[]\n",
    "    def net_u(self, x, t):  \n",
    "        u1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,0],1)\n",
    "        v1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,1],1)\n",
    "        u2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,2],1)\n",
    "        v2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,3],1)\n",
    "        return u1, v1, u2, v2\n",
    "\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        beta=1\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        lambda_3 = -1\n",
    "        lambda_4 = 0\n",
    "        u1, v1,u2, v2= self.net_u(x, t)\n",
    "        \n",
    "        u1_t = torch.autograd.grad(\n",
    "            u1, t, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_x = torch.autograd.grad(\n",
    "            u1, x, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_xx = torch.autograd.grad(\n",
    "            u1_x, x, \n",
    "            grad_outputs=torch.ones_like(u1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v1_t = torch.autograd.grad(\n",
    "            v1, t, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_x = torch.autograd.grad(\n",
    "            v1, x, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_xx = torch.autograd.grad(\n",
    "            v1_x, x, \n",
    "            grad_outputs=torch.ones_like(v1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        u2_t = torch.autograd.grad(\n",
    "            u2, t, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_x = torch.autograd.grad(\n",
    "            u2, x, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_xx = torch.autograd.grad(\n",
    "            u2_x, x, \n",
    "            grad_outputs=torch.ones_like(u2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v2_t = torch.autograd.grad(\n",
    "            v2, t, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_x = torch.autograd.grad(\n",
    "            v2, x, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_xx = torch.autograd.grad(\n",
    "            v2_x, x, \n",
    "            grad_outputs=torch.ones_like(v2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f_u1 = (\n",
    "            v1_t + u1_xx\n",
    "        + lambda_1*u1*(u1**2 + v1**2) + lambda_2*u1*(u2**2 + v2**2) + lambda_3*(u1*(u2**2 - v2**2) + 2*u2*v1*v2) + lambda_4*(u2*(u1**2 - v1**2) + 2*u1*v1*v2)\n",
    "    )\n",
    "\n",
    "        f_v1 = (\n",
    "                -u1_t + v1_xx\n",
    "            + lambda_1*v1*(u1**2 + v1**2) + lambda_2*v1*(u2**2 + v2**2) - lambda_3*(v1*(u2**2 - v2**2) - 2*u1*u2*v2) - lambda_4*(v2*(u1**2 - v1**2) - 2*u1*u2*v1)\n",
    "        )\n",
    "\n",
    "        f_u2 = (\n",
    "                v2_t + beta*u2_xx\n",
    "            + lambda_1*u2*(u2**2 + v2**2) + lambda_2*u2*(u1**2 + v1**2) + lambda_3*(u2*(u1**2 - v1**2) + 2*u1*v2*v1) + lambda_4*(u1*(u2**2 - v2**2) + 2*u2*v2*v1)\n",
    "        )\n",
    "\n",
    "        f_v2 = (\n",
    "                -u2_t + beta*v2_xx\n",
    "            + lambda_1*v2*(u2**2 + v2**2) + lambda_2*v2*(u1**2 + v1**2) - lambda_3*(v2*(u1**2 - v1**2) - 2*u2*u1*v1) - lambda_4*(v1*(u2**2 - v2**2) - 2*u2*u1*v2)\n",
    "        )\n",
    "        a=u1_x\n",
    "        b=v1_x\n",
    "        c=u2_x\n",
    "        d=v2_x\n",
    "        return f_u1, f_v1, f_u2, f_v2, a, b, c, d\n",
    "     \n",
    "    def loss_func(self):\n",
    "        for self.iter in tqdm(range(50000)):\n",
    "            torch.cuda.empty_cache()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            u10_pred, v10_pred, u20_pred, v20_pred = self.net_u(self.u10_x,self.u10_t)\n",
    "            u30_pred1, v30_pred1, u30_pred2, v30_pred2 = self.net_u(self.u30_x,self.u30_t)\n",
    "            u1b_pred1, v1b_pred1, u2b_pred1, v2b_pred1 = self.net_u(self.ub_x1,self.ub_t1)\n",
    "            u1b_pred2, v1b_pred2, u2b_pred2, v2b_pred2 = self .net_u(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            \n",
    "            a1,b1,c1,d1,u1bx_pred1, v1bx_pred1,u2bx_pred1, v2bx_pred1 = self.net_f(self.ub_x1,self.ub_t1)\n",
    "            a2,b2,c2,d2,u1bx_pred2, v1bx_pred2,u2bx_pred2, v2bx_pred2 = self.net_f(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            f_predu1, f_predv1, f_predu2, f_predv2,a1,b1,c1,d1 = self.net_f(self.uf_x,self.uf_t)\n",
    "            \n",
    "            loss_u0 = torch.mean((self.u10_r - u10_pred) ** 2)+torch.mean((self.u10_c - v10_pred) ** 2)+torch.mean((self.u20_r - u20_pred) ** 2)+torch.mean((self.u20_c - v20_pred) ** 2)\n",
    "            loss_ub = torch.mean((u1b_pred1 - u1b_pred2) ** 2)+torch.mean((v1b_pred1 - v1b_pred2) ** 2)+torch.mean((u2b_pred1 - u2b_pred2) ** 2)+torch.mean((v2b_pred1 - v2b_pred2) ** 2)\n",
    "            loss_ubx = torch.mean((u1bx_pred1 - u1bx_pred2) ** 2)+torch.mean((v1bx_pred1 - v1bx_pred2) ** 2)+torch.mean((u2bx_pred1 - u2bx_pred2) ** 2)+torch.mean((v2bx_pred1 - v2bx_pred2) ** 2)\n",
    "            loss_f = torch.mean(f_predu1 ** 2)+torch.mean(f_predv1 ** 2)+torch.mean(f_predu2 ** 2)+torch.mean(f_predv2 ** 2)\n",
    "            loss_u1=torch.mean((self.u30_r1 - u30_pred1) ** 2)+torch.mean((self.u30_c1 - v30_pred1) ** 2)+torch.mean((self.u30_r2 - u30_pred2) ** 2)+torch.mean((self.u30_c2 - v30_pred2) ** 2)\n",
    "            \n",
    "            loss = loss_f+loss_u0 + loss_ub + loss_ubx+loss_u1\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losshistory.append(loss.clone().detach().cpu())\n",
    "            \n",
    "            self.iter += 1\n",
    "            with torch.no_grad():\n",
    "                if self.iter % 500 == 0:\n",
    "                    print('Iter %d, Loss: %.5e, Loss_u0: %.5e, Loss_ub: %.5e, Loss_ubx: %.5e, Loss_f: %.5e, Loss_u1: %.5e, lambda1: %.5e, lambda2: %.5e' % (self.iter, loss.item(), loss_u0.item(), loss_ub.item(), loss_ubx.item(), loss_f.item(), loss_u1.item(), self.lambda_1,self.lambda_2))\n",
    "        return float(loss)\n",
    "        \n",
    " \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "        \n",
    "    def predict1(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u1, v1 , u2, v2= self.net_u(x, t)\n",
    "        u1 = u1.detach().cpu().numpy()\n",
    "        v1 = v1.detach().cpu().numpy()\n",
    "        u2 = u2.detach().cpu().numpy()\n",
    "        v2 = v2.detach().cpu().numpy()\n",
    "        return u1, v1, u2, v2\n",
    "    \n",
    "    def predict2(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        f1, f2, f3, f4, a, b, c, d = self.net_f(x, t)\n",
    "        f1 = f1.detach().cpu().numpy()\n",
    "        f2 = f2.detach().cpu().numpy()\n",
    "        f3 = f3.detach().cpu().numpy()\n",
    "        f4 = f4.detach().cpu().numpy()\n",
    "        return  f1, f2, f3, f4, a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08905a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PhysicsInformedNN(train10_x, train10_t, train10_r, train10_c, \n",
    "                          trainb_x1, trainb_t1, trainb_x2, trainb_t2,\n",
    "                        train20_r, train20_c, trainu_x, trainu_t,\n",
    "                         train30_x,train30_t, train30_r1, train30_c1,train30_r2,train30_c2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7626fd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/50000 [01:04<1:48:02,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500, Loss: 1.43137e-01, Loss_u0: 1.48627e-02, Loss_ub: 1.49355e-05, Loss_ubx: 8.44356e-06, Loss_f: 3.69829e-02, Loss_u1: 9.12680e-02, lambda1: 8.72215e-01, lambda2: 2.18399e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1001/50000 [02:08<1:48:39,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Loss: 1.31958e-01, Loss_u0: 9.21825e-03, Loss_ub: 1.12949e-05, Loss_ubx: 1.53566e-05, Loss_f: 4.02607e-02, Loss_u1: 8.24520e-02, lambda1: 9.10708e-01, lambda2: 2.15191e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1501/50000 [03:12<1:40:28,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1500, Loss: 1.00501e-01, Loss_u0: 2.62592e-03, Loss_ub: 2.20755e-05, Loss_ubx: 1.05109e-05, Loss_f: 2.94407e-02, Loss_u1: 6.84014e-02, lambda1: 9.27444e-01, lambda2: 2.13349e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2001/50000 [04:16<1:42:23,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2000, Loss: 8.47002e-02, Loss_u0: 1.41431e-03, Loss_ub: 9.52451e-06, Loss_ubx: 2.45954e-05, Loss_f: 2.05244e-02, Loss_u1: 6.27274e-02, lambda1: 9.28000e-01, lambda2: 2.12787e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2501/50000 [05:20<1:42:36,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2500, Loss: 1.46439e-01, Loss_u0: 4.55466e-03, Loss_ub: 4.66211e-05, Loss_ubx: 2.27249e-05, Loss_f: 6.01664e-02, Loss_u1: 8.16489e-02, lambda1: 9.59157e-01, lambda2: 2.12020e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3001/50000 [06:25<1:38:26,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3000, Loss: 9.00587e-02, Loss_u0: 3.51494e-03, Loss_ub: 1.17443e-05, Loss_ubx: 3.11293e-05, Loss_f: 2.42183e-02, Loss_u1: 6.22826e-02, lambda1: 9.77775e-01, lambda2: 2.09475e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3501/50000 [07:29<1:40:21,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3500, Loss: 1.87639e-01, Loss_u0: 3.25088e-03, Loss_ub: 1.12136e-06, Loss_ubx: 2.20794e-06, Loss_f: 7.73075e-02, Loss_u1: 1.07077e-01, lambda1: 9.93969e-01, lambda2: 2.08377e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4001/50000 [08:33<1:38:00,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4000, Loss: 8.55616e-02, Loss_u0: 1.90204e-03, Loss_ub: 1.12221e-05, Loss_ubx: 1.27553e-05, Loss_f: 1.90972e-02, Loss_u1: 6.45383e-02, lambda1: 9.98885e-01, lambda2: 2.07848e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4501/50000 [09:37<1:37:50,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4500, Loss: 7.49195e-02, Loss_u0: 1.63701e-03, Loss_ub: 1.13478e-06, Loss_ubx: 1.90543e-06, Loss_f: 2.02734e-02, Loss_u1: 5.30060e-02, lambda1: 9.94685e-01, lambda2: 2.06992e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5001/50000 [10:41<1:37:26,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5000, Loss: 6.15331e-02, Loss_u0: 1.53297e-03, Loss_ub: 3.07463e-06, Loss_ubx: 3.71498e-06, Loss_f: 1.48105e-02, Loss_u1: 4.51828e-02, lambda1: 9.90134e-01, lambda2: 2.06080e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5501/50000 [11:45<1:36:06,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5500, Loss: 4.92083e-02, Loss_u0: 1.20655e-03, Loss_ub: 4.00695e-06, Loss_ubx: 4.64708e-06, Loss_f: 1.12055e-02, Loss_u1: 3.67875e-02, lambda1: 9.81025e-01, lambda2: 2.04975e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6001/50000 [12:48<1:30:44,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6000, Loss: 4.59024e-02, Loss_u0: 1.10289e-03, Loss_ub: 8.65442e-07, Loss_ubx: 1.61092e-06, Loss_f: 1.08821e-02, Loss_u1: 3.39149e-02, lambda1: 9.69525e-01, lambda2: 2.03669e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6501/50000 [13:52<1:35:20,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6500, Loss: 9.68499e-01, Loss_u0: 4.68706e-01, Loss_ub: 6.45317e-17, Loss_ubx: 1.85172e-16, Loss_f: 2.21283e-02, Loss_u1: 4.77665e-01, lambda1: 9.45775e-01, lambda2: 2.01967e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7001/50000 [14:56<1:33:12,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7000, Loss: 9.56111e-01, Loss_u0: 4.59216e-01, Loss_ub: 1.87281e-15, Loss_ubx: 5.48355e-15, Loss_f: 2.33401e-02, Loss_u1: 4.73555e-01, lambda1: 9.45775e-01, lambda2: 2.01967e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 7501/50000 [16:00<1:29:58,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7500, Loss: 9.51639e-01, Loss_u0: 4.63089e-01, Loss_ub: 1.93665e-15, Loss_ubx: 5.94761e-15, Loss_f: 1.89575e-02, Loss_u1: 4.69592e-01, lambda1: 9.45775e-01, lambda2: 2.01967e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8001/50000 [17:03<1:29:43,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8000, Loss: 9.21635e-01, Loss_u0: 4.33470e-01, Loss_ub: 5.07927e-15, Loss_ubx: 1.51200e-14, Loss_f: 2.62273e-02, Loss_u1: 4.61938e-01, lambda1: 9.45775e-01, lambda2: 2.01967e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 8476/50000 [18:04<1:28:34,  7.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28096\\3773420853.py\u001b[0m in \u001b[0;36mloss_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss_u0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_ub\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_ubx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss_u1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.losshistory=[]            \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "117e3d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268597665b0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losshistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e027874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_3512\\3777535164.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "u00_1, v00_1, u00_2, v00_2 = model.predict1(test10_x,test10_t)\n",
    "u11_1, v11_1, u11_2, v11_2 = model.predict1(testb_x1,testb_t1)\n",
    "u22_1, v22_1, u22_2, v22_2 = model.predict1(testb_x2,testb_t2)\n",
    "a1,b1,c1,d1, ux11_1, vx11_1, ux11_2, vx11_2 = model.predict2(testb_x1,testb_t1)\n",
    "a1,b1,c1,d1, ux22_1, vx22_1, ux22_2, vx22_2 = model.predict2(testb_x2,testb_t2)\n",
    "\n",
    "f1, f2, f3, f4,a1,b1,c1,d1 = model.predict2(testu_x,testu_t)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c0224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_u0: 5.27684e-02, Error_ub: 0.00000e+00, Error_ubx: 4.74056e-67, Error_f: 8.12052e-08\n"
     ]
    }
   ],
   "source": [
    "error_u0= ((np.linalg.norm(test10_r - u00_1, 2))**2+(np.linalg.norm(test10_c - v00_1, 2))**2+(np.linalg.norm(test20_r - u00_2, 2))**2+(np.linalg.norm(test20_c - v00_2, 2))**2)/20\n",
    "error_ub= ((np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2+(np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2)/20\n",
    "error_ubx= (\n",
    "    (np.linalg.norm(ux11_1.cpu().detach().numpy() - ux22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_1.cpu().detach().numpy() - vx22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(ux11_2.cpu().detach().numpy() - ux22_2.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_2.cpu().detach().numpy() - vx22_2.cpu().detach().numpy(), 2))**2)/20\n",
    "\n",
    "error_f= ((np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2+(np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2)/2000\n",
    "print('Error_u0: %.5e, Error_ub: %.5e, Error_ubx: %.5e, Error_f: %.5e' % (error_u0.item(), error_ub.item(), error_ubx.item(), error_f.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
