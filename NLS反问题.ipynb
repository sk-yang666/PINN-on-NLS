{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cdd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, optim \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import random\n",
    "from torch.utils import data\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e41b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dca358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=pd.read_csv(\"data_x_new.csv\")\n",
    "data_t=pd.read_csv(\"data_t_new.csv\")\n",
    "data_xx=pd.read_csv(\"data_x_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969f73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h10_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h10_t=data_x.iloc[:, [1]]\n",
    "h10_R=data_x.iloc[:,[2]]#初值h实部\n",
    "h10_C=data_x.iloc[:,[3]]#初值h虚部\n",
    "hb_x1=data_t.iloc[:,[1]]#边值-5，t\n",
    "hb_t1=data_t.iloc[:,[0]]\n",
    "hb_x2=data_t.iloc[:,[2]]#边值5，t\n",
    "hb_t2=data_t.iloc[:,[0]]\n",
    "\n",
    "h20_x=data_x.iloc[:, [0]]#初值x,t\n",
    "h20_t=data_x.iloc[:, [1]]\n",
    "h20_R=data_x.iloc[:,[4]]#初值h实部\n",
    "h20_C=data_x.iloc[:,[5]]#初值h虚部\n",
    "\n",
    "h30_x=data_xx.iloc[:, [0]]\n",
    "h30_t=data_xx.iloc[:, [1]]\n",
    "h30_r1=data_xx.iloc[:, [2]]\n",
    "h30_c1=data_xx.iloc[:, [3]]\n",
    "h30_r2=data_xx.iloc[:, [4]]\n",
    "h30_c2=data_xx.iloc[:, [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bd3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x['x']\n",
    "t = data_t['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2ca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc71aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf= np.hstack((X.flatten()[:,None], T.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950258e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_x=hf[:,[0]]\n",
    "hf_t=hf[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ff78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train10_x,test10_x,train10_t,test10_t,train10_r,test10_r,train10_c,test10_c,trainb_x1,testb_x1,trainb_t1,testb_t1,trainb_x2,testb_x2,trainb_t2,testb_t2,train20_r,test20_r,train20_c,test20_c = train_test_split(h10_x,h10_t,h10_R,h10_C,hb_x1,hb_t1,hb_x2,hb_t2,h20_R,h20_C,test_size=0.2)\n",
    "\n",
    "train30_x,test30_x,train30_t,test30_t,train30_r1,test30_r1,train30_c1,test30_c1,train30_r2,test30_r2,train30_c2,test30_c2=train_test_split(h30_x,h30_t,h30_r1,h30_c1,h30_r2,h30_c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8301e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainu_x, testu_x, trainu_t, testu_t = train_test_split(hf_x, hf_t, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c1b3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train10_x=torch.from_numpy(train10_x.to_numpy()).float()\n",
    "train10_t=torch.from_numpy(train10_t.to_numpy()).float()\n",
    "\n",
    "train10_r=torch.from_numpy(train10_r.to_numpy()).float()\n",
    "train10_c=torch.from_numpy(train10_c.to_numpy()).float()\n",
    "\n",
    "trainb_x1=torch.from_numpy(trainb_x1.to_numpy()).float()\n",
    "trainb_t1=torch.from_numpy(trainb_t1.to_numpy()).float()\n",
    "trainb_x2=torch.from_numpy(trainb_x2.to_numpy()).float()\n",
    "trainb_t2=torch.from_numpy(trainb_t2.to_numpy()).float()\n",
    "\n",
    "# train20_x=torch.from_numpy(train20_x.to_numpy()).float()\n",
    "# train20_t=torch.from_numpy(train20_t.to_numpy()).float()\n",
    "train20_r=torch.from_numpy(train20_r.to_numpy()).float()\n",
    "train20_c=torch.from_numpy(train20_c.to_numpy()).float()\n",
    "\n",
    "\n",
    "train30_x=torch.from_numpy(train30_x.to_numpy()).float()\n",
    "train30_t=torch.from_numpy(train30_t.to_numpy()).float()\n",
    "train30_r1=torch.from_numpy(train30_r1.to_numpy()).float()\n",
    "train30_c1=torch.from_numpy(train30_c1.to_numpy()).float()\n",
    "train30_r2=torch.from_numpy(train30_r2.to_numpy()).float()\n",
    "train30_c2=torch.from_numpy(train30_c2.to_numpy()).float()\n",
    "\n",
    "trainu_x=torch.from_numpy(trainu_x).float()\n",
    "trainu_t=torch.from_numpy(trainu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e60ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10_x=torch.from_numpy(test10_x.to_numpy()).float()\n",
    "test10_t=torch.from_numpy(test10_t.to_numpy()).float()\n",
    "test10_r=torch.from_numpy(test10_r.to_numpy()).float()\n",
    "test10_c=torch.from_numpy(test10_c.to_numpy()).float()\n",
    "\n",
    "testb_x1=torch.from_numpy(testb_x1.to_numpy()).float()\n",
    "testb_t1=torch.from_numpy(testb_t1.to_numpy()).float()\n",
    "testb_x2=torch.from_numpy(testb_x2.to_numpy()).float()\n",
    "testb_t2=torch.from_numpy(testb_t2.to_numpy()).float()\n",
    "\n",
    "# test20_x=torch.from_numpy(test20_x.to_numpy()).float()\n",
    "# test20_t=torch.from_numpy(test20_t.to_numpy()).float()\n",
    "test20_r=torch.from_numpy(test20_r.to_numpy()).float()\n",
    "test20_c=torch.from_numpy(test20_c.to_numpy()).float()\n",
    "\n",
    "testu_x=torch.from_numpy(testu_x).float()\n",
    "testu_t=torch.from_numpy(testu_t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c0aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(60, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f5e4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN():\n",
    "    def __init__(self, u10_x, u10_t, u10_r, u10_c, ub_x1, ub_t1, ub_x2, ub_t2, u20_r, u20_c, uf_x, uf_t,u30_x,u30_t,u30_r1,u30_c1,u30_r2,u30_c2):\n",
    "        self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
    "        self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
    "        self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
    "        self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
    "        self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
    "        self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
    "        self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
    "        \n",
    "#         self.u20_x = u20_x.clone().detach().requires_grad_(True).float().to(device)\n",
    "#         self.u20_t = torch.tensor(u20_t, requires_grad=True).float().to(device)\n",
    "        self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
    "        self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
    "        self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
    "        self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
    "        self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
    "        self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
    "        self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
    "      \n",
    "        \n",
    "        self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
    "        self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.tensor([1.2], requires_grad=True).to(device)\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        \n",
    "        self.lambda_2 = torch.tensor([2.2], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        self.dnn = DNN().to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params':self.dnn.parameters()},\n",
    "            {'params': [self.lambda_1], 'lr': 0.0001},\n",
    "            {'params': [self.lambda_2], 'lr': 0.0001}\n",
    "        ],lr=0.0005 )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_u(self, x, t):  \n",
    "        u1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,0],1)\n",
    "        v1 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,1],1)\n",
    "        u2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,2],1)\n",
    "        v2 = torch.unsqueeze(self.dnn(torch.cat([x, t], dim=1))[:,3],1)\n",
    "        return u1, v1, u2, v2\n",
    "\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        beta=1\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        lambda_3 = -1\n",
    "        lambda_4 = 0\n",
    "        u1, v1,u2, v2= self.net_u(x, t)\n",
    "        \n",
    "        u1_t = torch.autograd.grad(\n",
    "            u1, t, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_x = torch.autograd.grad(\n",
    "            u1, x, \n",
    "            grad_outputs=torch.ones_like(u1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u1_xx = torch.autograd.grad(\n",
    "            u1_x, x, \n",
    "            grad_outputs=torch.ones_like(u1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v1_t = torch.autograd.grad(\n",
    "            v1, t, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_x = torch.autograd.grad(\n",
    "            v1, x, \n",
    "            grad_outputs=torch.ones_like(v1),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v1_xx = torch.autograd.grad(\n",
    "            v1_x, x, \n",
    "            grad_outputs=torch.ones_like(v1_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        u2_t = torch.autograd.grad(\n",
    "            u2, t, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_x = torch.autograd.grad(\n",
    "            u2, x, \n",
    "            grad_outputs=torch.ones_like(u2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u2_xx = torch.autograd.grad(\n",
    "            u2_x, x, \n",
    "            grad_outputs=torch.ones_like(u2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        v2_t = torch.autograd.grad(\n",
    "            v2, t, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_x = torch.autograd.grad(\n",
    "            v2, x, \n",
    "            grad_outputs=torch.ones_like(v2),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v2_xx = torch.autograd.grad(\n",
    "            v2_x, x, \n",
    "            grad_outputs=torch.ones_like(v2_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f_u1 = (\n",
    "            v1_t + u1_xx\n",
    "        + lambda_1*u1*(u1**2 + v1**2) + lambda_2*u1*(u2**2 + v2**2) + lambda_3*(u1*(u2**2 - v2**2) + 2*u2*v1*v2) + lambda_4*(u2*(u1**2 - v1**2) + 2*u1*v1*v2)\n",
    "    )\n",
    "\n",
    "        f_v1 = (\n",
    "                -u1_t + v1_xx\n",
    "            + lambda_1*v1*(u1**2 + v1**2) + lambda_2*v1*(u2**2 + v2**2) - lambda_3*(v1*(u2**2 - v2**2) - 2*u1*u2*v2) - lambda_4*(v2*(u1**2 - v1**2) - 2*u1*u2*v1)\n",
    "        )\n",
    "\n",
    "        f_u2 = (\n",
    "                v2_t + beta*u2_xx\n",
    "            + lambda_1*u2*(u2**2 + v2**2) + lambda_2*u2*(u1**2 + v1**2) + lambda_3*(u2*(u1**2 - v1**2) + 2*u1*v2*v1) + lambda_4*(u1*(u2**2 - v2**2) + 2*u2*v2*v1)\n",
    "        )\n",
    "\n",
    "        f_v2 = (\n",
    "                -u2_t + beta*v2_xx\n",
    "            + lambda_1*v2*(u2**2 + v2**2) + lambda_2*v2*(u1**2 + v1**2) - lambda_3*(v2*(u1**2 - v1**2) - 2*u2*u1*v1) - lambda_4*(v1*(u2**2 - v2**2) - 2*u2*u1*v2)\n",
    "        )\n",
    "        a=u1_x\n",
    "        b=v1_x\n",
    "        c=u2_x\n",
    "        d=v2_x\n",
    "        return f_u1, f_v1, f_u2, f_v2, a, b, c, d\n",
    "     \n",
    "    def loss_func(self):\n",
    "        for self.iter in tqdm(range(50000)):\n",
    "            torch.cuda.empty_cache()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            u10_pred, v10_pred, u20_pred, v20_pred = self.net_u(self.u10_x,self.u10_t)\n",
    "            u30_pred1, v30_pred1, u30_pred2, v30_pred2 = self.net_u(self.u30_x,self.u30_t)\n",
    "            u1b_pred1, v1b_pred1, u2b_pred1, v2b_pred1 = self.net_u(self.ub_x1,self.ub_t1)\n",
    "            u1b_pred2, v1b_pred2, u2b_pred2, v2b_pred2 = self .net_u(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            \n",
    "            a1,b1,c1,d1,u1bx_pred1, v1bx_pred1,u2bx_pred1, v2bx_pred1 = self.net_f(self.ub_x1,self.ub_t1)\n",
    "            a2,b2,c2,d2,u1bx_pred2, v1bx_pred2,u2bx_pred2, v2bx_pred2 = self.net_f(self.ub_x2,self.ub_t2)\n",
    "            \n",
    "            f_predu1, f_predv1, f_predu2, f_predv2,a1,b1,c1,d1 = self.net_f(self.uf_x,self.uf_t)\n",
    "            \n",
    "            loss_u0 = torch.mean((self.u10_r - u10_pred) ** 2)+torch.mean((self.u10_c - v10_pred) ** 2)+torch.mean((self.u20_r - u20_pred) ** 2)+torch.mean((self.u20_c - v20_pred) ** 2)\n",
    "            loss_ub = torch.mean((u1b_pred1 - u1b_pred2) ** 2)+torch.mean((v1b_pred1 - v1b_pred2) ** 2)+torch.mean((u2b_pred1 - u2b_pred2) ** 2)+torch.mean((v2b_pred1 - v2b_pred2) ** 2)\n",
    "            loss_ubx = torch.mean((u1bx_pred1 - u1bx_pred2) ** 2)+torch.mean((v1bx_pred1 - v1bx_pred2) ** 2)+torch.mean((u2bx_pred1 - u2bx_pred2) ** 2)+torch.mean((v2bx_pred1 - v2bx_pred2) ** 2)\n",
    "            loss_f = torch.mean(f_predu1 ** 2)+torch.mean(f_predv1 ** 2)+torch.mean(f_predu2 ** 2)+torch.mean(f_predv2 ** 2)\n",
    "            loss_u1=torch.mean((self.u30_r1 - u30_pred1) ** 2)+torch.mean((self.u30_c1 - v30_pred1) ** 2)+torch.mean((self.u30_r2 - u30_pred2) ** 2)+torch.mean((self.u30_c2 - v30_pred2) ** 2)\n",
    "            \n",
    "            loss = loss_f+loss_u0 + loss_ub + loss_ubx+loss_u1\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.iter += 1\n",
    "            with torch.no_grad():\n",
    "                if self.iter % 500 == 0:\n",
    "                    print('Iter %d, Loss: %.5e, Loss_u0: %.5e, Loss_ub: %.5e, Loss_ubx: %.5e, Loss_f: %.5e, Loss_u1: %.5e, lambda1: %.5e, lambda2: %.5e' % (self.iter, loss.item(), loss_u0.item(), loss_ub.item(), loss_ubx.item(), loss_f.item(), loss_u1.item(), self.lambda_1,self.lambda_2))\n",
    "        return float(loss)\n",
    "        \n",
    " \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "        \n",
    "    def predict1(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u1, v1 , u2, v2= self.net_u(x, t)\n",
    "        u1 = u1.detach().cpu().numpy()\n",
    "        v1 = v1.detach().cpu().numpy()\n",
    "        u2 = u2.detach().cpu().numpy()\n",
    "        v2 = v2.detach().cpu().numpy()\n",
    "        return u1, v1, u2, v2\n",
    "    \n",
    "    def predict2(self, X, Y):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        f1, f2, f3, f4, a, b, c, d = self.net_f(x, t)\n",
    "        f1 = f1.detach().cpu().numpy()\n",
    "        f2 = f2.detach().cpu().numpy()\n",
    "        f3 = f3.detach().cpu().numpy()\n",
    "        f4 = f4.detach().cpu().numpy()\n",
    "        return  f1, f2, f3, f4, a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08905a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_x = torch.tensor(u10_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_t = torch.tensor(u10_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_r = torch.tensor(u10_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u10_c = torch.tensor(u10_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x1 = torch.tensor(ub_x1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_x2 = torch.tensor(ub_x2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t1 = torch.tensor(ub_t1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ub_t2 = torch.tensor(ub_t2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_r = torch.tensor(u20_r, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u20_c = torch.tensor(u20_c, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_x = torch.tensor(u30_x, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_t = torch.tensor(u30_t, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r1 = torch.tensor(u30_r1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c1 = torch.tensor(u30_c1, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_r2 = torch.tensor(u30_r2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u30_c2 = torch.tensor(u30_c2, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_x = torch.tensor(uf_x,requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.uf_t = torch.tensor(uf_t,requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PhysicsInformedNN(train10_x, train10_t, train10_r, train10_c, \n",
    "                          trainb_x1, trainb_t1, trainb_x2, trainb_t2,\n",
    "                        train20_r, train20_c, trainu_x, trainu_t,\n",
    "                         train30_x,train30_t, train30_r1, train30_c1,train30_r2,train30_c2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7626fd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500, Loss: 3.08486e-01, Loss_u0: 2.42591e-02, Loss_ub: 2.38473e-05, Loss_ubx: 4.47527e-05, Loss_f: 5.86209e-02, Loss_u1: 2.25537e-01, lambda1: 1.26534e+00, lambda2: 2.24408e+00\n",
      "Iter 1000, Loss: 1.49298e-01, Loss_u0: 1.49296e-02, Loss_ub: 8.02627e-05, Loss_ubx: 1.38212e-04, Loss_f: 3.40719e-02, Loss_u1: 1.00077e-01, lambda1: 1.29435e+00, lambda2: 2.23519e+00\n",
      "Iter 1500, Loss: 1.30049e-01, Loss_u0: 1.30593e-02, Loss_ub: 5.92679e-05, Loss_ubx: 4.09880e-05, Loss_f: 3.04001e-02, Loss_u1: 8.64889e-02, lambda1: 1.29068e+00, lambda2: 2.15277e+00\n",
      "Iter 2000, Loss: 1.23800e-01, Loss_u0: 1.19477e-02, Loss_ub: 3.50936e-05, Loss_ubx: 3.09842e-05, Loss_f: 2.95802e-02, Loss_u1: 8.22057e-02, lambda1: 1.27647e+00, lambda2: 2.09015e+00\n",
      "Iter 2500, Loss: 1.20674e-01, Loss_u0: 1.11728e-02, Loss_ub: 1.79728e-05, Loss_ubx: 3.22710e-05, Loss_f: 2.90848e-02, Loss_u1: 8.03661e-02, lambda1: 1.25859e+00, lambda2: 2.03456e+00\n",
      "Iter 3000, Loss: 1.17921e-01, Loss_u0: 1.04023e-02, Loss_ub: 1.33287e-05, Loss_ubx: 2.99162e-05, Loss_f: 2.86431e-02, Loss_u1: 7.88325e-02, lambda1: 1.23858e+00, lambda2: 1.98340e+00\n",
      "Iter 3500, Loss: 1.13049e-01, Loss_u0: 8.55011e-03, Loss_ub: 1.29207e-05, Loss_ubx: 2.14604e-05, Loss_f: 2.75640e-02, Loss_u1: 7.69010e-02, lambda1: 1.20816e+00, lambda2: 1.93787e+00\n",
      "Iter 4000, Loss: 1.10050e-01, Loss_u0: 7.40618e-03, Loss_ub: 1.54496e-05, Loss_ubx: 2.08630e-05, Loss_f: 2.64520e-02, Loss_u1: 7.61553e-02, lambda1: 1.18575e+00, lambda2: 1.89554e+00\n",
      "Iter 4500, Loss: 1.07888e-01, Loss_u0: 6.62955e-03, Loss_ub: 1.74750e-05, Loss_ubx: 2.49535e-05, Loss_f: 2.58820e-02, Loss_u1: 7.53338e-02, lambda1: 1.17164e+00, lambda2: 1.85437e+00\n",
      "Iter 5000, Loss: 1.06082e-01, Loss_u0: 6.04004e-03, Loss_ub: 1.90733e-05, Loss_ubx: 2.56826e-05, Loss_f: 2.54722e-02, Loss_u1: 7.45248e-02, lambda1: 1.15656e+00, lambda2: 1.81527e+00\n",
      "Iter 5500, Loss: 1.04513e-01, Loss_u0: 5.67941e-03, Loss_ub: 2.00666e-05, Loss_ubx: 2.26033e-05, Loss_f: 2.51226e-02, Loss_u1: 7.36683e-02, lambda1: 1.13319e+00, lambda2: 1.77881e+00\n",
      "Iter 6000, Loss: 1.03147e-01, Loss_u0: 5.40177e-03, Loss_ub: 2.09592e-05, Loss_ubx: 1.98820e-05, Loss_f: 2.47744e-02, Loss_u1: 7.29304e-02, lambda1: 1.10335e+00, lambda2: 1.74506e+00\n",
      "Iter 6500, Loss: 1.02076e-01, Loss_u0: 5.14194e-03, Loss_ub: 2.49794e-05, Loss_ubx: 1.98366e-05, Loss_f: 2.44478e-02, Loss_u1: 7.24414e-02, lambda1: 1.07296e+00, lambda2: 1.71410e+00\n",
      "Iter 7000, Loss: 1.00972e-01, Loss_u0: 5.00686e-03, Loss_ub: 2.22826e-05, Loss_ubx: 1.88333e-05, Loss_f: 2.40430e-02, Loss_u1: 7.18814e-02, lambda1: 1.04446e+00, lambda2: 1.68627e+00\n",
      "Iter 7500, Loss: 1.00530e-01, Loss_u0: 4.77100e-03, Loss_ub: 2.39346e-05, Loss_ubx: 2.14931e-05, Loss_f: 2.40740e-02, Loss_u1: 7.16400e-02, lambda1: 1.02121e+00, lambda2: 1.66188e+00\n",
      "Iter 8000, Loss: 9.93457e-02, Loss_u0: 4.73184e-03, Loss_ub: 1.75723e-05, Loss_ubx: 2.10250e-05, Loss_f: 2.34770e-02, Loss_u1: 7.10983e-02, lambda1: 1.00603e+00, lambda2: 1.64054e+00\n",
      "Iter 8500, Loss: 9.86659e-02, Loss_u0: 4.62672e-03, Loss_ub: 1.64936e-05, Loss_ubx: 2.19316e-05, Loss_f: 2.32821e-02, Loss_u1: 7.07186e-02, lambda1: 9.98303e-01, lambda2: 1.62136e+00\n",
      "Iter 9000, Loss: 9.78464e-02, Loss_u0: 4.51686e-03, Loss_ub: 1.67863e-05, Loss_ubx: 2.27235e-05, Loss_f: 2.30130e-02, Loss_u1: 7.02771e-02, lambda1: 9.95499e-01, lambda2: 1.60347e+00\n",
      "Iter 9500, Loss: 9.69608e-02, Loss_u0: 4.39034e-03, Loss_ub: 1.66019e-05, Loss_ubx: 2.37740e-05, Loss_f: 2.27633e-02, Loss_u1: 6.97668e-02, lambda1: 9.95771e-01, lambda2: 1.58598e+00\n",
      "Iter 10000, Loss: 9.61259e-02, Loss_u0: 4.20525e-03, Loss_ub: 1.83829e-05, Loss_ubx: 2.47808e-05, Loss_f: 2.27327e-02, Loss_u1: 6.91448e-02, lambda1: 1.00272e+00, lambda2: 1.56665e+00\n",
      "Iter 10500, Loss: 9.47571e-02, Loss_u0: 3.93021e-03, Loss_ub: 2.04318e-05, Loss_ubx: 2.35407e-05, Loss_f: 2.23633e-02, Loss_u1: 6.84196e-02, lambda1: 1.02463e+00, lambda2: 1.54149e+00\n",
      "Iter 11000, Loss: 9.35147e-02, Loss_u0: 3.62713e-03, Loss_ub: 2.44789e-05, Loss_ubx: 2.12302e-05, Loss_f: 2.23749e-02, Loss_u1: 6.74669e-02, lambda1: 1.06344e+00, lambda2: 1.50776e+00\n",
      "Iter 11500, Loss: 9.21334e-02, Loss_u0: 3.27803e-03, Loss_ub: 2.91380e-05, Loss_ubx: 2.04847e-05, Loss_f: 2.29654e-02, Loss_u1: 6.58403e-02, lambda1: 1.11368e+00, lambda2: 1.47016e+00\n",
      "Iter 12000, Loss: 8.83926e-02, Loss_u0: 2.99293e-03, Loss_ub: 3.77941e-05, Loss_ubx: 1.32670e-05, Loss_f: 2.21926e-02, Loss_u1: 6.31560e-02, lambda1: 1.16705e+00, lambda2: 1.44066e+00\n",
      "Iter 12500, Loss: 8.49180e-02, Loss_u0: 3.11511e-03, Loss_ub: 3.50829e-05, Loss_ubx: 1.21042e-05, Loss_f: 2.11044e-02, Loss_u1: 6.06513e-02, lambda1: 1.19857e+00, lambda2: 1.43102e+00\n",
      "Iter 13000, Loss: 8.27648e-02, Loss_u0: 3.15509e-03, Loss_ub: 4.03845e-05, Loss_ubx: 1.07866e-05, Loss_f: 2.01587e-02, Loss_u1: 5.93998e-02, lambda1: 1.21025e+00, lambda2: 1.44919e+00\n",
      "Iter 13500, Loss: 8.12720e-02, Loss_u0: 3.16740e-03, Loss_ub: 2.82585e-05, Loss_ubx: 1.12014e-05, Loss_f: 1.95507e-02, Loss_u1: 5.85144e-02, lambda1: 1.20866e+00, lambda2: 1.47883e+00\n",
      "Iter 14000, Loss: 8.02109e-02, Loss_u0: 3.04485e-03, Loss_ub: 2.73620e-05, Loss_ubx: 1.26724e-05, Loss_f: 1.92375e-02, Loss_u1: 5.78885e-02, lambda1: 1.20067e+00, lambda2: 1.50958e+00\n",
      "Iter 14500, Loss: 7.91506e-02, Loss_u0: 3.01787e-03, Loss_ub: 2.38811e-05, Loss_ubx: 1.44952e-05, Loss_f: 1.88333e-02, Loss_u1: 5.72610e-02, lambda1: 1.18990e+00, lambda2: 1.53879e+00\n",
      "Iter 15000, Loss: 7.82193e-02, Loss_u0: 2.98141e-03, Loss_ub: 2.47069e-05, Loss_ubx: 1.60094e-05, Loss_f: 1.85232e-02, Loss_u1: 5.66740e-02, lambda1: 1.17828e+00, lambda2: 1.56612e+00\n",
      "Iter 15500, Loss: 7.73849e-02, Loss_u0: 2.89661e-03, Loss_ub: 2.68145e-05, Loss_ubx: 1.69229e-05, Loss_f: 1.82528e-02, Loss_u1: 5.61918e-02, lambda1: 1.16731e+00, lambda2: 1.59057e+00\n",
      "Iter 16000, Loss: 7.66140e-02, Loss_u0: 2.85670e-03, Loss_ub: 2.27442e-05, Loss_ubx: 1.70540e-05, Loss_f: 1.79701e-02, Loss_u1: 5.57474e-02, lambda1: 1.15787e+00, lambda2: 1.61276e+00\n",
      "Iter 16500, Loss: 7.60301e-02, Loss_u0: 2.79637e-03, Loss_ub: 2.64652e-05, Loss_ubx: 1.63195e-05, Loss_f: 1.78016e-02, Loss_u1: 5.53894e-02, lambda1: 1.15013e+00, lambda2: 1.63260e+00\n",
      "Iter 17000, Loss: 7.55565e-02, Loss_u0: 2.72186e-03, Loss_ub: 1.96616e-05, Loss_ubx: 1.58357e-05, Loss_f: 1.76483e-02, Loss_u1: 5.51508e-02, lambda1: 1.14348e+00, lambda2: 1.65079e+00\n",
      "Iter 17500, Loss: 7.51099e-02, Loss_u0: 2.67648e-03, Loss_ub: 1.98460e-05, Loss_ubx: 1.47188e-05, Loss_f: 1.74852e-02, Loss_u1: 5.49136e-02, lambda1: 1.13750e+00, lambda2: 1.66648e+00\n",
      "Iter 18000, Loss: 7.49412e-02, Loss_u0: 2.67558e-03, Loss_ub: 1.78848e-05, Loss_ubx: 1.42689e-05, Loss_f: 1.74334e-02, Loss_u1: 5.48001e-02, lambda1: 1.13191e+00, lambda2: 1.68014e+00\n",
      "Iter 18500, Loss: 7.44566e-02, Loss_u0: 2.63388e-03, Loss_ub: 1.73176e-05, Loss_ubx: 1.35661e-05, Loss_f: 1.72336e-02, Loss_u1: 5.45582e-02, lambda1: 1.12672e+00, lambda2: 1.69118e+00\n",
      "Iter 19000, Loss: 7.45331e-02, Loss_u0: 2.60962e-03, Loss_ub: 5.73751e-05, Loss_ubx: 1.40377e-05, Loss_f: 1.73858e-02, Loss_u1: 5.44663e-02, lambda1: 1.12180e+00, lambda2: 1.70053e+00\n",
      "Iter 19500, Loss: 7.39139e-02, Loss_u0: 2.56170e-03, Loss_ub: 1.66608e-05, Loss_ubx: 1.27832e-05, Loss_f: 1.70664e-02, Loss_u1: 5.42564e-02, lambda1: 1.11723e+00, lambda2: 1.70815e+00\n",
      "Iter 20000, Loss: 7.36855e-02, Loss_u0: 2.55593e-03, Loss_ub: 1.63194e-05, Loss_ubx: 1.27325e-05, Loss_f: 1.69776e-02, Loss_u1: 5.41229e-02, lambda1: 1.11307e+00, lambda2: 1.71410e+00\n",
      "Iter 20500, Loss: 7.34469e-02, Loss_u0: 2.54772e-03, Loss_ub: 1.55990e-05, Loss_ubx: 1.27686e-05, Loss_f: 1.68830e-02, Loss_u1: 5.39878e-02, lambda1: 1.10936e+00, lambda2: 1.71889e+00\n",
      "Iter 21000, Loss: 7.32416e-02, Loss_u0: 2.53470e-03, Loss_ub: 1.53204e-05, Loss_ubx: 1.28661e-05, Loss_f: 1.68103e-02, Loss_u1: 5.38684e-02, lambda1: 1.10603e+00, lambda2: 1.72210e+00\n",
      "Iter 21500, Loss: 7.31711e-02, Loss_u0: 2.50337e-03, Loss_ub: 1.94158e-05, Loss_ubx: 1.30608e-05, Loss_f: 1.68828e-02, Loss_u1: 5.37525e-02, lambda1: 1.10321e+00, lambda2: 1.72464e+00\n",
      "Iter 22000, Loss: 7.29541e-02, Loss_u0: 2.47220e-03, Loss_ub: 1.68388e-05, Loss_ubx: 1.36802e-05, Loss_f: 1.67500e-02, Loss_u1: 5.37014e-02, lambda1: 1.10067e+00, lambda2: 1.72658e+00\n",
      "Iter 22500, Loss: 7.26980e-02, Loss_u0: 2.52910e-03, Loss_ub: 1.56840e-05, Loss_ubx: 1.36440e-05, Loss_f: 1.65956e-02, Loss_u1: 5.35439e-02, lambda1: 1.09847e+00, lambda2: 1.72810e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23000, Loss: 7.25056e-02, Loss_u0: 2.50908e-03, Loss_ub: 1.46290e-05, Loss_ubx: 1.39194e-05, Loss_f: 1.65233e-02, Loss_u1: 5.34448e-02, lambda1: 1.09651e+00, lambda2: 1.72910e+00\n",
      "Iter 23500, Loss: 7.24063e-02, Loss_u0: 2.47449e-03, Loss_ub: 1.47864e-05, Loss_ubx: 1.44883e-05, Loss_f: 1.65563e-02, Loss_u1: 5.33462e-02, lambda1: 1.09476e+00, lambda2: 1.72983e+00\n",
      "Iter 24000, Loss: 7.21738e-02, Loss_u0: 2.48635e-03, Loss_ub: 1.44499e-05, Loss_ubx: 1.47842e-05, Loss_f: 1.64057e-02, Loss_u1: 5.32525e-02, lambda1: 1.09320e+00, lambda2: 1.73037e+00\n",
      "Iter 24500, Loss: 7.24195e-02, Loss_u0: 2.52558e-03, Loss_ub: 3.49706e-05, Loss_ubx: 1.59107e-05, Loss_f: 1.66486e-02, Loss_u1: 5.31944e-02, lambda1: 1.09175e+00, lambda2: 1.73076e+00\n",
      "Iter 25000, Loss: 7.18341e-02, Loss_u0: 2.47127e-03, Loss_ub: 1.34140e-05, Loss_ubx: 1.56738e-05, Loss_f: 1.62753e-02, Loss_u1: 5.30584e-02, lambda1: 1.09043e+00, lambda2: 1.73114e+00\n",
      "Iter 25500, Loss: 7.16655e-02, Loss_u0: 2.46017e-03, Loss_ub: 1.30363e-05, Loss_ubx: 1.60989e-05, Loss_f: 1.62260e-02, Loss_u1: 5.29502e-02, lambda1: 1.08916e+00, lambda2: 1.73154e+00\n",
      "Iter 26000, Loss: 7.15038e-02, Loss_u0: 2.46678e-03, Loss_ub: 1.29391e-05, Loss_ubx: 1.64588e-05, Loss_f: 1.61519e-02, Loss_u1: 5.28558e-02, lambda1: 1.08797e+00, lambda2: 1.73210e+00\n",
      "Iter 26500, Loss: 7.14144e-02, Loss_u0: 2.48020e-03, Loss_ub: 1.34393e-05, Loss_ubx: 1.68244e-05, Loss_f: 1.61201e-02, Loss_u1: 5.27838e-02, lambda1: 1.08684e+00, lambda2: 1.73267e+00\n",
      "Iter 27000, Loss: 7.11779e-02, Loss_u0: 2.46413e-03, Loss_ub: 1.10934e-05, Loss_ubx: 1.68616e-05, Loss_f: 1.60696e-02, Loss_u1: 5.26163e-02, lambda1: 1.08568e+00, lambda2: 1.73343e+00\n",
      "Iter 27500, Loss: 7.09489e-02, Loss_u0: 2.41451e-03, Loss_ub: 1.06396e-05, Loss_ubx: 1.71701e-05, Loss_f: 1.60063e-02, Loss_u1: 5.25003e-02, lambda1: 1.08461e+00, lambda2: 1.73430e+00\n",
      "Iter 28000, Loss: 7.07599e-02, Loss_u0: 2.39372e-03, Loss_ub: 1.07346e-05, Loss_ubx: 1.73519e-05, Loss_f: 1.59685e-02, Loss_u1: 5.23696e-02, lambda1: 1.08356e+00, lambda2: 1.73526e+00\n",
      "Iter 28500, Loss: 7.06188e-02, Loss_u0: 2.40061e-03, Loss_ub: 9.57604e-06, Loss_ubx: 1.75474e-05, Loss_f: 1.59217e-02, Loss_u1: 5.22694e-02, lambda1: 1.08254e+00, lambda2: 1.73647e+00\n",
      "Iter 29000, Loss: 7.03910e-02, Loss_u0: 2.39458e-03, Loss_ub: 8.96238e-06, Loss_ubx: 1.77989e-05, Loss_f: 1.58907e-02, Loss_u1: 5.20789e-02, lambda1: 1.08151e+00, lambda2: 1.73773e+00\n",
      "Iter 29500, Loss: 7.01470e-02, Loss_u0: 2.35387e-03, Loss_ub: 9.16615e-06, Loss_ubx: 1.81211e-05, Loss_f: 1.58169e-02, Loss_u1: 5.19490e-02, lambda1: 1.08038e+00, lambda2: 1.73927e+00\n",
      "Iter 30000, Loss: 7.00905e-02, Loss_u0: 2.32690e-03, Loss_ub: 1.41305e-05, Loss_ubx: 1.89294e-05, Loss_f: 1.58681e-02, Loss_u1: 5.18624e-02, lambda1: 1.07918e+00, lambda2: 1.74098e+00\n",
      "Iter 30500, Loss: 6.98188e-02, Loss_u0: 2.35483e-03, Loss_ub: 2.72366e-05, Loss_ubx: 1.94383e-05, Loss_f: 1.58260e-02, Loss_u1: 5.15913e-02, lambda1: 1.07784e+00, lambda2: 1.74292e+00\n",
      "Iter 31000, Loss: 6.97979e-02, Loss_u0: 2.34961e-03, Loss_ub: 2.12527e-05, Loss_ubx: 2.14330e-05, Loss_f: 1.58660e-02, Loss_u1: 5.15397e-02, lambda1: 1.07634e+00, lambda2: 1.74512e+00\n",
      "Iter 31500, Loss: 6.93468e-02, Loss_u0: 2.23264e-03, Loss_ub: 7.81257e-06, Loss_ubx: 2.05982e-05, Loss_f: 1.57426e-02, Loss_u1: 5.13431e-02, lambda1: 1.07471e+00, lambda2: 1.74777e+00\n",
      "Iter 32000, Loss: 6.91549e-02, Loss_u0: 2.21242e-03, Loss_ub: 7.71693e-06, Loss_ubx: 2.14023e-05, Loss_f: 1.56817e-02, Loss_u1: 5.12317e-02, lambda1: 1.07298e+00, lambda2: 1.75061e+00\n",
      "Iter 32500, Loss: 6.89892e-02, Loss_u0: 2.19327e-03, Loss_ub: 1.04438e-05, Loss_ubx: 2.20599e-05, Loss_f: 1.56789e-02, Loss_u1: 5.10846e-02, lambda1: 1.07121e+00, lambda2: 1.75356e+00\n",
      "Iter 33000, Loss: 6.88041e-02, Loss_u0: 2.14394e-03, Loss_ub: 7.31218e-06, Loss_ubx: 2.25981e-05, Loss_f: 1.56531e-02, Loss_u1: 5.09772e-02, lambda1: 1.06943e+00, lambda2: 1.75659e+00\n",
      "Iter 33500, Loss: 6.87978e-02, Loss_u0: 2.11319e-03, Loss_ub: 3.06709e-05, Loss_ubx: 2.37450e-05, Loss_f: 1.56968e-02, Loss_u1: 5.09334e-02, lambda1: 1.06771e+00, lambda2: 1.75983e+00\n",
      "Iter 34000, Loss: 6.85073e-02, Loss_u0: 2.11738e-03, Loss_ub: 1.10157e-05, Loss_ubx: 2.35494e-05, Loss_f: 1.56855e-02, Loss_u1: 5.06698e-02, lambda1: 1.06606e+00, lambda2: 1.76298e+00\n",
      "Iter 34500, Loss: 6.82929e-02, Loss_u0: 2.07035e-03, Loss_ub: 7.62725e-06, Loss_ubx: 2.42427e-05, Loss_f: 1.55671e-02, Loss_u1: 5.06235e-02, lambda1: 1.06451e+00, lambda2: 1.76608e+00\n",
      "Iter 35000, Loss: 6.82228e-02, Loss_u0: 2.02160e-03, Loss_ub: 2.31104e-05, Loss_ubx: 2.43788e-05, Loss_f: 1.56462e-02, Loss_u1: 5.05076e-02, lambda1: 1.06311e+00, lambda2: 1.76914e+00\n",
      "Iter 35500, Loss: 6.80275e-02, Loss_u0: 1.97976e-03, Loss_ub: 9.43958e-06, Loss_ubx: 2.48554e-05, Loss_f: 1.55812e-02, Loss_u1: 5.04322e-02, lambda1: 1.06181e+00, lambda2: 1.77196e+00\n",
      "Iter 36000, Loss: 6.78314e-02, Loss_u0: 1.96411e-03, Loss_ub: 1.33566e-05, Loss_ubx: 2.55423e-05, Loss_f: 1.55120e-02, Loss_u1: 5.03163e-02, lambda1: 1.06065e+00, lambda2: 1.77468e+00\n",
      "Iter 36500, Loss: 6.76395e-02, Loss_u0: 1.96427e-03, Loss_ub: 1.76247e-05, Loss_ubx: 2.60048e-05, Loss_f: 1.54856e-02, Loss_u1: 5.01460e-02, lambda1: 1.05959e+00, lambda2: 1.77711e+00\n",
      "Iter 37000, Loss: 6.75939e-02, Loss_u0: 1.89745e-03, Loss_ub: 1.10695e-05, Loss_ubx: 2.58818e-05, Loss_f: 1.55837e-02, Loss_u1: 5.00757e-02, lambda1: 1.05864e+00, lambda2: 1.77947e+00\n",
      "Iter 37500, Loss: 6.72917e-02, Loss_u0: 1.90390e-03, Loss_ub: 1.06778e-05, Loss_ubx: 2.72321e-05, Loss_f: 1.54010e-02, Loss_u1: 4.99488e-02, lambda1: 1.05771e+00, lambda2: 1.78154e+00\n",
      "Iter 38000, Loss: 6.71501e-02, Loss_u0: 1.89517e-03, Loss_ub: 1.08796e-05, Loss_ubx: 2.81089e-05, Loss_f: 1.53337e-02, Loss_u1: 4.98822e-02, lambda1: 1.05684e+00, lambda2: 1.78363e+00\n",
      "Iter 38500, Loss: 6.70418e-02, Loss_u0: 1.89997e-03, Loss_ub: 1.51050e-05, Loss_ubx: 2.87694e-05, Loss_f: 1.53569e-02, Loss_u1: 4.97411e-02, lambda1: 1.05591e+00, lambda2: 1.78559e+00\n",
      "Iter 39000, Loss: 6.71170e-02, Loss_u0: 1.78846e-03, Loss_ub: 5.75052e-05, Loss_ubx: 2.82760e-05, Loss_f: 1.55969e-02, Loss_u1: 4.96459e-02, lambda1: 1.05489e+00, lambda2: 1.78739e+00\n",
      "Iter 39500, Loss: 6.66006e-02, Loss_u0: 1.83049e-03, Loss_ub: 1.30231e-05, Loss_ubx: 2.95881e-05, Loss_f: 1.52230e-02, Loss_u1: 4.95045e-02, lambda1: 1.05373e+00, lambda2: 1.78924e+00\n",
      "Iter 40000, Loss: 6.64347e-02, Loss_u0: 1.82176e-03, Loss_ub: 1.37797e-05, Loss_ubx: 3.01443e-05, Loss_f: 1.51977e-02, Loss_u1: 4.93712e-02, lambda1: 1.05239e+00, lambda2: 1.79110e+00\n",
      "Iter 40500, Loss: 6.64333e-02, Loss_u0: 1.75495e-03, Loss_ub: 4.87139e-05, Loss_ubx: 2.97776e-05, Loss_f: 1.53103e-02, Loss_u1: 4.92896e-02, lambda1: 1.05079e+00, lambda2: 1.79301e+00\n",
      "Iter 41000, Loss: 6.59992e-02, Loss_u0: 1.75669e-03, Loss_ub: 2.73427e-05, Loss_ubx: 3.00575e-05, Loss_f: 1.50886e-02, Loss_u1: 4.90966e-02, lambda1: 1.04893e+00, lambda2: 1.79509e+00\n",
      "Iter 41500, Loss: 6.57323e-02, Loss_u0: 1.73658e-03, Loss_ub: 1.60703e-05, Loss_ubx: 2.96439e-05, Loss_f: 1.49820e-02, Loss_u1: 4.89680e-02, lambda1: 1.04678e+00, lambda2: 1.79734e+00\n",
      "Iter 42000, Loss: 6.56063e-02, Loss_u0: 1.72318e-03, Loss_ub: 2.92334e-05, Loss_ubx: 2.92138e-05, Loss_f: 1.49579e-02, Loss_u1: 4.88668e-02, lambda1: 1.04433e+00, lambda2: 1.79984e+00\n",
      "Iter 42500, Loss: 6.52853e-02, Loss_u0: 1.69748e-03, Loss_ub: 1.54839e-05, Loss_ubx: 2.85106e-05, Loss_f: 1.48283e-02, Loss_u1: 4.87155e-02, lambda1: 1.04162e+00, lambda2: 1.80258e+00\n",
      "Iter 43000, Loss: 6.51511e-02, Loss_u0: 1.65657e-03, Loss_ub: 1.57441e-05, Loss_ubx: 2.82055e-05, Loss_f: 1.47524e-02, Loss_u1: 4.86982e-02, lambda1: 1.03864e+00, lambda2: 1.80545e+00\n",
      "Iter 43500, Loss: 6.48737e-02, Loss_u0: 1.69670e-03, Loss_ub: 1.60750e-05, Loss_ubx: 2.72873e-05, Loss_f: 1.46964e-02, Loss_u1: 4.84373e-02, lambda1: 1.03547e+00, lambda2: 1.80850e+00\n",
      "Iter 44000, Loss: 6.47881e-02, Loss_u0: 1.68320e-03, Loss_ub: 1.91692e-05, Loss_ubx: 2.73797e-05, Loss_f: 1.45578e-02, Loss_u1: 4.85005e-02, lambda1: 1.03214e+00, lambda2: 1.81171e+00\n",
      "Iter 44500, Loss: 6.44632e-02, Loss_u0: 1.67756e-03, Loss_ub: 2.03172e-05, Loss_ubx: 2.60696e-05, Loss_f: 1.44363e-02, Loss_u1: 4.83029e-02, lambda1: 1.02870e+00, lambda2: 1.81487e+00\n",
      "Iter 45000, Loss: 6.43968e-02, Loss_u0: 1.69023e-03, Loss_ub: 2.40180e-05, Loss_ubx: 2.54888e-05, Loss_f: 1.45338e-02, Loss_u1: 4.81232e-02, lambda1: 1.02517e+00, lambda2: 1.81810e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 45500, Loss: 6.40382e-02, Loss_u0: 1.64397e-03, Loss_ub: 1.46536e-05, Loss_ubx: 2.43074e-05, Loss_f: 1.42585e-02, Loss_u1: 4.80968e-02, lambda1: 1.02161e+00, lambda2: 1.82126e+00\n",
      "Iter 46000, Loss: 6.40142e-02, Loss_u0: 1.67206e-03, Loss_ub: 6.63071e-05, Loss_ubx: 2.35511e-05, Loss_f: 1.44449e-02, Loss_u1: 4.78074e-02, lambda1: 1.01808e+00, lambda2: 1.82432e+00\n",
      "Iter 46500, Loss: 6.42924e-02, Loss_u0: 1.72389e-03, Loss_ub: 1.81841e-05, Loss_ubx: 2.47656e-05, Loss_f: 1.45682e-02, Loss_u1: 4.79573e-02, lambda1: 1.01458e+00, lambda2: 1.82714e+00\n",
      "Iter 47000, Loss: 6.35204e-02, Loss_u0: 1.62182e-03, Loss_ub: 1.49509e-05, Loss_ubx: 2.23936e-05, Loss_f: 1.40546e-02, Loss_u1: 4.78066e-02, lambda1: 1.01114e+00, lambda2: 1.82995e+00\n",
      "Iter 47500, Loss: 6.34945e-02, Loss_u0: 1.62905e-03, Loss_ub: 3.09862e-05, Loss_ubx: 2.25861e-05, Loss_f: 1.40956e-02, Loss_u1: 4.77164e-02, lambda1: 1.00775e+00, lambda2: 1.83255e+00\n",
      "Iter 48000, Loss: 6.31958e-02, Loss_u0: 1.62056e-03, Loss_ub: 1.84281e-05, Loss_ubx: 2.18291e-05, Loss_f: 1.39379e-02, Loss_u1: 4.75971e-02, lambda1: 1.00445e+00, lambda2: 1.83492e+00\n",
      "Iter 48500, Loss: 6.30307e-02, Loss_u0: 1.57977e-03, Loss_ub: 1.40142e-05, Loss_ubx: 2.12440e-05, Loss_f: 1.38100e-02, Loss_u1: 4.76057e-02, lambda1: 1.00123e+00, lambda2: 1.83713e+00\n",
      "Iter 49000, Loss: 6.29044e-02, Loss_u0: 1.58259e-03, Loss_ub: 1.36380e-05, Loss_ubx: 2.14502e-05, Loss_f: 1.37525e-02, Loss_u1: 4.75343e-02, lambda1: 9.98130e-01, lambda2: 1.83925e+00\n",
      "Iter 49500, Loss: 6.27607e-02, Loss_u0: 1.57760e-03, Loss_ub: 1.29251e-05, Loss_ubx: 2.09100e-05, Loss_f: 1.37128e-02, Loss_u1: 4.74364e-02, lambda1: 9.95090e-01, lambda2: 1.84136e+00\n",
      "Iter 50000, Loss: 6.26563e-02, Loss_u0: 1.60594e-03, Loss_ub: 1.77353e-05, Loss_ubx: 2.06261e-05, Loss_f: 1.37794e-02, Loss_u1: 4.72326e-02, lambda1: 9.92136e-01, lambda2: 1.84317e+00\n",
      "Wall time: 1h 57min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e027874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, requires_grad=True).float().to(device)\n",
      "C:\\Users\\npofsi\\AppData\\Local\\Temp\\ipykernel_1304\\1292812349.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(Y, requires_grad=True).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "u00_1, v00_1, u00_2, v00_2 = model.predict1(test10_x,test10_t)\n",
    "u11_1, v11_1, u11_2, v11_2 = model.predict1(testb_x1,testb_t1)\n",
    "u22_1, v22_1, u22_2, v22_2 = model.predict1(testb_x2,testb_t2)\n",
    "a1,b1,c1,d1, ux11_1, vx11_1, ux11_2, vx11_2 = model.predict2(testb_x1,testb_t1)\n",
    "a1,b1,c1,d1, ux22_1, vx22_1, ux22_2, vx22_2 = model.predict2(testb_x2,testb_t2)\n",
    "\n",
    "f1, f2, f3, f4,a1,b1,c1,d1 = model.predict2(testu_x,testu_t)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f5c0224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_u0: 8.17190e-03, Error_ub: 2.01331e-04, Error_ubx: 2.21011e-05, Error_f: 3.60797e-02\n"
     ]
    }
   ],
   "source": [
    "error_u0= ((np.linalg.norm(test10_r - u00_1, 2))**2+(np.linalg.norm(test10_c - v00_1, 2))**2+(np.linalg.norm(test20_r - u00_2, 2))**2+(np.linalg.norm(test20_c - v00_2, 2))**2)/20\n",
    "error_ub= ((np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2+(np.linalg.norm(u11_1 - u22_1, 2))**2+(np.linalg.norm(v11_1 - v22_1, 2))**2)/20\n",
    "error_ubx= (\n",
    "    (np.linalg.norm(ux11_1.cpu().detach().numpy() - ux22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_1.cpu().detach().numpy() - vx22_1.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(ux11_2.cpu().detach().numpy() - ux22_2.cpu().detach().numpy(), 2))**2+\n",
    "    (np.linalg.norm(vx11_2.cpu().detach().numpy() - vx22_2.cpu().detach().numpy(), 2))**2)/20\n",
    "\n",
    "error_f= ((np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2+(np.linalg.norm(f1, 2))**2+(np.linalg.norm(f2, 2))**2)/2000\n",
    "print('Error_u0: %.5e, Error_ub: %.5e, Error_ubx: %.5e, Error_f: %.5e' % (error_u0.item(), error_ub.item(), error_ubx.item(), error_f.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
